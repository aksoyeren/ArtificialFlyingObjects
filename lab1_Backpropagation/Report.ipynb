{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0081d975-63e0-49e6-b0d7-a597288d6423",
   "metadata": {},
   "source": [
    "# Report\n",
    "\n",
    "Summary your findings and motivate your choice of approach. A better motivation show your understanding of the lab. Dont forget to include the result from part 1!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28c7c81-20d8-48f8-85ce-e25d78cf81f1",
   "metadata": {},
   "source": [
    "**Name:** Denis Markitanov\\\n",
    "**Date:** 25.11.21\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This lab was aimed to show the difference in Neural Networks made with numpy and pytorch. While I clearly understood the difference, the taks in exercise 1 and 2 were not identical.\n",
    "\n",
    "With numpy I've learned bacis concepts of NNs like forward and back propogation, layers, activation functions and how to improve results with gradien loss.\n",
    "\n",
    "With pytorch I dug deeper into the NNs themselves. Deeper understanding of layers and nodes allowed me to hypertune parameters (aformentioned layers and nodes) and monitor the performance and error losses of models.\n",
    "\n",
    "## Result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5594ad",
   "metadata": {},
   "source": [
    "### Answers to the Numpy exercise:\n",
    "\n",
    "1) The results were satisfactionary. However, as any ML model it may or may not degrade in performance when facing unknown data. This can be solved with further study of the data, which includes hypertuning parameters (layers and nodes) that was studied in 2nd exercise\n",
    "\n",
    "2) This needs to be discovered with experiments. The lower border is of course reasonable minimum under which the model will converge and deliver a good result. The upper border for epochs must be also studied after running test. For this exact exercise 15 was enough. I would suggest that it is pointless to run more than 25 or 30 epochs as the results would be insufficiend and it will take up to 2 times more time to run\n",
    "\n",
    "3) Concept of learning rate is easier to understand if we think about it as a step. Step towards the minimum. We want to minimize the loss gradinet. Which in return will adjust and optimize our weights. Picking an optimized learning rate value is crucial to almost any ML algorithm. Learning rate directly affects the performance and convergence of a model. Small learning rate will make smaller steps. Thus the model will take longer time to find local minimum as we approach it. Higher learning rate will simply \"over step\" the minimum and miss it completely."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d424f355",
   "metadata": {},
   "source": [
    "### Answers to the Pytorch exercise:\n",
    "\n",
    "1) Compared to Perceptron model, the MLP feedforward performed almost 6 times worse. And going further we can see (Combined plots of training loss) that MLP will never be able to converge better.\n",
    "\n",
    "2) Task 3 clearly shows that the loss of model with higher amount of nodes is decreasing. While it is sometimes \"spikes\" I would assueme its due to the randomness of the numbers of nodes I chose. Between 2 and 20 nodes the drop is significant, However, as we go further, it will increase when approaghing 50 and stay on the relatively same value for 100, It significantly dropped at 200 and increased at 400 again. The optimal value would be 200 nodes.\n",
    "\n",
    "Increasing layers showed degradation between 20 and 50 layers. It is expected as the gap between 20 and 50 layers is considerably big. But loss between 50 and following 100 is decreasing again.\n",
    "\n",
    "3) Both layers and nodes affect the performance and convergence time of the model. I would suggest that yes, we can. But only on a small scale and in special circumstances. Nodes represents the features that model learns on. Features may or may not be linearly dependent. We also know that with PCA we can reduce the number of features. The layers on the other hand more direcly affect performance. Higher number of layers will improve the model. But too much layers and performance would be insufficient. So, if we descrease number of nodes (with feature selection of any sort) and substitute it with 1 or 2 new layers instead, then maybe we can expect a better result. But, it highly depends on the data we are working with and considerable time of data mining and examination should be performed beforehand"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
