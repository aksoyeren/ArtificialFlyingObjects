{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "272e1b50",
   "metadata": {},
   "source": [
    "<center><h1 style=\"font-size:40px;\">Exercise I:<br> Backpropagation in Numpy\n",
    "</h1></center>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e887d46c",
   "metadata": {},
   "source": [
    "Welcome to the first lab for Deep Learning!\n",
    "\n",
    "In this lab we will scratch the surface of deep learning. For the first part of this lab we are going to introduce the concept of deep learning. For this lab we will perform and train a network with the help of Numpy to get an idea of how a neural network works.\n",
    "\n",
    "For this lab all tasks include **TODO's** these are expected to be done before the deadline. The labs also include **Question** which should be answered and included in the report. To give a hand if the answer is correct a few asserts and expected results are given. Some sections does not contain any **TODO's** but is good to understand them. \n",
    "\n",
    "\n",
    "Good luck!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd10e95",
   "metadata": {},
   "source": [
    "## Neuron\n",
    "A neuron is a part of a neural network and in it simplest form we can define it as a perceptron. A perceptron can solve binary problems based on $x_{p}$ number of inputs. In other words, problems that can be solved with a linear function.\n",
    "\n",
    "The math for the perceptron or a single neuron are defined as; $y(x,w) = p_{0}(\\sum_{k=1}^{P} w_{k}x_{k} + w_{0}) = \\sum_{k=0}^{P} w_{k}x_{k} = p_{0}(w^{T}x)$ where $p_{0}$ is an activation function to introduce non-linearity into the output of a neuron. For the image below the concept of bias is used and corresponds to $w_{0}$ in the equation above.\n",
    "\n",
    "<center><img src=\"../data/images/activation_node.png\" width=\"600\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe07cc26",
   "metadata": {},
   "source": [
    "## MLP\n",
    "In order to extend from a binary problem we can define a multi-layer perceptron (MLP) that consist of at least one one hidden layer. Each layer can in turn consist of multiple neurons. \n",
    "\n",
    "In this lab we will create a Feedforward Multilayer network. This will consist of a backpropagation algorithm (SGD with the chain rule) which allow us to calculate partial derivatives in terms of other partial derivatives. The backpropagation algorithm will use sigmoid activation function (binary) between the layers and the final layer to classification uses softmax (multi-class).\n",
    "<center><img src=\"../data/images/MLP.PNG\" width=\"600\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cb6fb6",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14b78c2b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torchvision\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd693c15",
   "metadata": {},
   "source": [
    "# Load dataset\n",
    "The first step is to import the dataset. We will focus on the digits dataset from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e394f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "\n",
    "data = np.asarray(digits.data, dtype='float32')\n",
    "target = np.asarray(digits.target, dtype='int32')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.15, random_state=37)\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395ff737",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_index = 40\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(torchvision.utils.make_grid(torch.from_numpy(digits.images[:sample_index]).unsqueeze(1), normalize=True).permute(1,2,0),cmap=plt.cm.gray_r,interpolation='nearest')\n",
    "plt.title(\"MNIST example data\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6097aa8b",
   "metadata": {},
   "source": [
    "# Implement Neural network\n",
    "This section we will implement a neural network. We will first implement the individual functions to ensure that they are correct and then we will add them to the neural network class before we start training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b5ad86",
   "metadata": {},
   "source": [
    "## Predefined settings\n",
    "Here we define some settings that we will use later. \n",
    "\n",
    "* EPSILON: To avoid instability if probability equals 0. \n",
    "* INPUT_SIZE: Define the shape of our input.\n",
    "* HIDDEN_SIZE: Number of neurons in the hidden layer.\n",
    "* OUTPUT_SIZE: Number of classes for calculate probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c473687c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating data set\n",
    "EPSILON = 1e-6\n",
    "INPUT_SIZE = X_train.shape[1]\n",
    "HIDDEN_SIZE = 10\n",
    "OUTPUT_SIZE = len(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9281650c",
   "metadata": {},
   "source": [
    "## One-hot encoding\n",
    "To differentiate between the classes from the dataset we one-hot encode the labels based on the unique length of the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5f44d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(n_classes, y):\n",
    "    return np.eye(n_classes)[y]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b567af0",
   "metadata": {},
   "source": [
    "## Activation function\n",
    "First task is to define the activation functions. Below is three function that should be implemented. Verify the output based on the given input.\n",
    "\n",
    "1. **TODO:** Implement sigmoid \n",
    "2. **TODO:** Implement dsigmoid which is the element-wise derivitate of sigmoid \n",
    "3. **TODO:** Implement softmax \n",
    "\n",
    "$\n",
    "sigmoid(x) = \\frac{1}{1 + e^{-x}}\n",
    "$\n",
    "\n",
    "$\n",
    "dsigmoid(x) = sigmoid(x) \\cdot (1 - sigmoid(x))\n",
    "$\n",
    "\n",
    "$\n",
    "softmax(\\mathbf{x}) = \\frac{1}{\\sum_{i=1}^{n}{e^{x_i}}}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "  e^{x_1}\\\\\\\\\n",
    "  e^{x_2}\\\\\\\\\n",
    "  \\vdots\\\\\\\\\n",
    "  e^{x_n}\n",
    "\\end{bmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0ea2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    #TODO\n",
    "    return NotImplemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dae47a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dsigmoid(X):\n",
    "    #TODO\n",
    "\n",
    "    return NotImplemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ce9cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    #TODO\n",
    "\n",
    "    return NotImplemented"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362e211f",
   "metadata": {},
   "source": [
    "To test the models. Run the following code and see if the plot is the same as the expected result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8a90c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5, 5, 100)\n",
    "plt.plot(x, sigmoid(x), label='sigmoid')\n",
    "plt.plot(x, dsigmoid(x), label='dsigmoid')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec354ad",
   "metadata": {},
   "source": [
    "### Expected result - sigmoid and dsigmoid\n",
    "<img src=\"data:image/png;base64,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\" width=\"370\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a48471c",
   "metadata": {},
   "source": [
    "### Test Softmax\n",
    "The verification of the softmax we can run the asserts to see if the result are the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddcab68",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[10, 2, -3],\n",
    "              [-1, 5, -20]])\n",
    "\n",
    "assert np.allclose(softmax([10, 2, -3]), [9.99662391e-01, 3.35349373e-04, 2.25956630e-06]), \"Wrong answer!\"\n",
    "assert np.allclose(softmax(X),[[9.99662391e-01, 3.35349373e-04, 2.25956630e-06],[2.47262316e-03, 9.97527377e-01, 1.38536042e-11]]), \"Wrong answer!\"\n",
    "assert (np.sum(softmax(X), axis=1) == [1,1]).all(), \"Probability not equal to 1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ef6455",
   "metadata": {},
   "source": [
    "## Loss function.\n",
    "\n",
    "\n",
    "**TODO:** Implement the negative log likelihood defined as:\n",
    "$L=âˆ’\\frac{1}{n}\\sum_{x}^{n}(ln(a_{y}^{L}))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775631a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(Y_true, Y_pred):\n",
    "    #TODO\n",
    "    \n",
    "    return NotImplemented"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0a3fda",
   "metadata": {},
   "source": [
    "### Test nll\n",
    "The verification of the nll we can run the asserts to see if the result are the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48b24d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert nll([1, 0, 0], [.99, 0.01, 0]) == 0.010049325753001472, \"Wrong output!\"\n",
    "\n",
    "Y_true = np.array([[0, 1, 0],[1, 0, 0],[0, 0, 1]])\n",
    "Y_pred = np.array([[0, 1, 0],[.99, 0.01, 0], [0, 0, 1]])\n",
    "\n",
    "assert nll(Y_true, Y_pred) == 0.0033491085846672117"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfda118",
   "metadata": {},
   "source": [
    "## Feedforward\n",
    "The feedforward function is defined with two things; \n",
    "\n",
    "* $\\mathbf{h} = sigmoid(\\mathbf{W}^h \\mathbf{x} + \\mathbf{b^h})$\n",
    "* $\\mathbf{y} = softmax(\\mathbf{W}^o \\mathbf{h} + \\mathbf{b^o})$\n",
    "\n",
    "The hidden layer will use sigmoid as activation function but for final classification we will use softmax.\n",
    "In case we want to observe the activations we can enable **keep_activations**. If enabled, return; \n",
    "* y (output activations),\n",
    "* h (hidden layer activations),\n",
    "* z_h (weights for hidden layer) \n",
    "otherwise return y.\n",
    "\n",
    "**TODO:** Implement the forward function for $h$, $y$ and $z\\_h$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71b8f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X, keep_activations=False):\n",
    "    # TODO\n",
    "    NotImplemented\n",
    "    \n",
    "    # Predefined and expected return. \n",
    "    if keep_activations:\n",
    "        return y, h, z_h\n",
    "    else:\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469a1ffc",
   "metadata": {},
   "source": [
    "## Init weights\n",
    "Random init of weights $W$. To help the architecture we can define random initiation of weights. \n",
    "\n",
    "* W_h: Weights for the hidden layer \n",
    "* b_h: Bias for the hidden layer \n",
    "* W_o: Weight for the output \n",
    "* b_o: Bias for the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dc9d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_h = np.random.uniform(size=(INPUT_SIZE, HIDDEN_SIZE), high=0.01, low=-0.01)\n",
    "b_h = np.zeros(HIDDEN_SIZE)\n",
    "W_o = np.random.uniform(size=(HIDDEN_SIZE, OUTPUT_SIZE), high=0.01, low=-0.01)\n",
    "b_o = np.zeros(OUTPUT_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6411c7fc",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "To get a reminder of backpropagation take a look at this [explaination](https://dfdazac.github.io/06-neural-networks-numpy.html). Below we summarize the calculation of one node w_{iL}. \n",
    "\n",
    "We have a loss function $\\mathcal{L}(\\theta)$ for the negative log likelihood over a single observation $\\mathcal{L}(\\theta) = -\\log(softmax(W \\dot x + b)) = -z_{kL} + log(Z)$. Observe that we skipped a few steps for this so please take a closer look at the *explaination*.\n",
    "The loss $\\mathcal{L}$  is a function of the quantity $z_{iL}$, which is a function of the weights $w_{iL}$ of unit $i$ in the layer $L$. Therefore, to obtain the gradient of the loss with respect to the weights $w_{iL}$. Therefore, we must use the chain rule to calculate:\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial w_{iL}} = \\frac{\\partial \\mathcal{L}}{\\partial z_{iL}} \\frac{\\partial z_{iL}}{\\partial w_{iL}}$$\n",
    "\n",
    "$\\frac{\\partial \\mathcal{L}}{\\partial w_{iL}}$ Indicates the first derivate and the second derivate through $\\frac{\\partial z_{iL}}{\\partial w_{iL}}$. This give us:\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial w_{iL}} = \\frac{\\partial \\mathcal{L}}{\\partial z_{iL}}a_{L-1}$$\n",
    "\n",
    "\n",
    "This means that for all weight vectors $w_{iL}$, the gradient is obtained by multiplying the activation vector of the previous layer by the derivative \n",
    "$\\frac{\\partial \\mathcal{L}}{\\partial z_{iL}}$\n",
    "\n",
    "If we take the derivatives $\\frac{\\partial \\mathcal{L}}{\\partial z_{iL}}$, for i = 1,...,K, in a column vector $\\frac{\\partial \\mathcal{L}}{\\partial z_{L}}$, then the gradient matrix can be calculated in a compact way using an **outer** product of vectors:\n",
    "\n",
    "$$\\nabla w_{L}\\mathcal{L} = \\frac{\\partial \\mathcal{L}}{\\partial z_{L}}a^{T}_{L-1}$$\n",
    "\n",
    "Now we have the equation derivate for a single node $w_{iL}$\n",
    "\n",
    "**TODO:** Calculate the gradient loss $\\frac{d}{dW} -\\log(softmax(W \\dot x + b))$. Where $x$ is the input and y is the truth values.\n",
    "\n",
    "**TODO:** Implement the gradient loss function for; \n",
    "* grad_W_h - Weight gradient of hidden layer\n",
    "* grad_W_o - Weight gradient of output layer\n",
    "* grad_b_h - Bias gradient of hidden layer\n",
    "* grad_b_o - Bias gradient of output layer\n",
    "\n",
    "**Hint:** Dot product and dsigmoid might prove helpful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab5e314",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_loss(x, y_true):\n",
    "    y, h, z_h = forward(x, keep_activations=True)\n",
    "    # TODO\n",
    "    \n",
    "    grads = {\"W_h\": grad_W_h, \"b_h\": grad_b_h,\n",
    "             \"W_o\": grad_W_o, \"b_o\": grad_b_o}\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436463da",
   "metadata": {},
   "source": [
    "## SGD\n",
    "Uses the grad function output to update $W$ and $b$\n",
    "\n",
    "**TODO:** Implement SGD to update the weights provided from **grad_loss function** with $W_{l} = W_{l}-\\alpha \\nabla w_{l}\\mathcal{L}(\\theta)$ where $\\mathcal{L}(\\theta)$ is the loss function, $\\alpha$ is the learning rate and $w$ is the weight at layer l in L."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fe6ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD(x, y, learning_rate):\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea60dacb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Loss\n",
    "Combine the nll with one-hot encoding and the predicted value from the the forward function to test the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b646de9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss(X, y):\n",
    "    return nll(one_hot(OUTPUT_SIZE, y), forward(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2b2fba",
   "metadata": {},
   "source": [
    "## Predict\n",
    "To predict we calculate the indices of the maximum values along an axis. on the forward pass. However we need to consider if the input is 1D (one input) or 2D (multiple inputs). We therefore check if len(X.shape) == 1, otherwise we extract the indices of the maximum values along axis=1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8579e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X):\n",
    "    return np.argmax(self.forward(X)) if len(X.shape) == 1 else np.argmax(self.forward(X), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b723ae47",
   "metadata": {},
   "source": [
    "## Metric\n",
    "To evaluate the network we are going to use accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc388bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_preds, y_truth):\n",
    "    return np.mean(y_preds == y_truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d74d9e",
   "metadata": {},
   "source": [
    "## Neural network\n",
    "Now to combine all previous steps into this class. Note that the function should now include **self** to access the internal memory of the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd70394",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet():\n",
    "    \"\"\"MLP with 1 hidden layer with a sigmoid activation\"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.W_h = np.random.uniform(size=(input_size, hidden_size), high=0.01, low=-0.01)\n",
    "        self.b_h = np.zeros(hidden_size)\n",
    "        self.W_o = np.random.uniform(size=(hidden_size, output_size), high=0.01, low=-0.01)\n",
    "        self.b_o = np.zeros(output_size)\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def forward(self, X, keep_activations=False):\n",
    "        #TODO\n",
    "    \n",
    "        return NotImplemented\n",
    "\n",
    "    def loss(self, X, y):\n",
    "        return nll(one_hot(self.output_size, y), self.forward(X))\n",
    "\n",
    "    def grad_loss(self, x, y_true):\n",
    "        #TODO\n",
    "    \n",
    "        return NotImplemented\n",
    "\n",
    "    def SGD(self, x, y, learning_rate):\n",
    "        #TODO\n",
    "    \n",
    "        return NotImplemented\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.forward(X)) if len(X.shape) == 1 else np.argmax(self.forward(X), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86eb73bd",
   "metadata": {},
   "source": [
    "## Training\n",
    "From this point everything above should be implemented and therefore we can define our neural network and try to run it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f918374",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNet(INPUT_SIZE, HIDDEN_SIZE, OUTPUT_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14333161",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses, accuracies, accuracies_test = [loss(X_train, y_train)], [accuracy(X_train, y_train)], [accuracy(X_test, y_test)]\n",
    "\n",
    "print(f\"Random init: train loss: {losses[-1]:0.5f}, train acc: {accuracies[-1]:0.3f}, test acc: {accuracies_test[-1]:0.3f}\")\n",
    "\n",
    "for epoch in range(15):\n",
    "    for i, (x, y) in enumerate(zip(X_train, y_train)):\n",
    "        model.SGD(x, y, 0.1)\n",
    "\n",
    "    losses.append(model.loss(X_train, y_train))\n",
    "    accuracies.append(accuracy(model.predict(X_train),y_train))\n",
    "    accuracies_test.append(accuracy(model.predict(X_test),y_test))\n",
    "    \n",
    "    print(f\"Epoch {epoch:d}, train loss: {losses[-1]:0.5f}, train acc: {accuracies[-1]:0.3f}, test acc: {accuracies_test[-1]:0.3f}\", end=\"\\r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca18688c",
   "metadata": {},
   "source": [
    "## Visualize the result\n",
    "With the help of the following plots we can observe the performance of our neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a102c476",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax0,ax1) = plt.subplots(1,2, figsize=(15,5))\n",
    "ax0.plot(losses)\n",
    "ax0.set_title(\"Training loss\");\n",
    "\n",
    "ax1.plot(accuracies, label='train')\n",
    "ax1.plot(accuracies_test, label='test')\n",
    "ax1.set_ylim(0, 1.1)\n",
    "ax1.set_ylabel(\"accuracy\")\n",
    "ax1.legend(loc='best');\n",
    "ax1.set_title(\"Accuracy\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b856d2d",
   "metadata": {},
   "source": [
    "**Question:** Can the result be improved? \\\n",
    "**Question:** How many epochs are reasonable to run?\\\n",
    "**Question:** How does the performance change if we modify the learning rate?\\"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c780c33",
   "metadata": {},
   "source": [
    "## End\n",
    "Now we have sucessfully trained a neural network! Assume it took some time to implement and the complexity would increase if we tried to make the architecture deeper by adding more layers a framework is more suited than manual implementation. Continue to part two to learn an easier way to implement a neural network! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "labs_new",
   "language": "python",
   "name": "labs_new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
