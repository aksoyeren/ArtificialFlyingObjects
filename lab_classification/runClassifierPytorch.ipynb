{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from skimage import io\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import Compose, ToTensor\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a model\n",
    "\n",
    "Here, we have the network model class definition. As defined in the exercises section, your task is to update the network architecture defined in this class such that the network will return the highest accuracy for the given training, validation, and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierDNNModel(nn.Module):\n",
    "    '''\n",
    "    Classifier DNN Class\n",
    "    Values:\n",
    "        input_dim: number of channels in the images\n",
    "        hidden_dim: inner dimension for conv layers\n",
    "        linear_dim: dimenstion for the linear layer\n",
    "        n_classes: number of output classes\n",
    "    '''\n",
    "    def __init__(self, input_dim=(128, 128, 3), hidden_dim=25, linear_dim=2048, n_classes=3, **kwargs):\n",
    "        super(ClassifierDNNModel, self).__init__()\n",
    "\n",
    "        (h, w, c) = input_dim\n",
    "\n",
    "        # Build our model\n",
    "        self.model = nn.Sequential(\n",
    "            # input is           (c) x h      x w\n",
    "            # output is (hidden_dim) x h / 2  x w / 2\n",
    "            self.make_conv_block(c, hidden_dim),\n",
    "            \n",
    "            # input is  (hidden_dim)     x h / 2  x w / 2\n",
    "            # output is (hidden_dim x 2) x h / 4  x w / 4\n",
    "            self.make_conv_block(hidden_dim, hidden_dim * 2),\n",
    "\n",
    "            nn.Flatten(),\n",
    "            # dimensions after flatten layer is the image size * dimensions.\n",
    "            self.make_linear_block(hidden_dim * 2 * h // 4 * w // 4, linear_dim),\n",
    "            self.make_linear_block(linear_dim, n_classes, last_layer=True),\n",
    "        )\n",
    "    \n",
    "    def make_conv_block(self, input_channels, output_channels):\n",
    "        '''\n",
    "        Returns a sequence corresponding to the convolutional layers in our DNN model.\n",
    "        Parameters:\n",
    "            input_channels: number of input channels to this block\n",
    "            output_channels: number of output channels from this block\n",
    "            batch_norm: if batch normalization should be used or not.\n",
    "        '''\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(input_channels, output_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(output_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "    def make_linear_block(self, input_channels, output_channels, last_layer=False):\n",
    "        '''\n",
    "        Returns a sequence corresponding to the linear layers in our DNN model.\n",
    "        Parameters:\n",
    "            input_channels: number of input channels to this block\n",
    "            output_channels: number of output channels from this block\n",
    "            batch_norm: if batch normalization should be used or not.\n",
    "            last_layer: if this is the final layers in our model\n",
    "        '''\n",
    "        if last_layer:\n",
    "            return nn.Sequential(\n",
    "                nn.Linear(input_channels, output_channels, bias=False),\n",
    "                nn.Softmax(dim=1)\n",
    "            )\n",
    "        else:\n",
    "            return nn.Sequential(\n",
    "                nn.Linear(input_channels, output_channels, bias=False),\n",
    "                nn.BatchNorm1d(output_channels),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "\n",
    "    def forward(self, image):\n",
    "        '''\n",
    "        Function for completing a forward pass: Given an image, \n",
    "        returns predicted class.\n",
    "        Parameters:\n",
    "            image: an image tensor with dimension (input_dim)\n",
    "        '''\n",
    "        return self.model(image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup network hyperparameters\n",
    "\n",
    "We import the network hyperparameters and build a simple cnn by calling the class introduced in the previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparam = {\n",
    "    'input_dim': (128, 128, 3),\n",
    "    'hidden_dim': 25,\n",
    "    'linear_dim': 2048,\n",
    "    'n_classes': 3,\n",
    "    'device': 'cuda',\n",
    "    'lr': 0.001,\n",
    "    'n_epochs': 5,\n",
    "}\n",
    "\n",
    "model = ClassifierDNNModel(**hparam).to(hparam['device'])\n",
    "model_opt = torch.optim.Adam(model.parameters(), lr=hparam['lr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ETL\n",
    "\n",
    "Extract, transform load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArtificialFlyingObjectsDataset(Dataset):\n",
    "    '''\n",
    "    Artificial flying objects dataset.\n",
    "    Values:\n",
    "        root_dir: root directory where images can be found.\n",
    "        split: If it should return training, validation, testing split\n",
    "        fineGrained: If we should use the fine grained classes\n",
    "        transforms: List of optional tensorvision transforms.\n",
    "    '''\n",
    "    def __init__(self, root_dir='../data/FlyingObjectDataset_10K', split='training', fineGrained=False, transforms=None):\n",
    "        import glob\n",
    "        self.split = split\n",
    "        self.datadir = root_dir\n",
    "        filenames = glob.glob(f'{root_dir}/{split}/image/*.png')\n",
    "        categories = []\n",
    "        for filename in filenames:\n",
    "            seg = filename.split('_')\n",
    "\n",
    "            if fineGrained:\n",
    "                categories.append(seg[2] + \"_\" + seg[3])\n",
    "            else:\n",
    "                categories.append(seg[2])\n",
    "\n",
    "        self.labels = pd.get_dummies(categories)\n",
    "        self.filenames = list(filenames)\n",
    "        \n",
    "        if transforms:\n",
    "            self.transforms = Compose(transforms.append(ToTensor()))\n",
    "        else:\n",
    "            self.transforms = Compose([ToTensor()])\n",
    "\n",
    "    def __len__(self):\n",
    "        '''\n",
    "        Length of dataset\n",
    "        '''\n",
    "        return len(self.filenames)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        Get individual item.\n",
    "        '''\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = self.filenames[idx]\n",
    "        X = io.imread(img_name)\n",
    "        y = torch.tensor(self.labels.iloc[idx, 0], dtype=torch.long)\n",
    "\n",
    "        X = self.transforms(X)\n",
    "        X = X.clone().detach()\n",
    "        return (X, y)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        head = \"Dataset \" + self.__class__.__name__ + \" : \" + self.split\n",
    "        body = [f\"Number of images: {self.__len__()}\"]\n",
    "        body.append(f\"Categories: {len(set(self.labels))}\")\n",
    "        body.append(f\"Root location: {self.datadir}\")\n",
    "        if hasattr(self, \"transforms\") and self.transforms is not None:\n",
    "            body += [f'Transforms: ' + repr(self.transforms)]\n",
    "        lines = [head] + [\" \" * 4 + line for line in body]\n",
    "        return '\\n'.join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add optional augmentation as a list of transforms\n",
    "# See: https://pytorch.org/docs/stable/torchvision/transforms.html\n",
    "transform = None\n",
    "\n",
    "train_ds = ArtificialFlyingObjectsDataset(transforms=transform)\n",
    "val_ds = ArtificialFlyingObjectsDataset(split='validation', transforms=transform)\n",
    "test_ds = ArtificialFlyingObjectsDataset(split='testing', transforms=transform)\n",
    "\n",
    "train_dl = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    drop_last=True)\n",
    "\n",
    "val_dl = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    drop_last=True)\n",
    "\n",
    "test_dl = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print statistics\n",
    "\n",
    "Print some statistics about the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ArtificialFlyingObjectsDataset : training\n",
      "    Number of images: 10817\n",
      "    Categories: 3\n",
      "    Root location: ../data/FlyingObjectDataset_10K\n",
      "    Transforms: Compose(\n",
      "    ToTensor()\n",
      ")\n",
      "Dataset ArtificialFlyingObjectsDataset : validation\n",
      "    Number of images: 2241\n",
      "    Categories: 3\n",
      "    Root location: ../data/FlyingObjectDataset_10K\n",
      "    Transforms: Compose(\n",
      "    ToTensor()\n",
      ")\n",
      "Dataset ArtificialFlyingObjectsDataset : testing\n",
      "    Number of images: 2220\n",
      "    Categories: 3\n",
      "    Root location: ../data/FlyingObjectDataset_10K\n",
      "    Transforms: Compose(\n",
      "    ToTensor()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(train_ds)\n",
    "print(val_ds)\n",
    "print(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Setup training hparams and execute training of dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bba50280792f4135bcfbf27bbde373ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Train', max=338, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation accuracy 0.51\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3b855b1d3164ca1812b0fa45ccaf907",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Train', max=338, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation accuracy 0.60\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86a44ea8686a4a3abf0992a78565b58f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Train', max=338, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation accuracy 0.54\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dff07693f3447c5b3b85f5fe938f385",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Train', max=338, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation accuracy 0.52\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1181904a42ef4db39d3c74dc39b983ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Train', max=338, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation accuracy 0.59\n",
      "Test accuracy 0.61\n"
     ]
    }
   ],
   "source": [
    "n_epochs = hparam['n_epochs']\n",
    "cur_step = 0\n",
    "mean_loss = 0\n",
    "mean_accuracy = 0\n",
    "device = hparam['device']\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Dataloader returns the batches\n",
    "\n",
    "    model.train()\n",
    "    for x, y in tqdm(train_dl, desc='Train'):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        ## Update model ##\n",
    "        model_opt.zero_grad()\n",
    "        yh = model(x)\n",
    "        loss = loss_fn(yh, y)\n",
    "        loss.backward()\n",
    "        model_opt.step()\n",
    "\n",
    "    # Do verification each epoch\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_dl:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            yh = model(x)\n",
    "            _, predicted = torch.max(yh.data, 1)\n",
    "            total += y.size(0)\n",
    "            correct += (predicted == y).sum().item()\n",
    "    print(f'Validation accuracy {correct / total:.2f}')\n",
    "\n",
    "# Calculate final test accuracy\n",
    "model.eval()\n",
    "total = 0\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for x, y in test_dl:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        yh = model(x)\n",
    "        _, predicted = torch.max(yh.data, 1)\n",
    "        total += y.size(0)\n",
    "        correct += (predicted == y).sum().item()\n",
    "\n",
    "print(f'Test accuracy {correct / total:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
