{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:40px;\"><center>Exercise I:<br> Multi-layer perceptrons\n",
    "for classification and regression problems.\n",
    "</center></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Short summary\n",
    "In this exercise you will: \n",
    "\n",
    "* train MLPs and for both classification and regression problems\n",
    "* overfit models and see how overfitting changes validation performance\n",
    "* avoid overfitting using regularization\n",
    "* do model selection\n",
    "\n",
    "There are 11 questions in this exercise. These 11 questions can be found in five different cells below (see section \"The Different Cells\"). All of the exercises deal with training and evaluation of the multi-layer perceptron (MLP) network. You are going to work with different datasets, binary and multiple classification problems and function approximation problems. For all questions, except the last two ones, code is available that you can run directly or only need to make small modifications to. For the last two questions we only provide a small part of the code and you should provide the rest. However, it is typically just a matter of paste and copy from the previous code cells (in a proper way).\n",
    "\n",
    "You will write the report of the exercise within this notebook. The details of how to do that can be found below in section \"Writing the report\". Finally before you start:\n",
    "\n",
    "\n",
    "## The data\n",
    "There are several datasets in this exercise. \n",
    "\n",
    "### syn1 - syn3\n",
    "Three different synthetic classification problems will be used. They are all 2D binary classification problems which allows for an easy visual inspection of the different classes and the decision boundary implemented by the network. They are called *syn1, syn2* and *syn3*. Each of these datasets are generated \"on the fly\" each time. They come from various normal distributions. Since they are generated using random numbers it means that each time you generate the data it will be slightly different from next time. You can control this by having a fixed *seed* to the random number generator. The cell \"PlotData\" will plot these datasets.\n",
    "\n",
    "### regr1\n",
    "There is also a synthetic regression problem, called *regr1*. It has 6 inputs (independent variables) and one output variable (dependent variable). It is generated according to the following formula:  \n",
    "\n",
    "$\\qquad d = 2x_1 + x_2x_3^2 + e^{x_4} + 5x_5x_6 + 3\\sin(2\\pi x_6) + \\alpha\\epsilon$  \n",
    "    \n",
    "where $\\epsilon$ is added normally distributed noise and $\\alpha$ is a parameter controlling the size of the added noise. Variables $x_1,...,x_4$ are normally distrubuted with zero mean and unit variance, whereas $x_5, x_6$ are uniformly distributed ($[0,1]$). The target value $d$ has a non-linear dependence on ***x***.\n",
    "\n",
    "### Spiral data\n",
    "This is the \"famous\" spiral dataset that consists of two 2-D spirals, one for each class. The perfect classification boundary is also a spiral. The cell \"PlotData\" will plot this dataset.\n",
    "\n",
    "### Japanese vowels dataset\n",
    "*This data set is taken from the UCI Machine Learning Repository [https://archive.ics.uci.edu/ml/datasets/Japanese+Vowels].* In short, nine male speakers uttered two Japanese vowels /ae/ successively. For each utterance, a discrete times series was produced where each time point consists of 12 (LPC cepstrum) coefficients. The length of each time series was between 7-29. \n",
    "Here we treat each point of the time series as a feature (12 inputs). In total we have 9961\n",
    "data points which then has been divided into 4274 for training, 2275 for validation and 3412 for test. The original data files are provided as *ae.train* and *ae.test*. The task is now based on a single sample value of one of the speakers, determine which speaker it was. This is, in summary, a 9-class classification problem with 12 input values for each case.\n",
    "\n",
    "## The exercises\n",
    "There are 11 questions, where the first 5 questions are dealing with 2D binary classification problems. Here you will be able to see the boundary implemented by the different MLPs that you train. Questions 6-9 deals with training a regression network for the *regr1* dataset. Here you are also going to use regularization to \"combat\" overtraining. Question 10 is the Japanese vowels classification problem, here your task is to come up with a model that optimizes the validation result. Finally, the last exercise is to find a model that can solve the spiral problem.\n",
    "\n",
    "## The different 'Cells'\n",
    "This notebook contains several cells with python code, together with the markdown cells (like this one) with only text. Each of the cells with python code has a \"header\" markdown cell with information about the code. The table below provides a short overview of the code cells. \n",
    "\n",
    "| #  |  CellName | CellType | Comment |\n",
    "| :--- | :-------- | :-------- | :------- |\n",
    "| 1 | Init | Needed | Sets up the environment|\n",
    "| 2 | MLP | Needed | Defines the MLP model |\n",
    "| 3 | Data | Needed | Defines the functions to generate the artificial datasets |\n",
    "| 4 | PlotData | Information | Plots the 2D classification datasets |\n",
    "| 5 | Statistics | Needed | Defines the functions that calculates various performance measures |\n",
    "| 6 | Boundary | Needed | Function that can show 2D classification boundaries | \n",
    "| 7 | Confusion | Needed | Function that plots the confusion matrix | \n",
    "| 8 | Ex1 | Exercise | For question 1 |\n",
    "| 9 | Ex2 | Exercise | For question 2-5 |\n",
    "| 10 | Ex3 | Exercise | For question 6-9 |\n",
    "| 11 | Ex4 | Exercise | For question 10 |\n",
    "| 12 | Ex5 | Exercise | For question 11 |\n",
    "\n",
    "In order for you to start with the exercise you need to run all cells with the celltype \"Needed\". The very first time you start with this exercise we suggest that you enter each of the needed cells, read the cell instruction and run the cell. It is important that you do this in the correct order, starting from the top and work you way down the cells. Later when you have started to work with the notebook it may be easier to use the command \"Run All\" found in the \"Cell\" dropdown menu.\n",
    "\n",
    "## Writing the report\n",
    "First the report should be written within this notebook. We have prepared the last cell in this notebook for you where you should write the report. The report should contain 4 parts:\n",
    "\n",
    "* Name:\n",
    "* Introduction: A **few** sentences where you give a small introduction of what you have done in the lab.\n",
    "* Answers to questions: For each of the questions provide an answer. It can be short answers or a longer ones depending on the nature of the questions, but try to be effective in your writing.\n",
    "* Conclusion: Summarize your findings in a few sentences.\n",
    "\n",
    "\n",
    "## Last but not least\n",
    "Have fun!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CellName: Init (#1)\n",
    "### CellType: Needed\n",
    "### Cell instruction: Initializing the libraries\n",
    "\n",
    "In the cell below, we import all the libraries that are needed for this exercises. There are two configuration parameters that you can change in this cell\n",
    "\n",
    "* The size of the plots\n",
    "* Inline or \"pop out\" plots.\n",
    "\n",
    "See comments in the cell for more information. Default is inline plots with a \"lagom\" size. Run the cell by entering into the cell and press \"CTRL Enter\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND']='tensorflow'\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.layers import Dense, Input\n",
    "from keras.models import Model\n",
    "from keras import metrics, regularizers\n",
    "\n",
    "from sklearn.metrics import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "\n",
    "# The size of the plots.\n",
    "mpl.rcParams['figure.figsize'] = (5,5)\n",
    "\n",
    "\n",
    "# To have the plots inside the notebook \"inlin\" should be True. \n",
    "# If \"inlin\" = False, then plots will pop out of the notebook\n",
    "inlin = True # True/False\n",
    "if inlin:\n",
    "    %matplotlib inline\n",
    "else:\n",
    "    %matplotlib \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CellName: MLP (#2)\n",
    "### CellType: Needed\n",
    "### Cell instruction: Defining the MLP model\n",
    "\n",
    "This cell defines the MLP model. There are a number of parameters that is needed to \n",
    "define a model. Here is a list of them: **Note:** They can all be specified when you call\n",
    "this function in later cells. The ones specified in this cell are the default values.\n",
    "\n",
    "* inp_dim: the input dimension (integer)\n",
    "\n",
    "* n_nod: size of the network, eg [5] for a one hidden layer with 5 nodes and [5,3] for a two layer network with 5 and 3 hidden nodes each.\n",
    "\n",
    "* act_fun: the activation function. Most common are\n",
    "    * 'linear'\n",
    "    * 'relu'\n",
    "    * 'tanh'\n",
    "    * 'sigmoid'\n",
    "        \n",
    "* out_act_fun: the activation function for the output nodes. Most common are\n",
    "    * 'linear'\n",
    "    * 'sigmoid'\n",
    "    * 'softmax'\n",
    "    \n",
    "* opt_method: The error minimization method. Common choices\n",
    "    * 'SGD'\n",
    "    * 'Adam'\n",
    "    * 'Nadam'\n",
    "    * 'RMSprop'\n",
    "    \n",
    "* cost_fun: The error function used during training. There are three common ones\n",
    "    * 'mean_squared_error'\n",
    "    * 'binary_crossentropy'\n",
    "    * 'categorical_crossentropy'\n",
    "\n",
    "* lr_rate: The learning rate. Note some of the minimization methods uses a dynamical learning rate. In such a cases this value sets the initial learning rate.\n",
    "\n",
    "* lambd: L2 regularization parameter\n",
    "\n",
    "* num_out: The number of output nodes\n",
    "\n",
    "Run the cell by entering into the cell and press \"CTRL Enter\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipline(inp_dim,\n",
    "            n_nod,\n",
    "            act_fun = 'relu',\n",
    "            out_act_fun = 'sigmoid',\n",
    "            opt_method = 'Adam',\n",
    "            cost_fun = 'binary_crossentropy',\n",
    "            lr_rate = 0.01, \n",
    "            lambd = 0.0, \n",
    "            num_out = None):\n",
    "    \n",
    "    lays = [inp_dim] + n_nod\n",
    "    \n",
    "    main_input = Input(shape=(inp_dim,), dtype='float32', name='main_input')\n",
    "    \n",
    "    X = main_input\n",
    "    for nod in n_nod:\n",
    "        X = Dense(nod, \n",
    "                  activation = act_fun,\n",
    "                  kernel_regularizer=regularizers.l2(lambd))(X)\n",
    "        \n",
    "    output = Dense(num_out, activation = out_act_fun )(X)\n",
    "    \n",
    "    method = getattr(keras.optimizers, opt_method)\n",
    "    \n",
    "    model =  Model(inputs=[main_input], outputs=[output])\n",
    "    model.compile(optimizer = method(lr = lr_rate),\n",
    "                  loss = cost_fun)\n",
    "                  #metrics=['accuracy', 'mse'])   \n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CellName: Data (#3)\n",
    "### CellType: Needed\n",
    "### Cell instruction: Defining synthetic data sets\n",
    "\n",
    "This cell defines the three different synthetic data sets and the regression dataset. It also provides functions for reading the Vowles dataset and the Spiral data. Run the cell by entering into the cell and press \"CTRL Enter\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def syn1(N):\n",
    "    \"\"\" data(samples, features)\"\"\"\n",
    "    \n",
    "    global seed\n",
    "    \n",
    "    data = np.empty(shape=(N,2), dtype = np.float32)  \n",
    "    tar = np.empty(shape=(N,), dtype = np.float32) \n",
    "    N1 = int(N/2)\n",
    "  \n",
    "    data[:N1,0] = 4 + np.random.normal(loc=.0, scale=1., size=(N1))\n",
    "    data[N1:,0] = -4 + np.random.normal(loc=.0, scale=1., size=(N-N1))\n",
    "    data[:,1] = 10*np.random.normal(loc=.0, scale=1., size=(N))\n",
    "    \n",
    "    \n",
    "    data = data / data.std(axis=0)\n",
    "    \n",
    "    # Target\n",
    "    tar[:N1] = np.ones(shape=(N1,))\n",
    "    tar[N1:] = np.zeros(shape=(N-N1,))\n",
    "    \n",
    "    # Rotation\n",
    "    theta = np.radians(30)\n",
    "    c, s = np.cos(theta), np.sin(theta)\n",
    "    R = np.array([[c,-s],[s,c]]) # rotation matrix\n",
    "    data = np.dot(data,R) \n",
    "    \n",
    "    return data,tar\n",
    "\n",
    "\n",
    "def syn2(N):\n",
    "    \"\"\" data(samples, features)\"\"\"\n",
    "    \n",
    "    global seed\n",
    "     \n",
    "    data = np.empty(shape=(N,2), dtype = np.float32)  \n",
    "    tar = np.empty(shape=(N,), dtype = np.float32) \n",
    "    N1 = int(N/2)\n",
    "\n",
    "    # Positive samples\n",
    "    data[:N1,:] = 0.8 + np.random.normal(loc=.0, scale=1., size=(N1,2))\n",
    "    # Negative samples \n",
    "    data[N1:,:] = -.8 + np.random.normal(loc=.0, scale=1., size=(N-N1,2))\n",
    "    \n",
    "    \n",
    "    # Target\n",
    "    tar[:N1] = np.ones(shape=(N1,))\n",
    "    tar[N1:] = np.zeros(shape=(N-N1,))\n",
    "\n",
    "    return data,tar\n",
    "\n",
    "\n",
    "def syn3(N):\n",
    "    \"\"\" data(samples, features)\"\"\"\n",
    "\n",
    "    global seed\n",
    "    \n",
    "    data = np.empty(shape=(N,2), dtype = np.float32)  \n",
    "    tar = np.empty(shape=(N,), dtype = np.float32) \n",
    "    N1 = int(2*N/3)\n",
    "    \n",
    "    # disk\n",
    "    teta_d = np.random.uniform(0, 2*np.pi, N1)\n",
    "    inner, outer = 2, 5\n",
    "    r2 = np.sqrt(np.random.uniform(inner**2, outer**2, N1))\n",
    "    data[:N1,0],data[:N1,1] = r2*np.cos(teta_d), r2*np.sin(teta_d)\n",
    "        \n",
    "    #circle\n",
    "    teta_c = np.random.uniform(0, 2*np.pi, N-N1)\n",
    "    inner, outer = 0, 3\n",
    "    r2 = np.sqrt(np.random.uniform(inner**2, outer**2, N-N1))\n",
    "    data[N1:,0],data[N1:,1] = r2*np.cos(teta_c), r2*np.sin(teta_c)\n",
    "    \n",
    "    # Normalization\n",
    "    #data = data - data.mean(axis=0)/data.std(axis=0)\n",
    "\n",
    "    tar[:N1] = np.ones(shape=(N1,))\n",
    "    tar[N1:] = np.zeros(shape=(N-N1,))\n",
    "    \n",
    "    return data, tar\n",
    "\n",
    "\n",
    "def regr1(N, v=0):\n",
    "    \"\"\" data(samples, features)\"\"\"\n",
    "\n",
    "    global seed\n",
    "\n",
    "    data = np.empty(shape=(N,6), dtype = np.float32)  \n",
    "    \n",
    "    uni = lambda n : np.random.uniform(0,1,n)\n",
    "    norm = lambda n : np.random.normal(0,1,n)\n",
    "    noise =  lambda  n : np.random.normal(0,1,n)\n",
    "    \n",
    "    \n",
    "    for i in range(4):\n",
    "        data[:,i] = norm(N)\n",
    "    for j in [4,5]:\n",
    "        data[:,j] = uni(N)\n",
    "    \n",
    "    tar =   2*data[:,0] + data[:,1]* data[:,2]**2 + np.exp(data[:,3]) + \\\n",
    "            5*data[:,4]*data[:,5]  + 3*np.sin(2*np.pi*data[:,5])\n",
    "    std_signal = np.std(tar)\n",
    "    tar = tar + v * std_signal * noise(N)\n",
    "        \n",
    "    return data, tar\n",
    "\n",
    "def spiral():\n",
    "    tmp = np.loadtxt(\"spiral.dat\")\n",
    "    data, tar = tmp[:, :2], tmp[:, 2]\n",
    "\n",
    "    return data, tar\n",
    "\n",
    "def vowels():\n",
    "    \n",
    "    def pre_proc(file_name):\n",
    "        block = []\n",
    "        x = []\n",
    "    \n",
    "        with open(file_name) as file:\n",
    "            for line in file:    \n",
    "                if line.strip():\n",
    "                    numbers = [float(n) for n in line.split()]\n",
    "                    block.append(numbers)\n",
    "                else:\n",
    "                    x.append(block)\n",
    "                    block = []\n",
    "                \n",
    "        ################################\n",
    "        x = [np.asarray(ar) for ar in x]    \n",
    "        return x\n",
    "\n",
    "    x_train = pre_proc('ae.train')\n",
    "    x_test = pre_proc('ae.test')\n",
    "\n",
    "    \n",
    "    ############## LABELS###########\n",
    "    chunk1 = list(range(30,270, 30))\n",
    "    y_train = []\n",
    "    person = 0\n",
    "\n",
    "    for i, block in enumerate(x_train):\n",
    "        if i in chunk1:\n",
    "            person += 1\n",
    "        y_train.extend([person]*block.shape[0])\n",
    "        \n",
    "    chunk2 = [31,35,88,44,29,24,40,50,29]\n",
    "    chunk2 = np.cumsum(chunk2)\n",
    "    y_test = []\n",
    "    person = 0\n",
    "    for i, block in enumerate(x_test):\n",
    "        if i in chunk2:\n",
    "            person += 1\n",
    "        y_test.extend([person]*block.shape[0])\n",
    "\n",
    "    x_train = np.vstack(x_train)\n",
    "    x_test = np.vstack(x_test)\n",
    "    \n",
    "    ## Split into train, validation and test\n",
    "    num_classes = 9\n",
    "    y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "    y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    x_test, x_val, y_test, y_val = train_test_split(x_test, y_test, test_size=0.4, random_state=42)\n",
    "\n",
    "    return x_train, y_train, x_val, y_val, x_test, y_test\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CellName: PlotData (#4)\n",
    "### CellType: Information\n",
    "### Cell instruction: Plotting the data\n",
    "\n",
    "Here we just generate 100 cases for syn1-syn3 and the spiral dataset and plot them. Run the cell by entering into the cell and press \"CTRL Enter\". \n",
    "\n",
    "**Note!** This cell is not needed for the actual exercises, it is just to visualize the four different 2D synthetic classification data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# seed = 0 means random, seed > 0 means fixed\n",
    "seed = 0\n",
    "np.random.seed(seed) if seed else None\n",
    "\n",
    "d,t = syn1(100)\n",
    "plt.figure(1)\n",
    "plt.scatter(d[:,0],d[:,1], c=t)\n",
    "\n",
    "d,t = syn2(100)\n",
    "plt.figure(2)\n",
    "plt.scatter(d[:,0],d[:,1], c=t)\n",
    "\n",
    "d,t = syn3(100)\n",
    "plt.figure(3)\n",
    "plt.scatter(d[:,0],d[:,1], c=t)\n",
    "\n",
    "d,t = spiral()\n",
    "plt.figure(4)\n",
    "plt.scatter(d[:,0],d[:,1], c=t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CellName: Statistics (#5)\n",
    "### CellType: Needed\n",
    "### Cell instruction: Present result for both classification and regression problems\n",
    "\n",
    "This cell defines two functions that we are going to call using a trained model to calculate both error and performance measures. Run the cell by entering into the cell and press \"CTRL Enter\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats_class(x = None, y = None, label = 'Training', modl = None):\n",
    "    \"\"\"\n",
    "    input :  \n",
    "             x = input\n",
    "             y = output\n",
    "             label = \"Provided text string\"\n",
    "             modl = the model\n",
    "             \n",
    "    output : \n",
    "             sensitivity = fraction of correctly classified positive cases\n",
    "             specificity = fraction of correctly classified negative cases\n",
    "             accuracy = fraction of correctly classified cases\n",
    "             loss = typically the cross-entropy error\n",
    "    \"\"\"\n",
    "    \n",
    "    def binary(y1):\n",
    "        y1[y1>.5] = 1.\n",
    "        y1[y1<= .5] = 0.        \n",
    "        return y1\n",
    "\n",
    "    y_pr = modl.predict(x, batch_size = x.shape[0], verbose=0).reshape(y.shape)\n",
    "                \n",
    "    nof_p, tp, nof_n, tn = [np.count_nonzero(k) for k in [y==1, y_pr[y==1.] > 0.5, y==0, y_pr[y==0.]<= 0.5]]\n",
    "    \n",
    "    sens = tp / nof_p\n",
    "    spec = tn / nof_n\n",
    "    acc = (tp + tn) / (len(y))\n",
    "    loss = modl.evaluate(x, y , batch_size =  x.shape[0], verbose=0)\n",
    "                \n",
    "    A = ['Accuracy', 'Sensitivity', 'Specificity', 'Loss']\n",
    "    B = [acc, sens, spec, loss]\n",
    "    \n",
    "    print('\\n','#'*10,'STATISTICS for {} Data'.format(label), '#'*10, '\\n')\n",
    "    for r in zip(A,B):\n",
    "         print(*r, sep = '   ')\n",
    "    return print('\\n','#'*50)\n",
    "\n",
    "def stats_reg(d = None, d_pred = None, label = 'Training', estimat = None):\n",
    "    \n",
    "    A = ['MSE', 'CorrCoeff']\n",
    "    \n",
    "    pcorr = np.corrcoef(d, d_pred)[1,0]\n",
    "    \n",
    "    if label.lower() in ['training', 'trn', 'train']:\n",
    "        mse = estimat.history['loss'][-1]\n",
    "    else:\n",
    "        mse = estimat.history['val_loss'][-1] \n",
    "\n",
    "    B = [mse, pcorr]\n",
    "    \n",
    "    print('\\n','#'*10,'STATISTICS for {} Data'.format(label), '#'*10, '\\n')\n",
    "    for r in zip(A,B):\n",
    "         print(*r, sep = '   ')\n",
    "    return print('\\n','#'*50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CellName: Confusion (#6)\n",
    "### CellType: Needed\n",
    "### Cell Instruction: Plot the confusion matrix\n",
    "\n",
    "This cell defines the function that plots the confusion matrix. A confusion matrix is a summary of the predictions made by a classifier. Each column of the matrix represents the instances of the predicted class while each row represents the instances of the actual class. Run the cell by entering into the cell and press \"CTRL Enter\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True):\n",
    "    \"\"\"\n",
    "    Citiation\n",
    "    ---------\n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "\n",
    "    \"\"\"\n",
    "    import itertools\n",
    "\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CellName: Boundary (#7)\n",
    "### CellType: Needed\n",
    "### Cell Instruction: Decision boundary\n",
    "\n",
    "This cell defines the function to plot the decision boundary for a 2D input binary MLP classifier. Run the cell by entering into the cell and press \"CTRL Enter\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_b(X = None, Y1 = None ):\n",
    "    \n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    # grid stepsize\n",
    "    h = 0.025\n",
    "\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    Z[Z>.5] = 1\n",
    "    Z[Z<= .5] = 0\n",
    "\n",
    "    Y_pr = model.predict(X, batch_size = X.shape[0], verbose=0).reshape(Y1.shape)\n",
    "  \n",
    "    Y = np.copy(Y1)\n",
    "    Y_pr[Y_pr>.5] = 1\n",
    "    Y_pr[Y_pr<= .5] = 0\n",
    "    Y[(Y!=Y_pr) & (Y==0)] = 2\n",
    "    Y[(Y!=Y_pr) & (Y==1)] = 3\n",
    "    \n",
    "    \n",
    "    plt.figure()\n",
    "    #plt.contourf(xx, yy, Z, cmap=plt.cm.PRGn, alpha = .9) \n",
    "    plt.contour(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "    \n",
    "    \n",
    "    plt.scatter(X[:, 0][Y==1], X[:, 1][Y==1], marker='+', c='k')\n",
    "    plt.scatter(X[:, 0][Y==0], X[:, 1][Y==0], marker='o', c='k')\n",
    "       \n",
    "    plt.scatter(X[:, 0][Y==3], X[:, 1][Y==3], marker = '+', c='r')   \n",
    "    plt.scatter(X[:, 0][Y==2], X[:, 1][Y==2], marker = 'o', c='r')\n",
    "    \n",
    "    \n",
    "    plt.ylabel('x2')\n",
    "    plt.xlabel('x1')\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "End of \"Needed\" and \"Information\" cells. Below are the cells for the actual exercise.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CellName: Ex1 (#8)\n",
    "### CellType: Exercise\n",
    "### Cell instruction: Instruction for question 1\n",
    "\n",
    "The cell below should be used for question 1. You can run the cell as it is (i.e. CTRL-Return). However, looking at the code will help you understand how the network is created, trained and evaluated. It will be useful for the other questions.\n",
    "\n",
    "#### Question 1\n",
    "\n",
    "Use synthetic data 1 (syn1) (100 data points) and train a linear MLP to separate the two classes, i.e. use a single hidden node. **Why can you solve this problem with a single hidden node?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# seed = 0 means random, seed > 0 means fixed\n",
    "seed = 0\n",
    "np.random.seed(seed) if seed else None\n",
    "    \n",
    "# Generate training data\n",
    "x_train, d_train = syn1(100)\n",
    "\n",
    "# Define the network, cost function and minimization method\n",
    "INPUT = {'inp_dim': x_train.shape[1],         \n",
    "         'n_nod': [1],                      # number of nodes in hidden layer\n",
    "         'act_fun': 'tanh',                 # activation functions for the hidden layer\n",
    "         'out_act_fun': 'sigmoid',          # output activation function\n",
    "         'opt_method': 'sgd',               # minimization method\n",
    "         'cost_fun': 'binary_crossentropy', # error function\n",
    "         'lr_rate': 0.1,                    # learningrate\n",
    "         'num_out' : 1 }              # if binary --> 1 |  regression--> num inputs | multi-class--> num of classes\n",
    "\n",
    "# Get the model\n",
    "model = pipline(**INPUT)\n",
    "\n",
    "# Print a summary of the model\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "estimator = model.fit(x_train, d_train,\n",
    "                      epochs = 300,                     # Number of epochs\n",
    "                      #validation_data=(x_val, y_val),  # We don't have any validation dataset!\n",
    "                      batch_size = x_train.shape[0],    # Use batch learning\n",
    "                      #batch_size=25,                   \n",
    "                      verbose = 0)\n",
    "\n",
    "# Call the stats function to print out statistics for the training\n",
    "stats_class(x_train, d_train, 'Training', model)\n",
    "\n",
    "# Some plotting\n",
    "plt.plot(estimator.history['loss'])\n",
    "plt.title('Model training')\n",
    "plt.ylabel('training error')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'], loc=0)\n",
    "plt.show()\n",
    "\n",
    "# Show the decision boundary\n",
    "decision_b(x_train, d_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CellName: Ex2 (#9)\n",
    "### CellType: Exercise\n",
    "### Cell instruction: Instruction for question 2-5\n",
    "\n",
    "The cell below should be used for questions 2-5. For question 2 you can run the cell as it is (i.e. CTRL-Return). For the other questions you need to modify the cell in order to change data set, vary the size of the network etc. There are brief comments in the code that will guide you here.\n",
    "\n",
    "From now on we will talk about *performance*! It can be performance of a trained model on the training dataset or the performance on the validation dataset. What do we mean by performance?  For classification problems we will provide 4 different measurements as returned by a call to the *stats_class* function. They are:\n",
    "* Sensitivity = fraction of correctly classified \"1\" cases\n",
    "* Specificity = fraction of correctly classified \"0\" cases\n",
    "* Accuracy = fraction of correctly classified cases\n",
    "* loss = cross-entropy error (so low loss means good performance!)\n",
    "\n",
    "Our suggestion for you is to either use accuracy or loss as your performance measure.\n",
    "\n",
    "#### Question 2\n",
    "Here you are going to train a classifier for the *syn2* dataset. You are also going to use a validation dataset as an estimate of the *true* performance. Since we generate these datasets we can allow for a relatively large validation dataset in order to get a more accurate estimation of *true* performance. The default value in the cell is to generate 1000 validation datapoints. \n",
    "\n",
    "Now, use synthetic data 2 (syn2)(100 training data points) and train a *linear* MLP to separate the two classes, i.e. use a single hidden node. **What is the performance you get on the validation dataset?** Note: Use a fixed random seed for this exercise since you will compare with runs in the next question.\n",
    "\n",
    "#### Question 3\n",
    "You are now going to overtrain the MLP! By increasing the number of hidden nodes we should be able to get better and better training performance. **How many hidden nodes do you need to reach an accuracy >95% on your training dataset?**\n",
    "\n",
    "**Hint:** Overtraining here often means finding good local minimum of the error function, which may require some tuning of the learning parameters. This means that you may have to change the learning rate, increase the number of epochs and use \"better\" minimization methods. Even though we have not yet talked about the *Adam* minimization method, it is generally better than vanilla *stochastic gradient descent*. It is therefore used in the cells below as the default minimizer. Also you may want to change the size of the \"batch_size\" parameter. It is by default using all data.\n",
    "\n",
    "#### Question 4\n",
    "The effect of overtraining can be monitored by looking at the validation performance. **(a) When you overtrained in the previous question, how much much did the validation *loss* increase, compared to the linear model of Q2?** **(b) What is the optimal number of hidden nodes for the syn2 dataset in order to maximize your validation performance?** \n",
    "\n",
    "#### Question 5\n",
    "Now you are going to use the *syn3* dataset. So, use **150** training datapoints from the synthetic dataset 3 and train an MLP to separate the two classes. Also use about 1000 datapoints for validation. **How many hidden nodes do you need to find a reasonable solution to the problem?  Extra: Can you figure out why this many?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# seed = 0 means random, seed > 0 means fixed\n",
    "seed = 3\n",
    "np.random.seed(seed) if seed else None\n",
    "\n",
    "# Generate training data\n",
    "x_train, d_train = syn2(100)\n",
    "x_val, d_val = syn2(1000)\n",
    "\n",
    "# Define the network, cost function and minimization method\n",
    "INPUT = {'inp_dim': x_train.shape[1],         \n",
    "         'n_nod': [1],                      # number of nodes in hidden layer\n",
    "         'act_fun': 'tanh',                 # activation functions for the hidden layer\n",
    "         'out_act_fun': 'sigmoid',          # output activation function\n",
    "         'opt_method': 'adam',               # minimization method\n",
    "         'cost_fun': 'binary_crossentropy', # error function\n",
    "         'lr_rate': 0.07 ,                    # learningrate\n",
    "         'num_out' : 1 }              # if binary --> 1 |  regression--> num inputs | multi-class--> num of classes\n",
    "\n",
    "# Get the model\n",
    "model = pipline(**INPUT)\n",
    "\n",
    "# Print a summary of the model\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "estimator = model.fit(x_train, d_train,\n",
    "                      epochs = 1000,      \n",
    "                      validation_data=(x_val, d_val),\n",
    "                      batch_size = x_train.shape[0],    # Batch size = all data (batch learning)\n",
    "                      #batch_size=50,                   # Batch size for true SGD\n",
    "                      verbose = 0)\n",
    "\n",
    "# Call the stats function to print out statistics for classification problems\n",
    "stats_class(x_train, d_train, 'Training', model)\n",
    "stats_class(x_val, d_val, 'Validation', model)\n",
    "\n",
    "# Plot the learning curves\n",
    "plt.plot(estimator.history['loss'])\n",
    "plt.plot(estimator.history['val_loss'])\n",
    "plt.title('Model training')\n",
    "plt.ylabel('training error')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc=0)\n",
    "plt.show()\n",
    "\n",
    "# Show the decision boundary for the training dataset\n",
    "decision_b(x_train, d_train)\n",
    "\n",
    "# If you uncomment this one you will see how the decsion boundary is with respect to the validation data\n",
    "#decision_b(x_val, d_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CellName: Ex3 (#10)\n",
    "### CellType: Exercise\n",
    "### Cell instruction: Instruction for question 6-9\n",
    "\n",
    "Now we are going to look at a regression problem. The data as described above (regr1) consists of 6 inputs (features) and one output (target) value. As for previous examples a new data set is generated each time you call the *regr1* function. To get exactly the same data set between different calls, use a fixed seed. New for this problem is that one can also control the amount of noise added to the target value. We are going to use a relatively small training dataset (~250) and a larger validation dataset (~1000) to get a more robust estimation of the generalization performance. For regression problems we also need new performance measures. The *stats_reg* function will give you two such measures:\n",
    "* MSE = mean squared error (low error mean good performance)\n",
    "* CorrCoeff = Pearson correlation coefficient for the scatter plot between predicted and true values.\n",
    "\n",
    "The cell below can be used as an template for all questions regarding this regression problem.\n",
    "\n",
    "#### Question 6\n",
    "Use 250 data points for training and about 1000 for validation and **no** added noise. Train an MLP to predict the target output. If you increase the complexity of the model (e.g. number of hidden nodes) you should be able to reach a very small training error. You will also most likely see that the validation error decreases as you increase the complexity or at least no clear sign of overtraining. **Even though the validation error is most likely still larger than the training error why do we not see any overtraining of the model? (Hint: What is it that typically causes overfitting?)**\n",
    "\n",
    "**Note:** As with previous examples you may need to tune the optimization parameters to make sure that you have \"optimal\" training. That is, increase or decrease the learningrate, possibly train longer times (increase *epochs*) and change the *batch_size* parameter.\n",
    "\n",
    "#### Question 7\n",
    "Use the same training and validation data sets as above, but add 0.4 units of noise (set the second parameter when calling the *regr1* function to 0.4 for both training and validation). Now train again, starting with a \"small\" model and increase the number of hidden nodes as you monitor the validation result for each model. **How many nodes do you have for opitimal validation performance, i.e. more hidden nodes results in overtraining?** Make a note of the validation error you obtained a this point!\n",
    "\n",
    "#### Question 8\n",
    "Instead of using the number of hidden nodes to control the complexity it is often better to use a regularization term added to the error function. You are now going to control the complexity by adding a *L2* regularizer (see the \"INPUT\" dictionary in the cell). You should modify this value until you find the \"near optimal\" validation performance. Use 15 hidden nodes. **Give the L2 value that you found to give \"optimal\" validation performance and compare with the result from  question 7 (optimal performance).**\n",
    "\n",
    "#### Question 9 \n",
    "**Summarize your findings in a few sentences.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# seed = 0 means random, seed > 0 means fixed\n",
    "seed = 10\n",
    "np.random.seed(seed) if seed else None\n",
    "\n",
    "# Generate training and validation data\n",
    "x_train, d_train = regr1(250, 0.0) # 250 data points with no noise\n",
    "x_val, d_val = regr1(1000, 0.0)\n",
    "\n",
    "# Here we need to normalize the target values\n",
    "norm_m = d_train.mean(axis=0)\n",
    "norm_s = d_train.std(axis=0)\n",
    "d_train = (d_train - norm_m) / norm_s\n",
    "\n",
    "# We use the same normalization for the validation data.\n",
    "d_val = (d_val - norm_m) / norm_s\n",
    "\n",
    "\n",
    "# Define the network, cost function and minimization method\n",
    "INPUT = {'inp_dim': x_train.shape[1],         \n",
    "         'n_nod': [3],                      # number of nodes in hidden layer\n",
    "         'act_fun': 'tanh',                 # activation functions for the hidden layer\n",
    "         'out_act_fun': 'linear',           # output activation function\n",
    "         'opt_method': 'adam',               # minimization method\n",
    "         'cost_fun': 'mse',                 # error function\n",
    "         'lr_rate': 0.01,                   # learningrate\n",
    "         'lambd' : 0.0,                    # L2\n",
    "         'num_out' : 1 }   # if binary --> 1 |  regression--> num output | multi-class--> num of classes\n",
    "\n",
    "# Get the model\n",
    "model = pipline(**INPUT)\n",
    "\n",
    "# Print a summary of the model\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "estimator = model.fit(x_train, \n",
    "                      d_train,\n",
    "                      epochs = 1000,      # Number of epochs\n",
    "                      validation_data=(x_val,d_val),\n",
    "                      #batch_size = x_train.shape[0],   # Batch size = all data (batch learning)\n",
    "                      batch_size=50,                    # Batch size for true SGD\n",
    "                      verbose = 0)\n",
    "\n",
    "# Call the stats function to print out statistics for classification problems\n",
    "pred_trn = model.predict(x_train).reshape(d_train.shape)\n",
    "pred_val = model.predict(x_val).reshape(d_val.shape)\n",
    "stats_reg(d_train, pred_trn, 'Training', estimator)\n",
    "stats_reg(d_val, pred_val, 'Validation', estimator)\n",
    "\n",
    "# Scatter plots of predicted and true values\n",
    "plt.figure()\n",
    "plt.plot(d_train, pred_trn, 'g*', label='Predict vs True (Training)')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(d_val, pred_val, 'b*', label='Predict vs True (Validation)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot the learning curves\n",
    "plt.figure()\n",
    "plt.plot(estimator.history['loss'], label='Training')\n",
    "plt.plot(estimator.history['val_loss'], label='Validation')\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CellName: Ex4 (#11)\n",
    "### CellType: Exercise\n",
    "### Cell instruction: Instruction for question 10\n",
    "\n",
    "For this exercise you are given a classification problem with a fixed training-, validation- and test dataset. The data is the Japanse vowels dataset described in the first cell. Your task is to do model selection, coming up with your optimal MLP architecture together with the hyperparameters you used. We do not provide any python code for this question, only the small part that reads the data (next code cell). There is an additional code cell that can be used to print the confusion matrix for the test data (see comments within that cell). (You can modify this function if you want to compute this matrix for both training and validation.)\n",
    "\n",
    "#### Question 10\n",
    "**Present an MLP with associated hyperparameters that maximizes the validation performance and give the test performance you obtained.**\n",
    "\n",
    "**Hint 1:** \n",
    "Remember to check if input data needs to be normalized. See the \"Ex3\" cell how this was done (in that case for the target data).\n",
    "\n",
    "**Hint 2:** \n",
    "This problem is a 9-class classification problem, meaning that you should use a specific output activation function (*out_act_fun*) and a specific loss/error function (*cost_fun*).\n",
    "\n",
    "**Hint 3:**\n",
    "The function *stats_class* function does not work here since it is designed for binary classification problems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# seed = 0 means random, seed > 0 means fixed\n",
    "seed = 0\n",
    "np.random.seed(seed) if seed else None\n",
    "\n",
    "x_train, y_train, x_val, y_val, x_test, y_test = vowels()\n",
    "num_classes = 9\n",
    "\n",
    "print('train input size: ',x_train.shape, 'train target size: ',y_train.shape)\n",
    "print('val   input size: ',x_val.shape, 'val   target size: ',y_val.shape)\n",
    "print('test  input size: ',x_test.shape, 'test  target size: ',y_test.shape)\n",
    "\n",
    "# Define the network, cost function and minimization method,\n",
    "# train and evaluate the result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell can be used to present the classification results. \n",
    "It assumes you trained model has the name 'model_vowels'. \n",
    "It shows the test data results, but it can be changed to show\n",
    "the training/validation.\n",
    "\"\"\"\n",
    "print('\\n','#'*10,'Result for {} Data'.format('Test'), '#'*10, '\\n')\n",
    "\n",
    "y_pred = model_vowels.predict(x_test, verbose=0 )\n",
    "print('log_loss:   ', log_loss(y_test, y_pred, eps=1e-15))\n",
    "\n",
    "y_true = y_test.argmax(axis=1)\n",
    "y_pred = y_pred.argmax(axis=1)\n",
    "print('accuracy:   ',(y_pred==y_true).mean(), '\\n')\n",
    "\n",
    "target_names = ['class {}'.format(i+1) for i in range(9)]\n",
    "print(classification_report(y_true, y_pred, target_names=target_names))\n",
    "\n",
    "confuTst = confusion_matrix(y_true, y_pred)\n",
    "plot_confusion_matrix(cm           = confuTst, \n",
    "                      normalize    = False,\n",
    "                      target_names = ['1', '2', '3', '4', '5', '6','7', '8','9'],\n",
    "                      title        = \"Confusion Matrix: Test data\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CellName: Ex5 (#12)\n",
    "### CellType: Exercise\n",
    "### Cell instruction: Instruction for question 11\n",
    "\n",
    "For this last exercise the task is to train a binary classifier for the spiral problem. The aim is to get *zero* classification error for the training data (there is no test data) with as small as possible model, in terms of the number of trainable weights. Also plot the boundary to see if it resembles a spriral. To pass this question you should at least try!\n",
    "\n",
    "#### Question 11\n",
    "**Train a classifier for the spiral problem with the aim of zero classification error with as small as possible model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# seed = 0 means random, seed > 0 means fixed\n",
    "seed = 0\n",
    "np.random.seed(seed) if seed else None\n",
    "    \n",
    "# Generate training data\n",
    "x_train, d_train = spiral()\n",
    "x_train = x_train / 5\n",
    "\n",
    "# Define the network, cost function and minimization method,\n",
    "# train and evaluate the result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The report!\n",
    "\n",
    "\n",
    "### Name\n",
    "\n",
    "### Introduction\n",
    "\n",
    "### Answers to questions\n",
    "\n",
    "### Summary\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
