{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:40px;\"><center>Exercise V:<br> GANs\n",
    "</center></h1>\n",
    "\n",
    "## Short summary\n",
    "In this exercise, we will design a generative network to generate the last rgb image given the first image. These folder has **three files**: \n",
    "- **configGAN.py:** this involves definitions of all parameters and data paths\n",
    "- **utilsGAN.py:** includes utility functions required to grab and visualize data \n",
    "- **runGAN.ipynb:** contains the script to design, train and test the network \n",
    "\n",
    "Make sure that before running this script, you created an environment and **installed all required libraries** such \n",
    "as keras.\n",
    "\n",
    "## The data\n",
    "There exists also a subfolder called **data** which contains the traning, validation, and testing data each has both RGB input images together with the corresponding ground truth images.\n",
    "\n",
    "\n",
    "## The exercises\n",
    "As for the previous lab all exercises are found below.\n",
    "\n",
    "\n",
    "## The different 'Cells'\n",
    "This notebook contains several cells with python code, together with the markdown cells (like this one) with only text. Each of the cells with python code has a \"header\" markdown cell with information about the code. The table below provides a short overview of the code cells. \n",
    "\n",
    "| #  |  CellName | CellType | Comment |\n",
    "| :--- | :-------- | :-------- | :------- |\n",
    "| 1 | Init | Needed | Sets up the environment|\n",
    "| 2 | Ex | Exercise 1| A class definition of a network model  |\n",
    "| 3 | Loading | Needed | Loading parameters and initializing the model |\n",
    "| 4 | Stats | Needed | Show data distribution | \n",
    "| 5 | Data | Needed | Generating the data batches |\n",
    "| 6 | Debug | Needed | Debugging the data |\n",
    "| 7 | Device | Needed | Selecting CPU/GPU |\n",
    "| 8 | Init | Needed | Sets up the timer and other neccessary components |\n",
    "| 9 | Training | Exercise 1-2 | Training the model   |\n",
    "| 10 | Testing | Exercise 1-2| Testing the  method   |  \n",
    "\n",
    "\n",
    "In order for you to start with the exercise you need to run all cells. It is important that you do this in the correct order, starting from the top and continuing with the next cells. Later when you have started to work with the notebook it may be easier to use the command \"Run All\" found in the \"Cell\" dropdown menu.\n",
    "\n",
    "## Writing the report\n",
    "\n",
    "There is no need to provide any report. However, implemented network architecuture and observed experimental results must be presented as a short presentation in the last lecture, May 28."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) We first start with importing all required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating network model using gpu 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from configGAN import *\n",
    "cfg = flying_objects_config()\n",
    "if cfg.GPU >=0:\n",
    "    print(\"creating network model using gpu \" + str(cfg.GPU))\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = str(cfg.GPU)\n",
    "elif cfg.GPU >=-1:\n",
    "    print(\"creating network model using cpu \")  \n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from utilsGAN import *\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# import seaborn as sns\n",
    "from datetime import datetime\n",
    "import imageio\n",
    "from skimage import img_as_ubyte\n",
    "\n",
    "import pprint\n",
    "# import the necessary packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv3D, Conv2D, Conv1D, Convolution2D, Deconvolution2D, Cropping2D, UpSampling2D\n",
    "from keras.layers import Input, Conv2DTranspose, ConvLSTM2D, TimeDistributed\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers import Concatenate, concatenate, Reshape\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.core import Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input, decode_predictions\n",
    "from keras.layers import Input, merge\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Input, merge, Convolution2D, MaxPooling2D, UpSampling2D, Reshape, core, Dropout, LeakyReLU\n",
    "import keras.backend as kb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Here, we have the network model class definition. In this class, the most important functions are **build_generator()** and **build_discriminator()**. As defined in the exercises section, your task is to update the both network architectures defined in these functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANModel():\n",
    "    def __init__(self, batch_size=32, inputShape=(64, 64, 3), dropout_prob=0.25): \n",
    "        self.batch_size = batch_size\n",
    "        self.inputShape = inputShape\n",
    "        self.dropout_prob = dropout_prob\n",
    "\n",
    "        # Calculate the shape of patches\n",
    "        patch = int(self.inputShape[0] / 2**4)\n",
    "        self.disc_patch = (patch, patch, 1)\n",
    "  \n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='mse', optimizer=Adam(0.0002, 0.5),metrics=['accuracy'])\n",
    " \n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # Input images and their conditioning images\n",
    "        first_frame = Input(shape=self.inputShape)\n",
    "        last_frame = Input(shape=self.inputShape)\n",
    "\n",
    "        # By conditioning on the first frame generate a fake version of the last frame\n",
    "        fake_last_frame = self.generator(first_frame)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "        \n",
    "        # Discriminators determines validity of fake and condition first image pairs\n",
    "        valid = self.discriminator([fake_last_frame, first_frame])\n",
    "\n",
    "        self.combined = Model(inputs=[last_frame, first_frame], outputs=[valid, fake_last_frame])\n",
    "        self.combined.compile(loss=['mse', 'mae'], # mean squared and mean absolute errors\n",
    "                              loss_weights=[1, 100],\n",
    "                              optimizer=Adam(0.0002, 0.5))\n",
    "\n",
    "    def build_generator(self):\n",
    "        inputs = Input(shape=self.inputShape)\n",
    "        \n",
    "        conv1= Conv2D(filters=32,kernel_size=3,activation='relu',padding='same',kernel_initializer='he_normal')(inputs)\n",
    "        conv1 = BatchNormalization(momentum=0.8)(conv1)\n",
    "        conv1= Conv2D(filters=32,kernel_size=3,activation='relu',padding='same',kernel_initializer='he_normal')(conv1)\n",
    "        conv1 = BatchNormalization(momentum=0.8)(conv1)\n",
    "        pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "        \n",
    "        conv2= Conv2D(filters=64,kernel_size=3,activation='relu',padding='same',kernel_initializer='he_normal')(pool1)\n",
    "        conv2 = BatchNormalization(momentum=0.8)(conv2)\n",
    "        conv2= Conv2D(filters=64,kernel_size=3,activation='relu',padding='same',kernel_initializer='he_normal')(conv2)\n",
    "        conv2 = BatchNormalization(momentum=0.8)(conv2)\n",
    "        pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "        \n",
    "        conv3= Conv2D(filters=128,kernel_size=3,activation='relu',padding='same',kernel_initializer='he_normal')(pool2)\n",
    "        conv3 = BatchNormalization(momentum=0.8)(conv3)\n",
    "        conv3= Conv2D(filters=128,kernel_size=3,activation='relu',padding='same',kernel_initializer='he_normal')(conv3)\n",
    "        conv3 = BatchNormalization(momentum=0.8)(conv3)\n",
    "        pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "        \n",
    "        conv4= Conv2D(filters=256,kernel_size=3,activation='relu',padding='same',kernel_initializer='he_normal')(pool3)\n",
    "        conv4 = BatchNormalization(momentum=0.8)(conv4)\n",
    "        conv4= Conv2D(filters=256,kernel_size=3,activation='relu',padding='same',kernel_initializer='he_normal')(conv4)\n",
    "        conv4 = BatchNormalization(momentum=0.8)(conv4)\n",
    "        drop4 = Dropout(0.5)(conv4)\n",
    "        '''pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "        \n",
    "        conv5= Conv2D(filters=256,kernel_size=3,activation='relu',padding='same',kernel_initializer='he_normal')(pool4)\n",
    "        conv5= Conv2D(filters=256,kernel_size=3,activation='relu',padding='same',kernel_initializer='he_normal')(conv5)\n",
    "        #pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "        drop5 = Dropout(0.5)(conv5)\n",
    "        \n",
    "        \n",
    "        up6 = UpSampling2D(size=(2, 2))(drop5)\n",
    "        up6 = Conv2D(filters=256,kernel_size=3,activation='relu',padding='same',kernel_initializer='he_normal')(up6)\n",
    "        up6 = Concatenate(axis=3)([drop4, up6])\n",
    "        conv6 = Conv2D(filters=256,kernel_size=3,activation='relu',padding='same',kernel_initializer='he_normal')(up6)\n",
    "        conv6 = Conv2D(filters=256,kernel_size=3,activation='relu',padding='same',kernel_initializer='he_normal')(conv6)'''\n",
    "\n",
    "        up7 = UpSampling2D(size=(2, 2))(conv4)\n",
    "        up7 = Conv2D(filters=256,kernel_size=2,activation='relu',padding='same',kernel_initializer='he_normal')(up7)\n",
    "        up7 = BatchNormalization(momentum=0.8)(up7)\n",
    "        up7 = Concatenate(axis=3)([conv3, up7])\n",
    "        conv7 = Conv2D(filters=256,kernel_size=3,activation='relu',padding='same',kernel_initializer='he_normal')(up7)\n",
    "        conv7 = BatchNormalization(momentum=0.8)(conv7)\n",
    "        conv7 = Conv2D(filters=256,kernel_size=3,activation='relu',padding='same',kernel_initializer='he_normal')(conv7)\n",
    "        conv7 = BatchNormalization(momentum=0.8)(conv7)\n",
    "\n",
    "        up8 = UpSampling2D(size=(2, 2))(conv7)\n",
    "        up8 = Conv2D(filters=128,kernel_size=2,activation='relu',padding='same',kernel_initializer='he_normal')(up8)\n",
    "        up8 = BatchNormalization(momentum=0.8)(up8)\n",
    "        up8 = Concatenate(axis=3)([conv2, up8])\n",
    "        conv8 = Conv2D(filters=128,kernel_size=3,activation='relu',padding='same',kernel_initializer='he_normal')(up8)\n",
    "        conv8 = BatchNormalization(momentum=0.8)(conv8)\n",
    "        conv8 = Conv2D(filters=128,kernel_size=3,activation='relu',padding='same',kernel_initializer='he_normal')(conv8)\n",
    "\n",
    "        up9 = UpSampling2D(size=(2, 2))(conv8)\n",
    "        up9 = Conv2D(filters=64,kernel_size=2,activation='relu',padding='same',kernel_initializer='he_normal')(up9)\n",
    "        up9 = BatchNormalization(momentum=0.8)(up9)\n",
    "        up9 = Concatenate(axis=3)([conv1, up9])\n",
    "        conv9 = Conv2D(filters=64,kernel_size=3,activation='relu',padding='same',kernel_initializer='he_normal')(up9)\n",
    "        conv9 = BatchNormalization(momentum=0.8)(conv9)\n",
    "        conv9 = Conv2D(filters=64,kernel_size=3,activation='relu',padding='same',kernel_initializer='he_normal')(conv9)\n",
    "        conv9 = BatchNormalization(momentum=0.8)(conv9)\n",
    "        conv9 = Conv2D(filters=32,kernel_size=3,activation='relu',padding='same',kernel_initializer='he_normal')(conv9)\n",
    "        conv9 = BatchNormalization(momentum=0.8)(conv9)\n",
    "        \n",
    "        nbr_img_channels = self.inputShape[2]\n",
    "\n",
    "        outputs = Conv2D(nbr_img_channels, (1, 1), activation='sigmoid')(conv9)\n",
    "\n",
    "        model = Model(inputs=inputs, outputs=outputs, name='Generator')\n",
    "        model.summary()\n",
    " \n",
    "        '''inputs = Input(shape=self.inputShape)\n",
    "        print(inputs.shape)\n",
    " \n",
    "        down1 = Conv2D(32, (3, 3),padding='same')(inputs)\n",
    "        down1 = Activation('relu')(down1) \n",
    "        down1_pool = MaxPooling2D((2, 2), strides=(2, 2))(down1)\n",
    "         \n",
    "        down2 = Conv2D(64, (3, 3), padding='same')(down1_pool)\n",
    "        down2 = Activation('relu')(down2) \n",
    "         \n",
    "\n",
    "        up1 = UpSampling2D((2, 2))(down2)\n",
    "        up1 = concatenate([down1, up1], axis=3)\n",
    "        up1 = Conv2D(256, (3, 3), padding='same')(up1) \n",
    "        up1 = Activation('relu')(up1) \n",
    "        \n",
    "        \n",
    "        up2 = Conv2D(256, (3, 3), padding='same')(up1) \n",
    "        up2 = Activation('relu')(up2) \n",
    "        \n",
    "        nbr_img_channels = self.inputShape[2]\n",
    "        outputs = Conv2D(nbr_img_channels, (1, 1), activation='sigmoid')(up2)\n",
    "\n",
    "        model = Model(inputs=inputs, outputs=outputs, name='Generator')\n",
    "        model.summary()'''\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def build_discriminator(self):\n",
    "  \n",
    "        last_img = Input(shape=self.inputShape)\n",
    "        first_img = Input(shape=self.inputShape)\n",
    "\n",
    "        # Concatenate image and conditioning image by channels to produce input\n",
    "        combined_imgs = Concatenate(axis=-1)([last_img, first_img])\n",
    "  \n",
    "        d1 = Conv2D(32, (3, 3), strides=2, padding='same')(combined_imgs) \n",
    "        d1 = Activation('relu')(d1) \n",
    "        d2 = Conv2D(64, (3, 3), strides=2, padding='same')(d1)\n",
    "        d2 = Activation('relu')(d2) \n",
    "        d3 = Conv2D(128, (3, 3), strides=2, padding='same')(d2)\n",
    "        d3 = Activation('relu')(d3) \n",
    "         \n",
    "        validity = Conv2D(1, (3, 3), strides=2, padding='same')(d3)\n",
    "\n",
    "        model = Model([last_img, first_img], validity)\n",
    "        model.summary()\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) We import the network **hyperparameters** and build a simple network by calling the class introduced in the previous step. Please note that to change the hyperparameters, you just need to change the values in the file called **configPredictor.py.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 64, 64, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 64, 64, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 64, 64, 6)    0           input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 32, 32, 32)   1760        concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 32, 32, 32)   0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 16, 16, 64)   18496       activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 16, 16, 64)   0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 8, 8, 128)    73856       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 8, 8, 128)    0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 4, 4, 1)      1153        activation_2[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 95,265\n",
      "Trainable params: 95,265\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"Generator\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 64, 64, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 64, 64, 32)   896         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 64, 64, 32)   9248        conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 64, 64, 32)   128         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 32, 32, 32)   0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 32, 64)   18496       max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 64)   256         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 32, 64)   36928       batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, 32, 64)   256         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 16, 16, 64)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 16, 16, 128)  73856       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 16, 16, 128)  512         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 16, 16, 128)  147584      batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 16, 16, 128)  512         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 8, 8, 128)    0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 8, 8, 256)    295168      max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 8, 8, 256)    1024        conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 8, 8, 256)    590080      batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 8, 8, 256)    1024        conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d (UpSampling2D)    (None, 16, 16, 256)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 16, 16, 256)  262400      up_sampling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 16, 16, 256)  1024        conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 16, 16, 384)  0           batch_normalization_4[0][0]      \n",
      "                                                                 batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 16, 16, 256)  884992      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 16, 16, 256)  1024        conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 16, 16, 256)  590080      batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 16, 16, 256)  1024        conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 32, 32, 256)  0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 32, 32, 128)  131200      up_sampling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 32, 32, 128)  512         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 32, 32, 192)  0           batch_normalization_2[0][0]      \n",
      "                                                                 batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 32, 32, 128)  221312      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 32, 32, 128)  512         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 32, 32, 128)  147584      batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)  (None, 64, 64, 128)  0           conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 64, 64, 64)   32832       up_sampling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 64, 64, 64)   256         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 64, 64, 96)   0           batch_normalization[0][0]        \n",
      "                                                                 batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 64, 64, 64)   55360       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 64, 64, 64)   256         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 64, 64, 64)   36928       batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 64, 64, 64)   256         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 64, 64, 32)   18464       batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 64, 64, 32)   128         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 64, 64, 3)    99          batch_normalization_15[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 3,562,211\n",
      "Trainable params: 3,557,859\n",
      "Non-trainable params: 4,352\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "image_shape = (cfg.IMAGE_HEIGHT, cfg.IMAGE_WIDTH, cfg.IMAGE_CHANNEL)\n",
    "modelObj = GANModel(batch_size=cfg.BATCH_SIZE, inputShape=image_shape,\n",
    "                                 dropout_prob=cfg.DROPOUT_PROB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) We call the utility function **show_statistics** to display the data distribution. This is just for debugging purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "######################################################################\n",
      "##################### Training Data Statistics #####################\n",
      "######################################################################\n",
      "total image number \t 10817\n",
      "total class number \t 3\n",
      "class square \t 3488 images\n",
      "class circular \t 3626 images\n",
      "class triangle \t 3703 images\n",
      "######################################################################\n",
      "\n",
      "######################################################################\n",
      "##################### Validation Data Statistics #####################\n",
      "######################################################################\n",
      "total image number \t 2241\n",
      "total class number \t 3\n",
      "class triangle \t 745 images\n",
      "class square \t 783 images\n",
      "class circular \t 713 images\n",
      "######################################################################\n",
      "\n",
      "######################################################################\n",
      "##################### Testing Data Statistics #####################\n",
      "######################################################################\n",
      "total image number \t 2220\n",
      "total class number \t 3\n",
      "class triangle \t 733 images\n",
      "class square \t 765 images\n",
      "class circular \t 722 images\n",
      "######################################################################\n"
     ]
    }
   ],
   "source": [
    "#### show how the data looks like\n",
    "show_statistics(cfg.training_data_dir, fineGrained=False, title=\" Training Data Statistics \")\n",
    "show_statistics(cfg.validation_data_dir, fineGrained=False, title=\" Validation Data Statistics \")\n",
    "show_statistics(cfg.testing_data_dir, fineGrained=False, title=\" Testing Data Statistics \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) We now create batch generators to get small batches from the entire dataset. There is no need to change these functions as they already return **normalized inputs as batches**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data batch generators are created!\n"
     ]
    }
   ],
   "source": [
    "nbr_train_data = get_dataset_size(cfg.training_data_dir)\n",
    "nbr_valid_data = get_dataset_size(cfg.validation_data_dir)\n",
    "nbr_test_data = get_dataset_size(cfg.testing_data_dir)\n",
    "train_batch_generator = generate_lastframepredictor_batches(cfg.training_data_dir, image_shape, cfg.BATCH_SIZE)\n",
    "valid_batch_generator = generate_lastframepredictor_batches(cfg.validation_data_dir, image_shape, cfg.BATCH_SIZE)\n",
    "test_batch_generator = generate_lastframepredictor_batches(cfg.testing_data_dir, image_shape, cfg.BATCH_SIZE)\n",
    "print(\"Data batch generators are created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) We can visualize how the data looks like for debugging purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x (30, 64, 64, 3) float32 0.0 1.0\n",
      "train_y (30, 64, 64, 3) float32 0.0 1.0\n",
      "{'BATCH_SIZE': 30,\n",
      " 'DATA_AUGMENTATION': True,\n",
      " 'DEBUG_MODE': True,\n",
      " 'DROPOUT_PROB': 0.5,\n",
      " 'GPU': 0,\n",
      " 'IMAGE_CHANNEL': 3,\n",
      " 'IMAGE_HEIGHT': 64,\n",
      " 'IMAGE_WIDTH': 64,\n",
      " 'LEARNING_RATE': 0.01,\n",
      " 'LR_DECAY_FACTOR': 0.1,\n",
      " 'NUM_EPOCHS': 5,\n",
      " 'PRINT_EVERY': 20,\n",
      " 'SAVE_EVERY': 1,\n",
      " 'SEQUENCE_LENGTH': 10,\n",
      " 'testing_data_dir': '../data/FlyingObjectDataset_10K/testing',\n",
      " 'training_data_dir': '../data/FlyingObjectDataset_10K/training',\n",
      " 'validation_data_dir': '../data/FlyingObjectDataset_10K/validation'}\n"
     ]
    }
   ],
   "source": [
    "if cfg.DEBUG_MODE:\n",
    "    t_x, t_y = next(train_batch_generator)\n",
    "    print('train_x', t_x.shape, t_x.dtype, t_x.min(), t_x.max())\n",
    "    print('train_y', t_y.shape, t_y.dtype, t_y.min(), t_y.max()) \n",
    "    #plot_sample_lastframepredictor_data_with_groundtruth(t_x, t_y, t_y)\n",
    "    pprint.pprint (cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) Start timer and init matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()\n",
    "# Adversarial loss ground truths\n",
    "valid = np.ones((cfg.BATCH_SIZE,) + modelObj.disc_patch)\n",
    "fake = np.zeros((cfg.BATCH_SIZE,) + modelObj.disc_patch)\n",
    "# log file\n",
    "output_log_dir = \"./logs/{}\".format(datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "if not os.path.exists(output_log_dir):\n",
    "    os.makedirs(output_log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8) We can now feed the training and validation data to the network. This will train the network for **some epochs**. Note that the epoch number is also predefined in the file called **configGAN.py.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 64, 64, 3)\n",
      "0.5123859\n",
      "[Epoch 0/5] [Batch 0/360] [D loss: 0.521882] [G loss: 51.729744] time: 0:01:55.399744\n",
      "(30, 64, 64, 3)\n",
      "0.46975246\n",
      "[Epoch 0/5] [Batch 1/360] [D loss: 0.382826] [G loss: 49.014347] time: 0:01:55.661834\n",
      "(30, 64, 64, 3)\n",
      "0.5017248\n",
      "[Epoch 0/5] [Batch 2/360] [D loss: 0.308842] [G loss: 47.192730] time: 0:01:55.842164\n",
      "(30, 64, 64, 3)\n",
      "0.5099958\n",
      "[Epoch 0/5] [Batch 3/360] [D loss: 0.263464] [G loss: 45.672604] time: 0:01:56.036658\n",
      "(30, 64, 64, 3)\n",
      "0.51462656\n",
      "[Epoch 0/5] [Batch 4/360] [D loss: 0.232008] [G loss: 44.569897] time: 0:01:56.244516\n",
      "(30, 64, 64, 3)\n",
      "0.52320975\n",
      "[Epoch 0/5] [Batch 5/360] [D loss: 0.214573] [G loss: 42.836452] time: 0:01:56.460845\n",
      "(30, 64, 64, 3)\n",
      "0.5407674\n",
      "[Epoch 0/5] [Batch 6/360] [D loss: 0.204692] [G loss: 41.815075] time: 0:01:56.650737\n",
      "(30, 64, 64, 3)\n",
      "0.56274575\n",
      "[Epoch 0/5] [Batch 7/360] [D loss: 0.199852] [G loss: 40.513672] time: 0:01:56.869258\n",
      "(30, 64, 64, 3)\n",
      "0.558219\n",
      "[Epoch 0/5] [Batch 8/360] [D loss: 0.197937] [G loss: 39.192722] time: 0:01:57.100405\n",
      "(30, 64, 64, 3)\n",
      "0.57522684\n",
      "[Epoch 0/5] [Batch 9/360] [D loss: 0.195689] [G loss: 38.770912] time: 0:01:57.297703\n",
      "(30, 64, 64, 3)\n",
      "0.61223525\n",
      "[Epoch 0/5] [Batch 10/360] [D loss: 0.194392] [G loss: 37.923569] time: 0:01:57.506539\n",
      "(30, 64, 64, 3)\n",
      "0.64503044\n",
      "[Epoch 0/5] [Batch 11/360] [D loss: 0.194248] [G loss: 37.632103] time: 0:01:57.685478\n",
      "(30, 64, 64, 3)\n",
      "0.6177899\n",
      "[Epoch 0/5] [Batch 12/360] [D loss: 0.192824] [G loss: 36.585800] time: 0:01:57.897593\n",
      "(30, 64, 64, 3)\n",
      "0.6607342\n",
      "[Epoch 0/5] [Batch 13/360] [D loss: 0.195086] [G loss: 35.837852] time: 0:01:58.086765\n",
      "(30, 64, 64, 3)\n",
      "0.7082332\n",
      "[Epoch 0/5] [Batch 14/360] [D loss: 0.192257] [G loss: 35.931820] time: 0:01:58.288584\n",
      "(30, 64, 64, 3)\n",
      "0.6127825\n",
      "[Epoch 0/5] [Batch 15/360] [D loss: 0.195523] [G loss: 34.343960] time: 0:01:58.464667\n",
      "(30, 64, 64, 3)\n",
      "0.59361637\n",
      "[Epoch 0/5] [Batch 16/360] [D loss: 0.189859] [G loss: 34.356030] time: 0:01:58.624539\n",
      "(30, 64, 64, 3)\n",
      "0.5967233\n",
      "[Epoch 0/5] [Batch 17/360] [D loss: 0.187571] [G loss: 33.540260] time: 0:01:58.801689\n",
      "(30, 64, 64, 3)\n",
      "0.6924434\n",
      "[Epoch 0/5] [Batch 18/360] [D loss: 0.182821] [G loss: 32.614262] time: 0:01:58.980561\n",
      "(30, 64, 64, 3)\n",
      "0.69658685\n",
      "[Epoch 0/5] [Batch 19/360] [D loss: 0.180001] [G loss: 32.091885] time: 0:01:59.148898\n",
      "(30, 64, 64, 3)\n",
      "0.67934245\n",
      "[Epoch 0/5] [Batch 20/360] [D loss: 0.179236] [G loss: 32.982826] time: 0:01:59.342928\n",
      "(30, 64, 64, 3)\n",
      "0.6189347\n",
      "[Epoch 0/5] [Batch 21/360] [D loss: 0.175368] [G loss: 32.358463] time: 0:01:59.536235\n",
      "(30, 64, 64, 3)\n",
      "0.6695819\n",
      "[Epoch 0/5] [Batch 22/360] [D loss: 0.171176] [G loss: 31.839575] time: 0:01:59.711505\n",
      "(30, 64, 64, 3)\n",
      "0.6538427\n",
      "[Epoch 0/5] [Batch 23/360] [D loss: 0.166772] [G loss: 31.374453] time: 0:01:59.898445\n",
      "(30, 64, 64, 3)\n",
      "0.67038816\n",
      "[Epoch 0/5] [Batch 24/360] [D loss: 0.159651] [G loss: 31.912554] time: 0:02:00.080578\n",
      "(30, 64, 64, 3)\n",
      "0.64601374\n",
      "[Epoch 0/5] [Batch 25/360] [D loss: 0.152984] [G loss: 31.202366] time: 0:02:00.257917\n",
      "(30, 64, 64, 3)\n",
      "0.6769021\n",
      "[Epoch 0/5] [Batch 26/360] [D loss: 0.149574] [G loss: 30.488167] time: 0:02:00.472089\n",
      "(30, 64, 64, 3)\n",
      "0.564105\n",
      "[Epoch 0/5] [Batch 27/360] [D loss: 0.146421] [G loss: 30.339027] time: 0:02:00.669467\n",
      "(30, 64, 64, 3)\n",
      "0.56238407\n",
      "[Epoch 0/5] [Batch 28/360] [D loss: 0.135006] [G loss: 30.110241] time: 0:02:00.885116\n",
      "(30, 64, 64, 3)\n",
      "0.72594863\n",
      "[Epoch 0/5] [Batch 29/360] [D loss: 0.126961] [G loss: 30.413858] time: 0:02:01.087997\n",
      "(30, 64, 64, 3)\n",
      "0.5908888\n",
      "[Epoch 0/5] [Batch 30/360] [D loss: 0.125321] [G loss: 30.480207] time: 0:02:01.283374\n",
      "(30, 64, 64, 3)\n",
      "0.61987364\n",
      "[Epoch 0/5] [Batch 31/360] [D loss: 0.117734] [G loss: 29.801392] time: 0:02:01.499260\n",
      "(30, 64, 64, 3)\n",
      "0.5751477\n",
      "[Epoch 0/5] [Batch 32/360] [D loss: 0.103989] [G loss: 30.570192] time: 0:02:01.673665\n",
      "(30, 64, 64, 3)\n",
      "0.69060975\n",
      "[Epoch 0/5] [Batch 33/360] [D loss: 0.090282] [G loss: 29.562237] time: 0:02:01.887374\n",
      "(30, 64, 64, 3)\n",
      "0.6775036\n",
      "[Epoch 0/5] [Batch 34/360] [D loss: 0.085323] [G loss: 28.962849] time: 0:02:02.090860\n",
      "(30, 64, 64, 3)\n",
      "0.61252403\n",
      "[Epoch 0/5] [Batch 35/360] [D loss: 0.073937] [G loss: 29.094028] time: 0:02:02.242036\n",
      "(30, 64, 64, 3)\n",
      "0.6485998\n",
      "[Epoch 0/5] [Batch 36/360] [D loss: 0.073477] [G loss: 30.136679] time: 0:02:02.471435\n",
      "(30, 64, 64, 3)\n",
      "0.6894965\n",
      "[Epoch 0/5] [Batch 37/360] [D loss: 0.060577] [G loss: 29.561184] time: 0:02:02.690959\n",
      "(30, 64, 64, 3)\n",
      "0.6365028\n",
      "[Epoch 0/5] [Batch 38/360] [D loss: 0.053819] [G loss: 30.129847] time: 0:02:02.896402\n",
      "(30, 64, 64, 3)\n",
      "0.70599204\n",
      "[Epoch 0/5] [Batch 39/360] [D loss: 0.045669] [G loss: 29.035303] time: 0:02:03.095090\n",
      "(30, 64, 64, 3)\n",
      "0.633985\n",
      "[Epoch 0/5] [Batch 40/360] [D loss: 0.035719] [G loss: 29.182255] time: 0:02:03.312937\n",
      "(30, 64, 64, 3)\n",
      "0.6192128\n",
      "[Epoch 0/5] [Batch 41/360] [D loss: 0.032138] [G loss: 29.022223] time: 0:02:03.498836\n",
      "(30, 64, 64, 3)\n",
      "0.67283374\n",
      "[Epoch 0/5] [Batch 42/360] [D loss: 0.031922] [G loss: 28.749182] time: 0:02:03.738325\n",
      "(30, 64, 64, 3)\n",
      "0.687185\n",
      "[Epoch 0/5] [Batch 43/360] [D loss: 0.026213] [G loss: 28.719744] time: 0:02:03.926111\n",
      "(30, 64, 64, 3)\n",
      "0.6240085\n",
      "[Epoch 0/5] [Batch 44/360] [D loss: 0.026381] [G loss: 29.246387] time: 0:02:04.107141\n",
      "(30, 64, 64, 3)\n",
      "0.5748121\n",
      "[Epoch 0/5] [Batch 45/360] [D loss: 0.019812] [G loss: 28.288385] time: 0:02:04.305959\n",
      "(30, 64, 64, 3)\n",
      "0.6181051\n",
      "[Epoch 0/5] [Batch 46/360] [D loss: 0.022145] [G loss: 27.961037] time: 0:02:04.473115\n",
      "(30, 64, 64, 3)\n",
      "0.6150862\n",
      "[Epoch 0/5] [Batch 47/360] [D loss: 0.018966] [G loss: 27.876186] time: 0:02:04.663238\n",
      "(30, 64, 64, 3)\n",
      "0.7814126\n",
      "[Epoch 0/5] [Batch 48/360] [D loss: 0.020166] [G loss: 28.679783] time: 0:02:04.850780\n",
      "(30, 64, 64, 3)\n",
      "0.58656424\n",
      "[Epoch 0/5] [Batch 49/360] [D loss: 0.019588] [G loss: 28.503212] time: 0:02:05.040777\n",
      "(30, 64, 64, 3)\n",
      "0.62443537\n",
      "[Epoch 0/5] [Batch 50/360] [D loss: 0.016852] [G loss: 27.472406] time: 0:02:05.236798\n",
      "(30, 64, 64, 3)\n",
      "0.6721311\n",
      "[Epoch 0/5] [Batch 51/360] [D loss: 0.019973] [G loss: 27.602173] time: 0:02:05.401295\n",
      "(30, 64, 64, 3)\n",
      "0.67418146\n",
      "[Epoch 0/5] [Batch 52/360] [D loss: 0.018032] [G loss: 27.539967] time: 0:02:05.584270\n",
      "(30, 64, 64, 3)\n",
      "0.6508574\n",
      "[Epoch 0/5] [Batch 53/360] [D loss: 0.019671] [G loss: 27.158953] time: 0:02:05.793990\n",
      "(30, 64, 64, 3)\n",
      "0.66510963\n",
      "[Epoch 0/5] [Batch 54/360] [D loss: 0.014121] [G loss: 27.624315] time: 0:02:05.978783\n",
      "(30, 64, 64, 3)\n",
      "0.6940932\n",
      "[Epoch 0/5] [Batch 55/360] [D loss: 0.020137] [G loss: 27.056953] time: 0:02:06.166308\n",
      "(30, 64, 64, 3)\n",
      "0.5655505\n",
      "[Epoch 0/5] [Batch 56/360] [D loss: 0.013460] [G loss: 28.618711] time: 0:02:06.375954\n",
      "(30, 64, 64, 3)\n",
      "0.6261441\n",
      "[Epoch 0/5] [Batch 57/360] [D loss: 0.021789] [G loss: 27.897959] time: 0:02:06.592447\n",
      "(30, 64, 64, 3)\n",
      "0.6623556\n",
      "[Epoch 0/5] [Batch 58/360] [D loss: 0.015561] [G loss: 27.271538] time: 0:02:06.820122\n",
      "(30, 64, 64, 3)\n",
      "0.76060796\n",
      "[Epoch 0/5] [Batch 59/360] [D loss: 0.017480] [G loss: 27.193972] time: 0:02:07.054230\n",
      "(30, 64, 64, 3)\n",
      "0.76513594\n",
      "[Epoch 0/5] [Batch 60/360] [D loss: 0.022780] [G loss: 27.114626] time: 0:02:07.270998\n",
      "(30, 64, 64, 3)\n",
      "0.7328604\n",
      "[Epoch 0/5] [Batch 61/360] [D loss: 0.012830] [G loss: 26.944971] time: 0:02:07.457541\n",
      "(30, 64, 64, 3)\n",
      "0.70357823\n",
      "[Epoch 0/5] [Batch 62/360] [D loss: 0.014118] [G loss: 27.393007] time: 0:02:07.640281\n",
      "(30, 64, 64, 3)\n",
      "0.6130781\n",
      "[Epoch 0/5] [Batch 63/360] [D loss: 0.013361] [G loss: 27.670343] time: 0:02:07.834291\n",
      "(30, 64, 64, 3)\n",
      "0.7073018\n",
      "[Epoch 0/5] [Batch 64/360] [D loss: 0.012796] [G loss: 26.774895] time: 0:02:08.048192\n",
      "(30, 64, 64, 3)\n",
      "0.6648252\n",
      "[Epoch 0/5] [Batch 65/360] [D loss: 0.014944] [G loss: 26.484747] time: 0:02:08.232434\n",
      "(30, 64, 64, 3)\n",
      "0.60865253\n",
      "[Epoch 0/5] [Batch 66/360] [D loss: 0.012262] [G loss: 26.813086] time: 0:02:08.425397\n",
      "(30, 64, 64, 3)\n",
      "0.69256145\n",
      "[Epoch 0/5] [Batch 67/360] [D loss: 0.017052] [G loss: 27.779707] time: 0:02:08.634811\n",
      "(30, 64, 64, 3)\n",
      "0.6764321\n",
      "[Epoch 0/5] [Batch 68/360] [D loss: 0.017146] [G loss: 26.449053] time: 0:02:08.830335\n",
      "(30, 64, 64, 3)\n",
      "0.6417978\n",
      "[Epoch 0/5] [Batch 69/360] [D loss: 0.011392] [G loss: 26.968487] time: 0:02:09.020873\n",
      "(30, 64, 64, 3)\n",
      "0.70213556\n",
      "[Epoch 0/5] [Batch 70/360] [D loss: 0.013112] [G loss: 26.744894] time: 0:02:09.219532\n",
      "(30, 64, 64, 3)\n",
      "0.61458963\n",
      "[Epoch 0/5] [Batch 71/360] [D loss: 0.010658] [G loss: 27.126829] time: 0:02:09.421085\n",
      "(30, 64, 64, 3)\n",
      "0.6639778\n",
      "[Epoch 0/5] [Batch 72/360] [D loss: 0.013123] [G loss: 26.011003] time: 0:02:09.615508\n",
      "(30, 64, 64, 3)\n",
      "0.73892146\n",
      "[Epoch 0/5] [Batch 73/360] [D loss: 0.017413] [G loss: 25.999971] time: 0:02:09.816683\n",
      "(30, 64, 64, 3)\n",
      "0.62463135\n",
      "[Epoch 0/5] [Batch 74/360] [D loss: 0.014285] [G loss: 26.254631] time: 0:02:09.982513\n",
      "(30, 64, 64, 3)\n",
      "0.551145\n",
      "[Epoch 0/5] [Batch 75/360] [D loss: 0.009854] [G loss: 26.141327] time: 0:02:10.202803\n",
      "(30, 64, 64, 3)\n",
      "0.69273335\n",
      "[Epoch 0/5] [Batch 76/360] [D loss: 0.010801] [G loss: 26.133678] time: 0:02:10.399207\n",
      "(30, 64, 64, 3)\n",
      "0.67151076\n",
      "[Epoch 0/5] [Batch 77/360] [D loss: 0.012695] [G loss: 26.353382] time: 0:02:10.603044\n",
      "(30, 64, 64, 3)\n",
      "0.7605972\n",
      "[Epoch 0/5] [Batch 78/360] [D loss: 0.033067] [G loss: 26.016432] time: 0:02:10.808345\n",
      "(30, 64, 64, 3)\n",
      "0.61671793\n",
      "[Epoch 0/5] [Batch 79/360] [D loss: 0.015674] [G loss: 25.587311] time: 0:02:11.029863\n",
      "(30, 64, 64, 3)\n",
      "0.65884143\n",
      "[Epoch 0/5] [Batch 80/360] [D loss: 0.016044] [G loss: 26.009743] time: 0:02:11.236985\n",
      "(30, 64, 64, 3)\n",
      "0.7099996\n",
      "[Epoch 0/5] [Batch 81/360] [D loss: 0.023153] [G loss: 26.390219] time: 0:02:11.446770\n",
      "(30, 64, 64, 3)\n",
      "0.70928806\n",
      "[Epoch 0/5] [Batch 82/360] [D loss: 0.010415] [G loss: 25.734440] time: 0:02:11.629822\n",
      "(30, 64, 64, 3)\n",
      "0.73869795\n",
      "[Epoch 0/5] [Batch 83/360] [D loss: 0.008567] [G loss: 26.441698] time: 0:02:11.854180\n",
      "(30, 64, 64, 3)\n",
      "0.69918174\n",
      "[Epoch 0/5] [Batch 84/360] [D loss: 0.013909] [G loss: 25.602587] time: 0:02:12.078478\n",
      "(30, 64, 64, 3)\n",
      "0.6936342\n",
      "[Epoch 0/5] [Batch 85/360] [D loss: 0.009918] [G loss: 25.968977] time: 0:02:12.271849\n",
      "(30, 64, 64, 3)\n",
      "0.73177224\n",
      "[Epoch 0/5] [Batch 86/360] [D loss: 0.034223] [G loss: 25.534449] time: 0:02:12.486856\n",
      "(30, 64, 64, 3)\n",
      "0.70246094\n",
      "[Epoch 0/5] [Batch 87/360] [D loss: 0.023187] [G loss: 25.110731] time: 0:02:12.672182\n",
      "(30, 64, 64, 3)\n",
      "0.7385576\n",
      "[Epoch 0/5] [Batch 88/360] [D loss: 0.019185] [G loss: 25.306429] time: 0:02:12.854894\n",
      "(30, 64, 64, 3)\n",
      "0.6412718\n",
      "[Epoch 0/5] [Batch 89/360] [D loss: 0.035861] [G loss: 25.137922] time: 0:02:13.049954\n",
      "(30, 64, 64, 3)\n",
      "0.6813607\n",
      "[Epoch 0/5] [Batch 90/360] [D loss: 0.012891] [G loss: 24.508562] time: 0:02:13.255216\n",
      "(30, 64, 64, 3)\n",
      "0.6634572\n",
      "[Epoch 0/5] [Batch 91/360] [D loss: 0.009231] [G loss: 24.908018] time: 0:02:13.457549\n",
      "(30, 64, 64, 3)\n",
      "0.69351107\n",
      "[Epoch 0/5] [Batch 92/360] [D loss: 0.006080] [G loss: 25.189596] time: 0:02:13.678964\n",
      "(30, 64, 64, 3)\n",
      "0.6998659\n",
      "[Epoch 0/5] [Batch 93/360] [D loss: 0.009230] [G loss: 24.548727] time: 0:02:13.904714\n",
      "(30, 64, 64, 3)\n",
      "0.58556676\n",
      "[Epoch 0/5] [Batch 94/360] [D loss: 0.008092] [G loss: 24.257200] time: 0:02:14.103211\n",
      "(30, 64, 64, 3)\n",
      "0.7079932\n",
      "[Epoch 0/5] [Batch 95/360] [D loss: 0.009584] [G loss: 24.801746] time: 0:02:14.296893\n",
      "(30, 64, 64, 3)\n",
      "0.690011\n",
      "[Epoch 0/5] [Batch 96/360] [D loss: 0.008696] [G loss: 24.527821] time: 0:02:14.486698\n",
      "(30, 64, 64, 3)\n",
      "0.7087186\n",
      "[Epoch 0/5] [Batch 97/360] [D loss: 0.006676] [G loss: 24.731321] time: 0:02:14.661069\n",
      "(30, 64, 64, 3)\n",
      "0.7287164\n",
      "[Epoch 0/5] [Batch 98/360] [D loss: 0.005972] [G loss: 24.745256] time: 0:02:14.851819\n",
      "(30, 64, 64, 3)\n",
      "0.66922766\n",
      "[Epoch 0/5] [Batch 99/360] [D loss: 0.008528] [G loss: 25.037413] time: 0:02:15.063134\n",
      "(30, 64, 64, 3)\n",
      "0.7472017\n",
      "[Epoch 0/5] [Batch 100/360] [D loss: 0.012495] [G loss: 24.762697] time: 0:02:15.260561\n",
      "(30, 64, 64, 3)\n",
      "0.71304697\n",
      "[Epoch 0/5] [Batch 101/360] [D loss: 0.008148] [G loss: 25.731260] time: 0:02:15.454503\n",
      "(30, 64, 64, 3)\n",
      "0.68221974\n",
      "[Epoch 0/5] [Batch 102/360] [D loss: 0.012907] [G loss: 24.966658] time: 0:02:15.672023\n",
      "(30, 64, 64, 3)\n",
      "0.7281988\n",
      "[Epoch 0/5] [Batch 103/360] [D loss: 0.022593] [G loss: 24.280764] time: 0:02:15.882281\n",
      "(30, 64, 64, 3)\n",
      "0.64991343\n",
      "[Epoch 0/5] [Batch 104/360] [D loss: 0.070364] [G loss: 25.121197] time: 0:02:16.064847\n",
      "(30, 64, 64, 3)\n",
      "0.67631644\n",
      "[Epoch 0/5] [Batch 105/360] [D loss: 0.023815] [G loss: 24.440727] time: 0:02:16.246119\n",
      "(30, 64, 64, 3)\n",
      "0.6250861\n",
      "[Epoch 0/5] [Batch 106/360] [D loss: 0.032124] [G loss: 23.901295] time: 0:02:16.458922\n",
      "(30, 64, 64, 3)\n",
      "0.73868245\n",
      "[Epoch 0/5] [Batch 107/360] [D loss: 0.017329] [G loss: 24.527252] time: 0:02:16.618698\n",
      "(30, 64, 64, 3)\n",
      "0.74258184\n",
      "[Epoch 0/5] [Batch 108/360] [D loss: 0.006957] [G loss: 23.865784] time: 0:02:16.821819\n",
      "(30, 64, 64, 3)\n",
      "0.76660115\n",
      "[Epoch 0/5] [Batch 109/360] [D loss: 0.016305] [G loss: 24.532391] time: 0:02:17.030180\n",
      "(30, 64, 64, 3)\n",
      "0.6812201\n",
      "[Epoch 0/5] [Batch 110/360] [D loss: 0.008858] [G loss: 24.008121] time: 0:02:17.247370\n",
      "(30, 64, 64, 3)\n",
      "0.64364845\n",
      "[Epoch 0/5] [Batch 111/360] [D loss: 0.008039] [G loss: 24.187166] time: 0:02:17.448796\n",
      "(30, 64, 64, 3)\n",
      "0.64303195\n",
      "[Epoch 0/5] [Batch 112/360] [D loss: 0.007085] [G loss: 24.132818] time: 0:02:17.611415\n",
      "(30, 64, 64, 3)\n",
      "0.64014673\n",
      "[Epoch 0/5] [Batch 113/360] [D loss: 0.012323] [G loss: 23.520546] time: 0:02:17.813337\n",
      "(30, 64, 64, 3)\n",
      "0.7419877\n",
      "[Epoch 0/5] [Batch 114/360] [D loss: 0.005913] [G loss: 24.074360] time: 0:02:17.994331\n",
      "(30, 64, 64, 3)\n",
      "0.75602454\n",
      "[Epoch 0/5] [Batch 115/360] [D loss: 0.005182] [G loss: 24.339649] time: 0:02:18.188097\n",
      "(30, 64, 64, 3)\n",
      "0.6787753\n",
      "[Epoch 0/5] [Batch 116/360] [D loss: 0.005764] [G loss: 23.839758] time: 0:02:18.359874\n",
      "(30, 64, 64, 3)\n",
      "0.68748456\n",
      "[Epoch 0/5] [Batch 117/360] [D loss: 0.007252] [G loss: 23.761801] time: 0:02:18.548806\n",
      "(30, 64, 64, 3)\n",
      "0.6385508\n",
      "[Epoch 0/5] [Batch 118/360] [D loss: 0.005017] [G loss: 23.868656] time: 0:02:18.721177\n",
      "(30, 64, 64, 3)\n",
      "0.67944765\n",
      "[Epoch 0/5] [Batch 119/360] [D loss: 0.005809] [G loss: 23.634714] time: 0:02:18.913284\n",
      "(30, 64, 64, 3)\n",
      "0.6728402\n",
      "[Epoch 0/5] [Batch 120/360] [D loss: 0.008137] [G loss: 23.056444] time: 0:02:19.120369\n",
      "(30, 64, 64, 3)\n",
      "0.67592734\n",
      "[Epoch 0/5] [Batch 121/360] [D loss: 0.005750] [G loss: 22.956636] time: 0:02:19.274556\n",
      "(30, 64, 64, 3)\n",
      "0.6424126\n",
      "[Epoch 0/5] [Batch 122/360] [D loss: 0.008223] [G loss: 23.469046] time: 0:02:19.469693\n",
      "(30, 64, 64, 3)\n",
      "0.755826\n",
      "[Epoch 0/5] [Batch 123/360] [D loss: 0.008272] [G loss: 23.768263] time: 0:02:19.631727\n",
      "(30, 64, 64, 3)\n",
      "0.7222281\n",
      "[Epoch 0/5] [Batch 124/360] [D loss: 0.007127] [G loss: 23.324516] time: 0:02:19.828478\n",
      "(30, 64, 64, 3)\n",
      "0.69405895\n",
      "[Epoch 0/5] [Batch 125/360] [D loss: 0.007152] [G loss: 23.154234] time: 0:02:20.037243\n",
      "(30, 64, 64, 3)\n",
      "0.6082084\n",
      "[Epoch 0/5] [Batch 126/360] [D loss: 0.005690] [G loss: 22.531027] time: 0:02:20.254180\n",
      "(30, 64, 64, 3)\n",
      "0.7152688\n",
      "[Epoch 0/5] [Batch 127/360] [D loss: 0.008566] [G loss: 23.138641] time: 0:02:20.420693\n",
      "(30, 64, 64, 3)\n",
      "0.7267654\n",
      "[Epoch 0/5] [Batch 128/360] [D loss: 0.010448] [G loss: 23.540173] time: 0:02:20.598980\n",
      "(30, 64, 64, 3)\n",
      "0.6752288\n",
      "[Epoch 0/5] [Batch 129/360] [D loss: 0.012454] [G loss: 23.148726] time: 0:02:20.777900\n",
      "(30, 64, 64, 3)\n",
      "0.65454847\n",
      "[Epoch 0/5] [Batch 130/360] [D loss: 0.006169] [G loss: 23.189882] time: 0:02:20.950432\n",
      "(30, 64, 64, 3)\n",
      "0.6445578\n",
      "[Epoch 0/5] [Batch 131/360] [D loss: 0.007619] [G loss: 23.240803] time: 0:02:21.147889\n",
      "(30, 64, 64, 3)\n",
      "0.7329698\n",
      "[Epoch 0/5] [Batch 132/360] [D loss: 0.005943] [G loss: 22.891701] time: 0:02:21.361962\n",
      "(30, 64, 64, 3)\n",
      "0.66356117\n",
      "[Epoch 0/5] [Batch 133/360] [D loss: 0.006622] [G loss: 22.347902] time: 0:02:21.575184\n",
      "(30, 64, 64, 3)\n",
      "0.69151855\n",
      "[Epoch 0/5] [Batch 134/360] [D loss: 0.005138] [G loss: 23.378637] time: 0:02:21.745576\n",
      "(30, 64, 64, 3)\n",
      "0.7207988\n",
      "[Epoch 0/5] [Batch 135/360] [D loss: 0.003746] [G loss: 23.070902] time: 0:02:21.925797\n",
      "(30, 64, 64, 3)\n",
      "0.7411954\n",
      "[Epoch 0/5] [Batch 136/360] [D loss: 0.006114] [G loss: 22.446808] time: 0:02:22.160562\n",
      "(30, 64, 64, 3)\n",
      "0.7770726\n",
      "[Epoch 0/5] [Batch 137/360] [D loss: 0.005328] [G loss: 22.372046] time: 0:02:22.361691\n",
      "(30, 64, 64, 3)\n",
      "0.74007136\n",
      "[Epoch 0/5] [Batch 138/360] [D loss: 0.012290] [G loss: 23.044474] time: 0:02:22.535979\n",
      "(30, 64, 64, 3)\n",
      "0.7214894\n",
      "[Epoch 0/5] [Batch 139/360] [D loss: 0.011149] [G loss: 21.998549] time: 0:02:22.741966\n",
      "(30, 64, 64, 3)\n",
      "0.6857675\n",
      "[Epoch 0/5] [Batch 140/360] [D loss: 0.016424] [G loss: 23.522943] time: 0:02:22.974254\n",
      "(30, 64, 64, 3)\n",
      "0.74278235\n",
      "[Epoch 0/5] [Batch 141/360] [D loss: 0.037606] [G loss: 22.222425] time: 0:02:23.184872\n",
      "(30, 64, 64, 3)\n",
      "0.806804\n",
      "[Epoch 0/5] [Batch 142/360] [D loss: 0.042214] [G loss: 22.760725] time: 0:02:23.348227\n",
      "(30, 64, 64, 3)\n",
      "0.70603395\n",
      "[Epoch 0/5] [Batch 143/360] [D loss: 0.011101] [G loss: 22.855669] time: 0:02:23.563317\n",
      "(30, 64, 64, 3)\n",
      "0.7706942\n",
      "[Epoch 0/5] [Batch 144/360] [D loss: 0.010405] [G loss: 23.382397] time: 0:02:23.739807\n",
      "(30, 64, 64, 3)\n",
      "0.7319676\n",
      "[Epoch 0/5] [Batch 145/360] [D loss: 0.022122] [G loss: 23.107281] time: 0:02:23.967049\n",
      "(30, 64, 64, 3)\n",
      "0.70898753\n",
      "[Epoch 0/5] [Batch 146/360] [D loss: 0.008602] [G loss: 22.270201] time: 0:02:24.175300\n",
      "(30, 64, 64, 3)\n",
      "0.7408007\n",
      "[Epoch 0/5] [Batch 147/360] [D loss: 0.025250] [G loss: 22.361868] time: 0:02:24.345953\n",
      "(30, 64, 64, 3)\n",
      "0.7056327\n",
      "[Epoch 0/5] [Batch 148/360] [D loss: 0.007189] [G loss: 21.525280] time: 0:02:24.558343\n",
      "(30, 64, 64, 3)\n",
      "0.76971716\n",
      "[Epoch 0/5] [Batch 149/360] [D loss: 0.004897] [G loss: 23.377596] time: 0:02:24.777261\n",
      "(30, 64, 64, 3)\n",
      "0.69629306\n",
      "[Epoch 0/5] [Batch 150/360] [D loss: 0.004924] [G loss: 21.839725] time: 0:02:24.989387\n",
      "(30, 64, 64, 3)\n",
      "0.74316835\n",
      "[Epoch 0/5] [Batch 151/360] [D loss: 0.005720] [G loss: 22.504208] time: 0:02:25.230811\n",
      "(30, 64, 64, 3)\n",
      "0.73087174\n",
      "[Epoch 0/5] [Batch 152/360] [D loss: 0.003755] [G loss: 22.195042] time: 0:02:25.418355\n",
      "(30, 64, 64, 3)\n",
      "0.7044309\n",
      "[Epoch 0/5] [Batch 153/360] [D loss: 0.004024] [G loss: 22.438469] time: 0:02:25.639092\n",
      "(30, 64, 64, 3)\n",
      "0.7239897\n",
      "[Epoch 0/5] [Batch 154/360] [D loss: 0.004323] [G loss: 22.264221] time: 0:02:25.826474\n",
      "(30, 64, 64, 3)\n",
      "0.7163673\n",
      "[Epoch 0/5] [Batch 155/360] [D loss: 0.006011] [G loss: 21.950752] time: 0:02:26.017825\n",
      "(30, 64, 64, 3)\n",
      "0.69925827\n",
      "[Epoch 0/5] [Batch 156/360] [D loss: 0.008188] [G loss: 21.918806] time: 0:02:26.227096\n",
      "(30, 64, 64, 3)\n",
      "0.7647745\n",
      "[Epoch 0/5] [Batch 157/360] [D loss: 0.005339] [G loss: 21.450489] time: 0:02:26.427937\n",
      "(30, 64, 64, 3)\n",
      "0.73845124\n",
      "[Epoch 0/5] [Batch 158/360] [D loss: 0.006963] [G loss: 22.356817] time: 0:02:26.665287\n",
      "(30, 64, 64, 3)\n",
      "0.75178546\n",
      "[Epoch 0/5] [Batch 159/360] [D loss: 0.005848] [G loss: 22.046598] time: 0:02:26.862330\n",
      "(30, 64, 64, 3)\n",
      "0.73805386\n",
      "[Epoch 0/5] [Batch 160/360] [D loss: 0.005226] [G loss: 21.832022] time: 0:02:27.025186\n",
      "(30, 64, 64, 3)\n",
      "0.6899292\n",
      "[Epoch 0/5] [Batch 161/360] [D loss: 0.004878] [G loss: 21.663408] time: 0:02:27.175592\n",
      "(30, 64, 64, 3)\n",
      "0.76498264\n",
      "[Epoch 0/5] [Batch 162/360] [D loss: 0.006013] [G loss: 21.220213] time: 0:02:27.371171\n",
      "(30, 64, 64, 3)\n",
      "0.6920287\n",
      "[Epoch 0/5] [Batch 163/360] [D loss: 0.004594] [G loss: 21.797070] time: 0:02:27.532501\n",
      "(30, 64, 64, 3)\n",
      "0.7175813\n",
      "[Epoch 0/5] [Batch 164/360] [D loss: 0.005165] [G loss: 21.304726] time: 0:02:27.760474\n",
      "(30, 64, 64, 3)\n",
      "0.7663326\n",
      "[Epoch 0/5] [Batch 165/360] [D loss: 0.006865] [G loss: 21.747213] time: 0:02:27.970623\n",
      "(30, 64, 64, 3)\n",
      "0.73790497\n",
      "[Epoch 0/5] [Batch 166/360] [D loss: 0.004575] [G loss: 22.035582] time: 0:02:28.156013\n",
      "(30, 64, 64, 3)\n",
      "0.7131322\n",
      "[Epoch 0/5] [Batch 167/360] [D loss: 0.004622] [G loss: 22.134373] time: 0:02:28.361364\n",
      "(30, 64, 64, 3)\n",
      "0.7467156\n",
      "[Epoch 0/5] [Batch 168/360] [D loss: 0.007694] [G loss: 21.620100] time: 0:02:28.570253\n",
      "(30, 64, 64, 3)\n",
      "0.73543745\n",
      "[Epoch 0/5] [Batch 169/360] [D loss: 0.003515] [G loss: 21.844011] time: 0:02:28.768712\n",
      "(30, 64, 64, 3)\n",
      "0.70486075\n",
      "[Epoch 0/5] [Batch 170/360] [D loss: 0.007560] [G loss: 20.810282] time: 0:02:28.962992\n",
      "(30, 64, 64, 3)\n",
      "0.699122\n",
      "[Epoch 0/5] [Batch 171/360] [D loss: 0.006291] [G loss: 20.982769] time: 0:02:29.169731\n",
      "(30, 64, 64, 3)\n",
      "0.7302776\n",
      "[Epoch 0/5] [Batch 172/360] [D loss: 0.003952] [G loss: 21.165592] time: 0:02:29.377341\n",
      "(30, 64, 64, 3)\n",
      "0.8237994\n",
      "[Epoch 0/5] [Batch 173/360] [D loss: 0.004635] [G loss: 21.029091] time: 0:02:29.564688\n",
      "(30, 64, 64, 3)\n",
      "0.76005083\n",
      "[Epoch 0/5] [Batch 174/360] [D loss: 0.004064] [G loss: 21.273434] time: 0:02:29.722152\n",
      "(30, 64, 64, 3)\n",
      "0.7100854\n",
      "[Epoch 0/5] [Batch 175/360] [D loss: 0.004049] [G loss: 21.325537] time: 0:02:29.954246\n",
      "(30, 64, 64, 3)\n",
      "0.650436\n",
      "[Epoch 0/5] [Batch 176/360] [D loss: 0.003777] [G loss: 21.981535] time: 0:02:30.131844\n",
      "(30, 64, 64, 3)\n",
      "0.7709503\n",
      "[Epoch 0/5] [Batch 177/360] [D loss: 0.004060] [G loss: 21.284750] time: 0:02:30.328180\n",
      "(30, 64, 64, 3)\n",
      "0.6850965\n",
      "[Epoch 0/5] [Batch 178/360] [D loss: 0.005197] [G loss: 20.829712] time: 0:02:30.531291\n",
      "(30, 64, 64, 3)\n",
      "0.72811633\n",
      "[Epoch 0/5] [Batch 179/360] [D loss: 0.010180] [G loss: 20.942110] time: 0:02:30.710241\n",
      "(30, 64, 64, 3)\n",
      "0.7118683\n",
      "[Epoch 0/5] [Batch 180/360] [D loss: 0.021682] [G loss: 21.113873] time: 0:02:30.880754\n",
      "(30, 64, 64, 3)\n",
      "0.7253241\n",
      "[Epoch 0/5] [Batch 181/360] [D loss: 0.059086] [G loss: 21.499578] time: 0:02:31.092551\n",
      "(30, 64, 64, 3)\n",
      "0.6986349\n",
      "[Epoch 0/5] [Batch 182/360] [D loss: 0.095216] [G loss: 21.278425] time: 0:02:31.308518\n",
      "(30, 64, 64, 3)\n",
      "0.71511966\n",
      "[Epoch 0/5] [Batch 183/360] [D loss: 0.022174] [G loss: 21.169138] time: 0:02:31.496421\n",
      "(30, 64, 64, 3)\n",
      "0.75717276\n",
      "[Epoch 0/5] [Batch 184/360] [D loss: 0.087520] [G loss: 20.297392] time: 0:02:31.720815\n",
      "(30, 64, 64, 3)\n",
      "0.6840756\n",
      "[Epoch 0/5] [Batch 185/360] [D loss: 0.127885] [G loss: 20.764269] time: 0:02:31.900544\n",
      "(30, 64, 64, 3)\n",
      "0.6988674\n",
      "[Epoch 0/5] [Batch 186/360] [D loss: 0.012839] [G loss: 21.051197] time: 0:02:32.080871\n",
      "(30, 64, 64, 3)\n",
      "0.73776346\n",
      "[Epoch 0/5] [Batch 187/360] [D loss: 0.050421] [G loss: 20.827662] time: 0:02:32.279411\n",
      "(30, 64, 64, 3)\n",
      "0.7458343\n",
      "[Epoch 0/5] [Batch 188/360] [D loss: 0.007357] [G loss: 21.707973] time: 0:02:32.491689\n",
      "(30, 64, 64, 3)\n",
      "0.7339032\n",
      "[Epoch 0/5] [Batch 189/360] [D loss: 0.011071] [G loss: 20.549242] time: 0:02:32.656950\n",
      "(30, 64, 64, 3)\n",
      "0.67983675\n",
      "[Epoch 0/5] [Batch 190/360] [D loss: 0.008143] [G loss: 21.205452] time: 0:02:32.849666\n",
      "(30, 64, 64, 3)\n",
      "0.78614336\n",
      "[Epoch 0/5] [Batch 191/360] [D loss: 0.010262] [G loss: 20.567020] time: 0:02:33.056202\n",
      "(30, 64, 64, 3)\n",
      "0.782983\n",
      "[Epoch 0/5] [Batch 192/360] [D loss: 0.006908] [G loss: 20.546278] time: 0:02:33.230574\n",
      "(30, 64, 64, 3)\n",
      "0.744054\n",
      "[Epoch 0/5] [Batch 193/360] [D loss: 0.006658] [G loss: 20.348690] time: 0:02:33.575364\n",
      "(30, 64, 64, 3)\n",
      "0.73216987\n",
      "[Epoch 0/5] [Batch 194/360] [D loss: 0.005795] [G loss: 20.371367] time: 0:02:33.765192\n",
      "(30, 64, 64, 3)\n",
      "0.6899789\n",
      "[Epoch 0/5] [Batch 195/360] [D loss: 0.005112] [G loss: 20.475201] time: 0:02:33.955516\n",
      "(30, 64, 64, 3)\n",
      "0.76003176\n",
      "[Epoch 0/5] [Batch 196/360] [D loss: 0.004330] [G loss: 20.777777] time: 0:02:34.160832\n",
      "(30, 64, 64, 3)\n",
      "0.75065786\n",
      "[Epoch 0/5] [Batch 197/360] [D loss: 0.004265] [G loss: 20.088335] time: 0:02:34.334657\n",
      "(30, 64, 64, 3)\n",
      "0.6991523\n",
      "[Epoch 0/5] [Batch 198/360] [D loss: 0.005516] [G loss: 20.413252] time: 0:02:34.551964\n",
      "(30, 64, 64, 3)\n",
      "0.7521227\n",
      "[Epoch 0/5] [Batch 199/360] [D loss: 0.005314] [G loss: 20.006470] time: 0:02:34.771183\n",
      "(30, 64, 64, 3)\n",
      "0.70553964\n",
      "[Epoch 0/5] [Batch 200/360] [D loss: 0.005383] [G loss: 19.672842] time: 0:02:34.990388\n",
      "(30, 64, 64, 3)\n",
      "0.6816532\n",
      "[Epoch 0/5] [Batch 201/360] [D loss: 0.004710] [G loss: 20.265093] time: 0:02:35.205106\n",
      "(30, 64, 64, 3)\n",
      "0.7203037\n",
      "[Epoch 0/5] [Batch 202/360] [D loss: 0.004179] [G loss: 20.814396] time: 0:02:35.416318\n",
      "(30, 64, 64, 3)\n",
      "0.75785303\n",
      "[Epoch 0/5] [Batch 203/360] [D loss: 0.004689] [G loss: 19.860996] time: 0:02:35.618609\n",
      "(30, 64, 64, 3)\n",
      "0.795481\n",
      "[Epoch 0/5] [Batch 204/360] [D loss: 0.003884] [G loss: 19.778522] time: 0:02:35.836265\n",
      "(30, 64, 64, 3)\n",
      "0.73262787\n",
      "[Epoch 0/5] [Batch 205/360] [D loss: 0.005224] [G loss: 19.440760] time: 0:02:36.010362\n",
      "(30, 64, 64, 3)\n",
      "0.7809368\n",
      "[Epoch 0/5] [Batch 206/360] [D loss: 0.003521] [G loss: 20.517738] time: 0:02:36.221970\n",
      "(30, 64, 64, 3)\n",
      "0.7800346\n",
      "[Epoch 0/5] [Batch 207/360] [D loss: 0.003502] [G loss: 20.439070] time: 0:02:36.435053\n",
      "(30, 64, 64, 3)\n",
      "0.6912895\n",
      "[Epoch 0/5] [Batch 208/360] [D loss: 0.004378] [G loss: 19.731628] time: 0:02:36.619829\n",
      "(30, 64, 64, 3)\n",
      "0.70280856\n",
      "[Epoch 0/5] [Batch 209/360] [D loss: 0.004405] [G loss: 20.169025] time: 0:02:36.809548\n",
      "(30, 64, 64, 3)\n",
      "0.7533956\n",
      "[Epoch 0/5] [Batch 210/360] [D loss: 0.006647] [G loss: 19.819710] time: 0:02:37.023074\n",
      "(30, 64, 64, 3)\n",
      "0.71523637\n",
      "[Epoch 0/5] [Batch 211/360] [D loss: 0.005328] [G loss: 19.291948] time: 0:02:37.191139\n",
      "(30, 64, 64, 3)\n",
      "0.74072295\n",
      "[Epoch 0/5] [Batch 212/360] [D loss: 0.005985] [G loss: 19.830008] time: 0:02:37.407459\n",
      "(30, 64, 64, 3)\n",
      "0.7858626\n",
      "[Epoch 0/5] [Batch 213/360] [D loss: 0.007253] [G loss: 19.324644] time: 0:02:37.617386\n",
      "(30, 64, 64, 3)\n",
      "0.703465\n",
      "[Epoch 0/5] [Batch 214/360] [D loss: 0.005554] [G loss: 19.557528] time: 0:02:37.859644\n",
      "(30, 64, 64, 3)\n",
      "0.8108079\n",
      "[Epoch 0/5] [Batch 215/360] [D loss: 0.004131] [G loss: 19.872610] time: 0:02:38.047562\n",
      "(30, 64, 64, 3)\n",
      "0.75132996\n",
      "[Epoch 0/5] [Batch 216/360] [D loss: 0.005182] [G loss: 19.305256] time: 0:02:38.258020\n",
      "(30, 64, 64, 3)\n",
      "0.79804105\n",
      "[Epoch 0/5] [Batch 217/360] [D loss: 0.005223] [G loss: 20.080688] time: 0:02:38.498996\n",
      "(30, 64, 64, 3)\n",
      "0.80678064\n",
      "[Epoch 0/5] [Batch 218/360] [D loss: 0.004115] [G loss: 19.488400] time: 0:02:38.695675\n",
      "(30, 64, 64, 3)\n",
      "0.7783349\n",
      "[Epoch 0/5] [Batch 219/360] [D loss: 0.004585] [G loss: 19.255194] time: 0:02:38.885851\n",
      "(30, 64, 64, 3)\n",
      "0.7522025\n",
      "[Epoch 0/5] [Batch 220/360] [D loss: 0.007821] [G loss: 18.870420] time: 0:02:39.053346\n",
      "(30, 64, 64, 3)\n",
      "0.76253086\n",
      "[Epoch 0/5] [Batch 221/360] [D loss: 0.004228] [G loss: 19.799822] time: 0:02:39.272501\n",
      "(30, 64, 64, 3)\n",
      "0.7748749\n",
      "[Epoch 0/5] [Batch 222/360] [D loss: 0.006159] [G loss: 18.506105] time: 0:02:39.495637\n",
      "(30, 64, 64, 3)\n",
      "0.8422127\n",
      "[Epoch 0/5] [Batch 223/360] [D loss: 0.007221] [G loss: 19.082033] time: 0:02:39.730303\n",
      "(30, 64, 64, 3)\n",
      "0.7691856\n",
      "[Epoch 0/5] [Batch 224/360] [D loss: 0.004567] [G loss: 19.180817] time: 0:02:39.935002\n",
      "(30, 64, 64, 3)\n",
      "0.748116\n",
      "[Epoch 0/5] [Batch 225/360] [D loss: 0.005905] [G loss: 19.381313] time: 0:02:40.152566\n",
      "(30, 64, 64, 3)\n",
      "0.72433424\n",
      "[Epoch 0/5] [Batch 226/360] [D loss: 0.005013] [G loss: 19.144453] time: 0:02:40.360105\n",
      "(30, 64, 64, 3)\n",
      "0.7556529\n",
      "[Epoch 0/5] [Batch 227/360] [D loss: 0.004973] [G loss: 18.896889] time: 0:02:40.546224\n",
      "(30, 64, 64, 3)\n",
      "0.7045899\n",
      "[Epoch 0/5] [Batch 228/360] [D loss: 0.004035] [G loss: 19.005075] time: 0:02:40.728157\n",
      "(30, 64, 64, 3)\n",
      "0.74115855\n",
      "[Epoch 0/5] [Batch 229/360] [D loss: 0.004116] [G loss: 18.913330] time: 0:02:40.925297\n",
      "(30, 64, 64, 3)\n",
      "0.74564236\n",
      "[Epoch 0/5] [Batch 230/360] [D loss: 0.004032] [G loss: 19.405169] time: 0:02:41.154102\n",
      "(30, 64, 64, 3)\n",
      "0.7702465\n",
      "[Epoch 0/5] [Batch 231/360] [D loss: 0.008475] [G loss: 19.216290] time: 0:02:41.343630\n",
      "(30, 64, 64, 3)\n",
      "0.71171755\n",
      "[Epoch 0/5] [Batch 232/360] [D loss: 0.004077] [G loss: 19.304739] time: 0:02:41.545326\n",
      "(30, 64, 64, 3)\n",
      "0.79918057\n",
      "[Epoch 0/5] [Batch 233/360] [D loss: 0.004201] [G loss: 18.802656] time: 0:02:41.791112\n",
      "(30, 64, 64, 3)\n",
      "0.80554277\n",
      "[Epoch 0/5] [Batch 234/360] [D loss: 0.002706] [G loss: 19.773149] time: 0:02:41.966748\n",
      "(30, 64, 64, 3)\n",
      "0.7766482\n",
      "[Epoch 0/5] [Batch 235/360] [D loss: 0.004276] [G loss: 18.380888] time: 0:02:42.189822\n",
      "(30, 64, 64, 3)\n",
      "0.7248693\n",
      "[Epoch 0/5] [Batch 236/360] [D loss: 0.004389] [G loss: 18.652290] time: 0:02:42.360844\n",
      "(30, 64, 64, 3)\n",
      "0.7681573\n",
      "[Epoch 0/5] [Batch 237/360] [D loss: 0.012657] [G loss: 19.025425] time: 0:02:42.586182\n",
      "(30, 64, 64, 3)\n",
      "0.7860141\n",
      "[Epoch 0/5] [Batch 238/360] [D loss: 0.014505] [G loss: 19.102051] time: 0:02:42.801190\n",
      "(30, 64, 64, 3)\n",
      "0.7604416\n",
      "[Epoch 0/5] [Batch 239/360] [D loss: 0.010991] [G loss: 18.594877] time: 0:02:42.995728\n",
      "(30, 64, 64, 3)\n",
      "0.7246766\n",
      "[Epoch 0/5] [Batch 240/360] [D loss: 0.009801] [G loss: 18.698719] time: 0:02:43.224375\n",
      "(30, 64, 64, 3)\n",
      "0.8070337\n",
      "[Epoch 0/5] [Batch 241/360] [D loss: 0.004403] [G loss: 18.619230] time: 0:02:43.410490\n",
      "(30, 64, 64, 3)\n",
      "0.7917769\n",
      "[Epoch 0/5] [Batch 242/360] [D loss: 0.004294] [G loss: 18.256693] time: 0:02:43.640869\n",
      "(30, 64, 64, 3)\n",
      "0.7631939\n",
      "[Epoch 0/5] [Batch 243/360] [D loss: 0.004599] [G loss: 18.555643] time: 0:02:43.882276\n",
      "(30, 64, 64, 3)\n",
      "0.7376079\n",
      "[Epoch 0/5] [Batch 244/360] [D loss: 0.003923] [G loss: 19.008333] time: 0:02:44.064883\n",
      "(30, 64, 64, 3)\n",
      "0.810134\n",
      "[Epoch 0/5] [Batch 245/360] [D loss: 0.003805] [G loss: 18.720051] time: 0:02:44.318861\n",
      "(30, 64, 64, 3)\n",
      "0.7173231\n",
      "[Epoch 0/5] [Batch 246/360] [D loss: 0.004901] [G loss: 18.427313] time: 0:02:44.510467\n",
      "(30, 64, 64, 3)\n",
      "0.73673564\n",
      "[Epoch 0/5] [Batch 247/360] [D loss: 0.003404] [G loss: 18.357851] time: 0:02:44.683320\n",
      "(30, 64, 64, 3)\n",
      "0.7802279\n",
      "[Epoch 0/5] [Batch 248/360] [D loss: 0.003408] [G loss: 19.031725] time: 0:02:44.859590\n",
      "(30, 64, 64, 3)\n",
      "0.748012\n",
      "[Epoch 0/5] [Batch 249/360] [D loss: 0.003558] [G loss: 18.577662] time: 0:02:45.019388\n",
      "(30, 64, 64, 3)\n",
      "0.64493006\n",
      "[Epoch 0/5] [Batch 250/360] [D loss: 0.005923] [G loss: 18.572920] time: 0:02:45.228140\n",
      "(30, 64, 64, 3)\n",
      "0.7659469\n",
      "[Epoch 0/5] [Batch 251/360] [D loss: 0.005220] [G loss: 18.207027] time: 0:02:45.452348\n",
      "(30, 64, 64, 3)\n",
      "0.68739015\n",
      "[Epoch 0/5] [Batch 252/360] [D loss: 0.005511] [G loss: 18.857718] time: 0:02:45.655667\n",
      "(30, 64, 64, 3)\n",
      "0.66253847\n",
      "[Epoch 0/5] [Batch 253/360] [D loss: 0.019234] [G loss: 17.984903] time: 0:02:45.843050\n",
      "(30, 64, 64, 3)\n",
      "0.8169797\n",
      "[Epoch 0/5] [Batch 254/360] [D loss: 0.060318] [G loss: 19.476921] time: 0:02:46.066988\n",
      "(30, 64, 64, 3)\n",
      "0.74439985\n",
      "[Epoch 0/5] [Batch 255/360] [D loss: 0.316534] [G loss: 18.670830] time: 0:02:46.255811\n",
      "(30, 64, 64, 3)\n",
      "0.7276368\n",
      "[Epoch 0/5] [Batch 256/360] [D loss: 0.122206] [G loss: 17.910076] time: 0:02:46.438010\n",
      "(30, 64, 64, 3)\n",
      "0.7160016\n",
      "[Epoch 0/5] [Batch 257/360] [D loss: 0.013852] [G loss: 18.701939] time: 0:02:46.641616\n",
      "(30, 64, 64, 3)\n",
      "0.72477347\n",
      "[Epoch 0/5] [Batch 258/360] [D loss: 0.025299] [G loss: 17.705444] time: 0:02:46.848418\n",
      "(30, 64, 64, 3)\n",
      "0.7858483\n",
      "[Epoch 0/5] [Batch 259/360] [D loss: 0.004691] [G loss: 17.753174] time: 0:02:47.040410\n",
      "(30, 64, 64, 3)\n",
      "0.7875426\n",
      "[Epoch 0/5] [Batch 260/360] [D loss: 0.005393] [G loss: 17.914730] time: 0:02:47.244979\n",
      "(30, 64, 64, 3)\n",
      "0.7443164\n",
      "[Epoch 0/5] [Batch 261/360] [D loss: 0.005549] [G loss: 18.249554] time: 0:02:47.459558\n",
      "(30, 64, 64, 3)\n",
      "0.7898237\n",
      "[Epoch 0/5] [Batch 262/360] [D loss: 0.005168] [G loss: 18.069458] time: 0:02:47.643819\n",
      "(30, 64, 64, 3)\n",
      "0.7427611\n",
      "[Epoch 0/5] [Batch 263/360] [D loss: 0.004750] [G loss: 18.596827] time: 0:02:47.852154\n",
      "(30, 64, 64, 3)\n",
      "0.7519443\n",
      "[Epoch 0/5] [Batch 264/360] [D loss: 0.004650] [G loss: 18.862680] time: 0:02:48.021787\n",
      "(30, 64, 64, 3)\n",
      "0.7515319\n",
      "[Epoch 0/5] [Batch 265/360] [D loss: 0.007191] [G loss: 18.061451] time: 0:02:48.230982\n",
      "(30, 64, 64, 3)\n",
      "0.70239186\n",
      "[Epoch 0/5] [Batch 266/360] [D loss: 0.005591] [G loss: 18.198130] time: 0:02:48.401649\n",
      "(30, 64, 64, 3)\n",
      "0.7778442\n",
      "[Epoch 0/5] [Batch 267/360] [D loss: 0.004962] [G loss: 18.167566] time: 0:02:48.604095\n",
      "(30, 64, 64, 3)\n",
      "0.6960225\n",
      "[Epoch 0/5] [Batch 268/360] [D loss: 0.003630] [G loss: 18.095066] time: 0:02:48.819697\n",
      "(30, 64, 64, 3)\n",
      "0.7206407\n",
      "[Epoch 0/5] [Batch 269/360] [D loss: 0.004348] [G loss: 17.378683] time: 0:02:49.007888\n",
      "(30, 64, 64, 3)\n",
      "0.73921585\n",
      "[Epoch 0/5] [Batch 270/360] [D loss: 0.003414] [G loss: 18.661472] time: 0:02:49.235886\n",
      "(30, 64, 64, 3)\n",
      "0.77003986\n",
      "[Epoch 0/5] [Batch 271/360] [D loss: 0.005022] [G loss: 17.219669] time: 0:02:49.412988\n",
      "(30, 64, 64, 3)\n",
      "0.76250166\n",
      "[Epoch 0/5] [Batch 272/360] [D loss: 0.003373] [G loss: 18.290649] time: 0:02:49.594365\n",
      "(30, 64, 64, 3)\n",
      "0.75990087\n",
      "[Epoch 0/5] [Batch 273/360] [D loss: 0.004982] [G loss: 18.162367] time: 0:02:49.826567\n",
      "(30, 64, 64, 3)\n",
      "0.6851118\n",
      "[Epoch 0/5] [Batch 274/360] [D loss: 0.004677] [G loss: 17.733223] time: 0:02:50.040787\n",
      "(30, 64, 64, 3)\n",
      "0.71609014\n",
      "[Epoch 0/5] [Batch 275/360] [D loss: 0.005353] [G loss: 17.092268] time: 0:02:50.213849\n",
      "(30, 64, 64, 3)\n",
      "0.742565\n",
      "[Epoch 0/5] [Batch 276/360] [D loss: 0.004504] [G loss: 16.978333] time: 0:02:50.423394\n",
      "(30, 64, 64, 3)\n",
      "0.741396\n",
      "[Epoch 0/5] [Batch 277/360] [D loss: 0.004736] [G loss: 17.439989] time: 0:02:50.649370\n",
      "(30, 64, 64, 3)\n",
      "0.82205796\n",
      "[Epoch 0/5] [Batch 278/360] [D loss: 0.004357] [G loss: 18.328596] time: 0:02:50.856246\n",
      "(30, 64, 64, 3)\n",
      "0.78237873\n",
      "[Epoch 0/5] [Batch 279/360] [D loss: 0.003912] [G loss: 16.894800] time: 0:02:51.036791\n",
      "(30, 64, 64, 3)\n",
      "0.75504524\n",
      "[Epoch 0/5] [Batch 280/360] [D loss: 0.003122] [G loss: 17.951532] time: 0:02:51.246392\n",
      "(30, 64, 64, 3)\n",
      "0.7524951\n",
      "[Epoch 0/5] [Batch 281/360] [D loss: 0.006059] [G loss: 17.594898] time: 0:02:51.442967\n",
      "(30, 64, 64, 3)\n",
      "0.78371996\n",
      "[Epoch 0/5] [Batch 282/360] [D loss: 0.003504] [G loss: 17.182140] time: 0:02:51.676877\n",
      "(30, 64, 64, 3)\n",
      "0.8190525\n",
      "[Epoch 0/5] [Batch 283/360] [D loss: 0.003640] [G loss: 18.624531] time: 0:02:51.881930\n",
      "(30, 64, 64, 3)\n",
      "0.7171524\n",
      "[Epoch 0/5] [Batch 284/360] [D loss: 0.003519] [G loss: 17.075802] time: 0:02:52.072631\n",
      "(30, 64, 64, 3)\n",
      "0.7366927\n",
      "[Epoch 0/5] [Batch 285/360] [D loss: 0.004717] [G loss: 16.652967] time: 0:02:52.297063\n",
      "(30, 64, 64, 3)\n",
      "0.7381428\n",
      "[Epoch 0/5] [Batch 286/360] [D loss: 0.003548] [G loss: 17.802803] time: 0:02:52.487824\n",
      "(30, 64, 64, 3)\n",
      "0.739322\n",
      "[Epoch 0/5] [Batch 287/360] [D loss: 0.004231] [G loss: 16.945568] time: 0:02:52.701750\n",
      "(30, 64, 64, 3)\n",
      "0.8017549\n",
      "[Epoch 0/5] [Batch 288/360] [D loss: 0.003775] [G loss: 17.469614] time: 0:02:52.905990\n",
      "(30, 64, 64, 3)\n",
      "0.830808\n",
      "[Epoch 0/5] [Batch 289/360] [D loss: 0.004408] [G loss: 17.096733] time: 0:02:53.101436\n",
      "(30, 64, 64, 3)\n",
      "0.78838235\n",
      "[Epoch 0/5] [Batch 290/360] [D loss: 0.003555] [G loss: 16.738586] time: 0:02:53.316298\n",
      "(30, 64, 64, 3)\n",
      "0.79905605\n",
      "[Epoch 0/5] [Batch 291/360] [D loss: 0.004168] [G loss: 17.105068] time: 0:02:53.481274\n",
      "(30, 64, 64, 3)\n",
      "0.75910425\n",
      "[Epoch 0/5] [Batch 292/360] [D loss: 0.003656] [G loss: 17.214102] time: 0:02:53.677975\n",
      "(30, 64, 64, 3)\n",
      "0.75241905\n",
      "[Epoch 0/5] [Batch 293/360] [D loss: 0.003262] [G loss: 17.045849] time: 0:02:53.875391\n",
      "(30, 64, 64, 3)\n",
      "0.8022451\n",
      "[Epoch 0/5] [Batch 294/360] [D loss: 0.003704] [G loss: 16.973854] time: 0:02:54.050135\n",
      "(30, 64, 64, 3)\n",
      "0.7688973\n",
      "[Epoch 0/5] [Batch 295/360] [D loss: 0.004257] [G loss: 16.660614] time: 0:02:54.238336\n",
      "(30, 64, 64, 3)\n",
      "0.7554781\n",
      "[Epoch 0/5] [Batch 296/360] [D loss: 0.004640] [G loss: 16.379541] time: 0:02:54.403736\n",
      "(30, 64, 64, 3)\n",
      "0.6888897\n",
      "[Epoch 0/5] [Batch 297/360] [D loss: 0.006695] [G loss: 16.966276] time: 0:02:54.617790\n",
      "(30, 64, 64, 3)\n",
      "0.7799003\n",
      "[Epoch 0/5] [Batch 298/360] [D loss: 0.004136] [G loss: 16.690910] time: 0:02:54.821837\n",
      "(30, 64, 64, 3)\n",
      "0.68228406\n",
      "[Epoch 0/5] [Batch 299/360] [D loss: 0.003990] [G loss: 16.099737] time: 0:02:55.050871\n",
      "(30, 64, 64, 3)\n",
      "0.79642755\n",
      "[Epoch 0/5] [Batch 300/360] [D loss: 0.004544] [G loss: 16.636393] time: 0:02:55.235398\n",
      "(30, 64, 64, 3)\n",
      "0.7825256\n",
      "[Epoch 0/5] [Batch 301/360] [D loss: 0.003586] [G loss: 17.608622] time: 0:02:55.413149\n",
      "(30, 64, 64, 3)\n",
      "0.78830177\n",
      "[Epoch 0/5] [Batch 302/360] [D loss: 0.004057] [G loss: 16.693066] time: 0:02:55.573620\n",
      "(30, 64, 64, 3)\n",
      "0.7357879\n",
      "[Epoch 0/5] [Batch 303/360] [D loss: 0.004395] [G loss: 16.326082] time: 0:02:55.792326\n",
      "(30, 64, 64, 3)\n",
      "0.8062961\n",
      "[Epoch 0/5] [Batch 304/360] [D loss: 0.005540] [G loss: 16.638878] time: 0:02:56.001375\n",
      "(30, 64, 64, 3)\n",
      "0.7135234\n",
      "[Epoch 0/5] [Batch 305/360] [D loss: 0.003224] [G loss: 16.870844] time: 0:02:56.208120\n",
      "(30, 64, 64, 3)\n",
      "0.78462344\n",
      "[Epoch 0/5] [Batch 306/360] [D loss: 0.003998] [G loss: 16.453732] time: 0:02:56.405715\n",
      "(30, 64, 64, 3)\n",
      "0.7701834\n",
      "[Epoch 0/5] [Batch 307/360] [D loss: 0.003674] [G loss: 16.507215] time: 0:02:56.616685\n",
      "(30, 64, 64, 3)\n",
      "0.7568538\n",
      "[Epoch 0/5] [Batch 308/360] [D loss: 0.004044] [G loss: 17.109625] time: 0:02:56.827164\n",
      "(30, 64, 64, 3)\n",
      "0.8089984\n",
      "[Epoch 0/5] [Batch 309/360] [D loss: 0.003565] [G loss: 16.722094] time: 0:02:57.040905\n",
      "(30, 64, 64, 3)\n",
      "0.77272224\n",
      "[Epoch 0/5] [Batch 310/360] [D loss: 0.003871] [G loss: 16.226736] time: 0:02:57.232503\n",
      "(30, 64, 64, 3)\n",
      "0.8011615\n",
      "[Epoch 0/5] [Batch 311/360] [D loss: 0.005058] [G loss: 16.839121] time: 0:02:57.462585\n",
      "(30, 64, 64, 3)\n",
      "0.84341496\n",
      "[Epoch 0/5] [Batch 312/360] [D loss: 0.003348] [G loss: 16.636749] time: 0:02:57.676246\n",
      "(30, 64, 64, 3)\n",
      "0.80351686\n",
      "[Epoch 0/5] [Batch 313/360] [D loss: 0.004095] [G loss: 16.732443] time: 0:02:57.905173\n",
      "(30, 64, 64, 3)\n",
      "0.78452826\n",
      "[Epoch 0/5] [Batch 314/360] [D loss: 0.007152] [G loss: 16.960217] time: 0:02:58.104304\n",
      "(30, 64, 64, 3)\n",
      "0.7586022\n",
      "[Epoch 0/5] [Batch 315/360] [D loss: 0.003730] [G loss: 17.010891] time: 0:02:58.303769\n",
      "(30, 64, 64, 3)\n",
      "0.81850624\n",
      "[Epoch 0/5] [Batch 316/360] [D loss: 0.003288] [G loss: 16.363802] time: 0:02:58.476422\n",
      "(30, 64, 64, 3)\n",
      "0.7612114\n",
      "[Epoch 0/5] [Batch 317/360] [D loss: 0.003396] [G loss: 16.599684] time: 0:02:58.690200\n",
      "(30, 64, 64, 3)\n",
      "0.7575524\n",
      "[Epoch 0/5] [Batch 318/360] [D loss: 0.004398] [G loss: 16.465334] time: 0:02:58.916450\n",
      "(30, 64, 64, 3)\n",
      "0.76719594\n",
      "[Epoch 0/5] [Batch 319/360] [D loss: 0.002829] [G loss: 17.142086] time: 0:02:59.127094\n",
      "(30, 64, 64, 3)\n",
      "0.82172227\n",
      "[Epoch 0/5] [Batch 320/360] [D loss: 0.003686] [G loss: 16.429943] time: 0:02:59.332431\n",
      "(30, 64, 64, 3)\n",
      "0.7533719\n",
      "[Epoch 0/5] [Batch 321/360] [D loss: 0.004500] [G loss: 15.882351] time: 0:02:59.531692\n",
      "(30, 64, 64, 3)\n",
      "0.8233974\n",
      "[Epoch 0/5] [Batch 322/360] [D loss: 0.002654] [G loss: 16.432064] time: 0:02:59.707847\n",
      "(30, 64, 64, 3)\n",
      "0.82469076\n",
      "[Epoch 0/5] [Batch 323/360] [D loss: 0.003618] [G loss: 16.786449] time: 0:02:59.894578\n",
      "(30, 64, 64, 3)\n",
      "0.75797534\n",
      "[Epoch 0/5] [Batch 324/360] [D loss: 0.003187] [G loss: 15.964214] time: 0:03:00.070952\n",
      "(30, 64, 64, 3)\n",
      "0.8079795\n",
      "[Epoch 0/5] [Batch 325/360] [D loss: 0.004036] [G loss: 16.085417] time: 0:03:00.262069\n",
      "(30, 64, 64, 3)\n",
      "0.7662625\n",
      "[Epoch 0/5] [Batch 326/360] [D loss: 0.004866] [G loss: 15.724848] time: 0:03:00.431144\n",
      "(30, 64, 64, 3)\n",
      "0.763128\n",
      "[Epoch 0/5] [Batch 327/360] [D loss: 0.002600] [G loss: 16.264114] time: 0:03:00.662637\n",
      "(30, 64, 64, 3)\n",
      "0.8187248\n",
      "[Epoch 0/5] [Batch 328/360] [D loss: 0.004295] [G loss: 15.980284] time: 0:03:00.851484\n",
      "(30, 64, 64, 3)\n",
      "0.7467795\n",
      "[Epoch 0/5] [Batch 329/360] [D loss: 0.004297] [G loss: 15.697162] time: 0:03:01.077218\n",
      "(30, 64, 64, 3)\n",
      "0.7700519\n",
      "[Epoch 0/5] [Batch 330/360] [D loss: 0.003725] [G loss: 15.950444] time: 0:03:01.283667\n",
      "(30, 64, 64, 3)\n",
      "0.7656393\n",
      "[Epoch 0/5] [Batch 331/360] [D loss: 0.010147] [G loss: 15.816264] time: 0:03:01.449153\n",
      "(30, 64, 64, 3)\n",
      "0.7842393\n",
      "[Epoch 0/5] [Batch 332/360] [D loss: 0.024345] [G loss: 16.031410] time: 0:03:01.631975\n",
      "(30, 64, 64, 3)\n",
      "0.7501275\n",
      "[Epoch 0/5] [Batch 333/360] [D loss: 0.006968] [G loss: 16.070568] time: 0:03:01.850466\n",
      "(30, 64, 64, 3)\n",
      "0.79534405\n",
      "[Epoch 0/5] [Batch 334/360] [D loss: 0.023009] [G loss: 16.120764] time: 0:03:02.085796\n",
      "(30, 64, 64, 3)\n",
      "0.7935074\n",
      "[Epoch 0/5] [Batch 335/360] [D loss: 0.046940] [G loss: 15.680136] time: 0:03:02.271128\n",
      "(30, 64, 64, 3)\n",
      "0.78418905\n",
      "[Epoch 0/5] [Batch 336/360] [D loss: 0.146115] [G loss: 16.457993] time: 0:03:02.478614\n",
      "(30, 64, 64, 3)\n",
      "0.75877565\n",
      "[Epoch 0/5] [Batch 337/360] [D loss: 0.408503] [G loss: 15.649289] time: 0:03:02.659942\n",
      "(30, 64, 64, 3)\n",
      "0.80507296\n",
      "[Epoch 0/5] [Batch 338/360] [D loss: 0.014646] [G loss: 16.067209] time: 0:03:02.851853\n",
      "(30, 64, 64, 3)\n",
      "0.7455309\n",
      "[Epoch 0/5] [Batch 339/360] [D loss: 0.072509] [G loss: 15.743466] time: 0:03:03.116942\n",
      "(30, 64, 64, 3)\n",
      "0.82345134\n",
      "[Epoch 0/5] [Batch 340/360] [D loss: 0.006045] [G loss: 16.497253] time: 0:03:03.395416\n",
      "(30, 64, 64, 3)\n",
      "0.82011145\n",
      "[Epoch 0/5] [Batch 341/360] [D loss: 0.013923] [G loss: 15.787183] time: 0:03:03.601769\n",
      "(30, 64, 64, 3)\n",
      "0.79009753\n",
      "[Epoch 0/5] [Batch 342/360] [D loss: 0.005562] [G loss: 15.571603] time: 0:03:03.785243\n",
      "(30, 64, 64, 3)\n",
      "0.78429586\n",
      "[Epoch 0/5] [Batch 343/360] [D loss: 0.008527] [G loss: 16.075327] time: 0:03:03.975952\n",
      "(30, 64, 64, 3)\n",
      "0.75202674\n",
      "[Epoch 0/5] [Batch 344/360] [D loss: 0.005125] [G loss: 15.743366] time: 0:03:04.175536\n",
      "(30, 64, 64, 3)\n",
      "0.751126\n",
      "[Epoch 0/5] [Batch 345/360] [D loss: 0.005091] [G loss: 15.811646] time: 0:03:04.394855\n",
      "(30, 64, 64, 3)\n",
      "0.7255834\n",
      "[Epoch 0/5] [Batch 346/360] [D loss: 0.005434] [G loss: 15.410718] time: 0:03:04.585811\n",
      "(30, 64, 64, 3)\n",
      "0.8165625\n",
      "[Epoch 0/5] [Batch 347/360] [D loss: 0.004901] [G loss: 15.417832] time: 0:03:04.780412\n",
      "(30, 64, 64, 3)\n",
      "0.70489883\n",
      "[Epoch 0/5] [Batch 348/360] [D loss: 0.005700] [G loss: 15.594007] time: 0:03:04.995897\n",
      "(30, 64, 64, 3)\n",
      "0.8001995\n",
      "[Epoch 0/5] [Batch 349/360] [D loss: 0.002961] [G loss: 15.784444] time: 0:03:05.203673\n",
      "(30, 64, 64, 3)\n",
      "0.8131946\n",
      "[Epoch 0/5] [Batch 350/360] [D loss: 0.004939] [G loss: 16.341028] time: 0:03:05.426149\n",
      "(30, 64, 64, 3)\n",
      "0.7513499\n",
      "[Epoch 0/5] [Batch 351/360] [D loss: 0.004512] [G loss: 15.510555] time: 0:03:05.650536\n",
      "(30, 64, 64, 3)\n",
      "0.7728014\n",
      "[Epoch 0/5] [Batch 352/360] [D loss: 0.003747] [G loss: 15.811953] time: 0:03:05.879590\n",
      "(30, 64, 64, 3)\n",
      "0.7573573\n",
      "[Epoch 0/5] [Batch 353/360] [D loss: 0.003094] [G loss: 16.124603] time: 0:03:06.088536\n",
      "(30, 64, 64, 3)\n",
      "0.8004399\n",
      "[Epoch 0/5] [Batch 354/360] [D loss: 0.003011] [G loss: 16.114723] time: 0:03:06.284225\n",
      "(30, 64, 64, 3)\n",
      "0.7504163\n",
      "[Epoch 0/5] [Batch 355/360] [D loss: 0.004778] [G loss: 15.627082] time: 0:03:06.482415\n",
      "(30, 64, 64, 3)\n",
      "0.8086811\n",
      "[Epoch 0/5] [Batch 356/360] [D loss: 0.003027] [G loss: 16.591644] time: 0:03:06.685640\n",
      "(30, 64, 64, 3)\n",
      "0.7988686\n",
      "[Epoch 0/5] [Batch 357/360] [D loss: 0.003426] [G loss: 15.432536] time: 0:03:06.892288\n",
      "(30, 64, 64, 3)\n",
      "0.8262453\n",
      "[Epoch 0/5] [Batch 358/360] [D loss: 0.004113] [G loss: 15.442795] time: 0:03:07.099523\n",
      "(30, 64, 64, 3)\n",
      "0.8303111\n",
      "[Epoch 1/5] [Batch 0/360] [D loss: 0.004771] [G loss: 14.632837] time: 0:03:07.374464\n",
      "(30, 64, 64, 3)\n",
      "0.8217961\n",
      "[Epoch 1/5] [Batch 1/360] [D loss: 0.004438] [G loss: 14.547250] time: 0:03:07.611936\n",
      "(30, 64, 64, 3)\n",
      "0.8533416\n",
      "[Epoch 1/5] [Batch 2/360] [D loss: 0.003604] [G loss: 15.166718] time: 0:03:07.833744\n",
      "(30, 64, 64, 3)\n",
      "0.7928872\n",
      "[Epoch 1/5] [Batch 3/360] [D loss: 0.005319] [G loss: 15.491902] time: 0:03:08.062976\n",
      "(30, 64, 64, 3)\n",
      "0.7809291\n",
      "[Epoch 1/5] [Batch 4/360] [D loss: 0.003738] [G loss: 15.332727] time: 0:03:08.239839\n",
      "(30, 64, 64, 3)\n",
      "0.7452016\n",
      "[Epoch 1/5] [Batch 5/360] [D loss: 0.003992] [G loss: 15.228599] time: 0:03:08.424529\n",
      "(30, 64, 64, 3)\n",
      "0.7698111\n",
      "[Epoch 1/5] [Batch 6/360] [D loss: 0.003316] [G loss: 15.488581] time: 0:03:08.595465\n",
      "(30, 64, 64, 3)\n",
      "0.7734607\n",
      "[Epoch 1/5] [Batch 7/360] [D loss: 0.003235] [G loss: 15.321564] time: 0:03:08.786767\n",
      "(30, 64, 64, 3)\n",
      "0.80780506\n",
      "[Epoch 1/5] [Batch 8/360] [D loss: 0.004980] [G loss: 14.625796] time: 0:03:08.968209\n",
      "(30, 64, 64, 3)\n",
      "0.75939614\n",
      "[Epoch 1/5] [Batch 9/360] [D loss: 0.003094] [G loss: 15.481733] time: 0:03:09.169812\n",
      "(30, 64, 64, 3)\n",
      "0.80304956\n",
      "[Epoch 1/5] [Batch 10/360] [D loss: 0.004602] [G loss: 15.873953] time: 0:03:09.375545\n",
      "(30, 64, 64, 3)\n",
      "0.7913478\n",
      "[Epoch 1/5] [Batch 11/360] [D loss: 0.003544] [G loss: 15.598336] time: 0:03:09.555094\n",
      "(30, 64, 64, 3)\n",
      "0.83034325\n",
      "[Epoch 1/5] [Batch 12/360] [D loss: 0.003167] [G loss: 15.001308] time: 0:03:09.741551\n",
      "(30, 64, 64, 3)\n",
      "0.81987286\n",
      "[Epoch 1/5] [Batch 13/360] [D loss: 0.003653] [G loss: 15.489408] time: 0:03:09.913469\n",
      "(30, 64, 64, 3)\n",
      "0.7411945\n",
      "[Epoch 1/5] [Batch 14/360] [D loss: 0.007628] [G loss: 15.143878] time: 0:03:10.125657\n",
      "(30, 64, 64, 3)\n",
      "0.79013914\n",
      "[Epoch 1/5] [Batch 15/360] [D loss: 0.004641] [G loss: 15.125076] time: 0:03:10.313339\n",
      "(30, 64, 64, 3)\n",
      "0.70341426\n",
      "[Epoch 1/5] [Batch 16/360] [D loss: 0.005779] [G loss: 14.396868] time: 0:03:10.503061\n",
      "(30, 64, 64, 3)\n",
      "0.8267915\n",
      "[Epoch 1/5] [Batch 17/360] [D loss: 0.003526] [G loss: 15.110131] time: 0:03:10.719823\n",
      "(30, 64, 64, 3)\n",
      "0.7462489\n",
      "[Epoch 1/5] [Batch 18/360] [D loss: 0.015293] [G loss: 14.218528] time: 0:03:10.930522\n",
      "(30, 64, 64, 3)\n",
      "0.7729786\n",
      "[Epoch 1/5] [Batch 19/360] [D loss: 0.004900] [G loss: 15.701384] time: 0:03:11.109634\n",
      "(30, 64, 64, 3)\n",
      "0.83290577\n",
      "[Epoch 1/5] [Batch 20/360] [D loss: 0.003141] [G loss: 14.922021] time: 0:03:11.358717\n",
      "(30, 64, 64, 3)\n",
      "0.82011765\n",
      "[Epoch 1/5] [Batch 21/360] [D loss: 0.002467] [G loss: 14.906857] time: 0:03:11.579783\n",
      "(30, 64, 64, 3)\n",
      "0.7468934\n",
      "[Epoch 1/5] [Batch 22/360] [D loss: 0.003990] [G loss: 14.678727] time: 0:03:11.755791\n",
      "(30, 64, 64, 3)\n",
      "0.82097197\n",
      "[Epoch 1/5] [Batch 23/360] [D loss: 0.003866] [G loss: 14.519544] time: 0:03:11.954516\n",
      "(30, 64, 64, 3)\n",
      "0.80668706\n",
      "[Epoch 1/5] [Batch 24/360] [D loss: 0.003382] [G loss: 14.515939] time: 0:03:12.191878\n",
      "(30, 64, 64, 3)\n",
      "0.8068137\n",
      "[Epoch 1/5] [Batch 25/360] [D loss: 0.004757] [G loss: 14.655063] time: 0:03:12.386699\n",
      "(30, 64, 64, 3)\n",
      "0.8215949\n",
      "[Epoch 1/5] [Batch 26/360] [D loss: 0.002925] [G loss: 15.312774] time: 0:03:12.610449\n",
      "(30, 64, 64, 3)\n",
      "0.7921641\n",
      "[Epoch 1/5] [Batch 27/360] [D loss: 0.003847] [G loss: 14.920681] time: 0:03:12.866251\n",
      "(30, 64, 64, 3)\n",
      "0.81752616\n",
      "[Epoch 1/5] [Batch 28/360] [D loss: 0.003039] [G loss: 14.443041] time: 0:03:13.043950\n",
      "(30, 64, 64, 3)\n",
      "0.85590315\n",
      "[Epoch 1/5] [Batch 29/360] [D loss: 0.007251] [G loss: 14.770486] time: 0:03:13.254534\n",
      "(30, 64, 64, 3)\n",
      "0.75610095\n",
      "[Epoch 1/5] [Batch 30/360] [D loss: 0.004729] [G loss: 14.493303] time: 0:03:13.462822\n",
      "(30, 64, 64, 3)\n",
      "0.8272709\n",
      "[Epoch 1/5] [Batch 31/360] [D loss: 0.002818] [G loss: 15.159875] time: 0:03:13.639682\n",
      "(30, 64, 64, 3)\n",
      "0.8082762\n",
      "[Epoch 1/5] [Batch 32/360] [D loss: 0.003692] [G loss: 14.608438] time: 0:03:13.847034\n",
      "(30, 64, 64, 3)\n",
      "0.787191\n",
      "[Epoch 1/5] [Batch 33/360] [D loss: 0.003533] [G loss: 14.084882] time: 0:03:14.056449\n",
      "(30, 64, 64, 3)\n",
      "0.82802534\n",
      "[Epoch 1/5] [Batch 34/360] [D loss: 0.017298] [G loss: 14.568651] time: 0:03:14.251362\n",
      "(30, 64, 64, 3)\n",
      "0.70508677\n",
      "[Epoch 1/5] [Batch 35/360] [D loss: 0.044722] [G loss: 14.580846] time: 0:03:14.437566\n",
      "(30, 64, 64, 3)\n",
      "0.786622\n",
      "[Epoch 1/5] [Batch 36/360] [D loss: 0.015652] [G loss: 14.104079] time: 0:03:14.655669\n",
      "(30, 64, 64, 3)\n",
      "0.763658\n",
      "[Epoch 1/5] [Batch 37/360] [D loss: 0.021089] [G loss: 14.344448] time: 0:03:14.857326\n",
      "(30, 64, 64, 3)\n",
      "0.8102062\n",
      "[Epoch 1/5] [Batch 38/360] [D loss: 0.013440] [G loss: 13.903037] time: 0:03:15.087063\n",
      "(30, 64, 64, 3)\n",
      "0.833981\n",
      "[Epoch 1/5] [Batch 39/360] [D loss: 0.003758] [G loss: 14.476164] time: 0:03:15.296131\n",
      "(30, 64, 64, 3)\n",
      "0.8403697\n",
      "[Epoch 1/5] [Batch 40/360] [D loss: 0.008181] [G loss: 14.180944] time: 0:03:15.477447\n",
      "(30, 64, 64, 3)\n",
      "0.8622529\n",
      "[Epoch 1/5] [Batch 41/360] [D loss: 0.004244] [G loss: 14.424094] time: 0:03:15.690831\n",
      "(30, 64, 64, 3)\n",
      "0.8044092\n",
      "[Epoch 1/5] [Batch 42/360] [D loss: 0.017650] [G loss: 14.773619] time: 0:03:15.868828\n",
      "(30, 64, 64, 3)\n",
      "0.7117174\n",
      "[Epoch 1/5] [Batch 43/360] [D loss: 0.006236] [G loss: 14.754608] time: 0:03:16.095143\n",
      "(30, 64, 64, 3)\n",
      "0.8272183\n",
      "[Epoch 1/5] [Batch 44/360] [D loss: 0.005216] [G loss: 14.443670] time: 0:03:16.274626\n",
      "(30, 64, 64, 3)\n",
      "0.75021935\n",
      "[Epoch 1/5] [Batch 45/360] [D loss: 0.004869] [G loss: 15.088010] time: 0:03:16.443441\n",
      "(30, 64, 64, 3)\n",
      "0.80698663\n",
      "[Epoch 1/5] [Batch 46/360] [D loss: 0.016735] [G loss: 14.350901] time: 0:03:16.661033\n",
      "(30, 64, 64, 3)\n",
      "0.7590025\n",
      "[Epoch 1/5] [Batch 47/360] [D loss: 0.004288] [G loss: 14.435834] time: 0:03:16.832437\n",
      "(30, 64, 64, 3)\n",
      "0.7900465\n",
      "[Epoch 1/5] [Batch 48/360] [D loss: 0.002816] [G loss: 14.297620] time: 0:03:17.028850\n",
      "(30, 64, 64, 3)\n",
      "0.75772566\n",
      "[Epoch 1/5] [Batch 49/360] [D loss: 0.003639] [G loss: 14.751752] time: 0:03:17.282160\n",
      "(30, 64, 64, 3)\n",
      "0.8202713\n",
      "[Epoch 1/5] [Batch 50/360] [D loss: 0.014154] [G loss: 14.416879] time: 0:03:17.510367\n",
      "(30, 64, 64, 3)\n",
      "0.8275261\n",
      "[Epoch 1/5] [Batch 51/360] [D loss: 0.009426] [G loss: 14.842789] time: 0:03:17.700357\n",
      "(30, 64, 64, 3)\n",
      "0.7923145\n",
      "[Epoch 1/5] [Batch 52/360] [D loss: 0.020862] [G loss: 14.212986] time: 0:03:17.888938\n",
      "(30, 64, 64, 3)\n",
      "0.8345575\n",
      "[Epoch 1/5] [Batch 53/360] [D loss: 0.114074] [G loss: 14.038408] time: 0:03:18.093695\n",
      "(30, 64, 64, 3)\n",
      "0.742614\n",
      "[Epoch 1/5] [Batch 54/360] [D loss: 0.330283] [G loss: 14.286210] time: 0:03:18.316985\n",
      "(30, 64, 64, 3)\n",
      "0.79336387\n",
      "[Epoch 1/5] [Batch 55/360] [D loss: 0.013716] [G loss: 14.211987] time: 0:03:18.521862\n",
      "(30, 64, 64, 3)\n",
      "0.7868187\n",
      "[Epoch 1/5] [Batch 56/360] [D loss: 0.066605] [G loss: 14.101266] time: 0:03:18.683236\n",
      "(30, 64, 64, 3)\n",
      "0.80629474\n",
      "[Epoch 1/5] [Batch 57/360] [D loss: 0.006612] [G loss: 13.956211] time: 0:03:18.879988\n",
      "(30, 64, 64, 3)\n",
      "0.81854135\n",
      "[Epoch 1/5] [Batch 58/360] [D loss: 0.018337] [G loss: 14.030013] time: 0:03:19.038687\n",
      "(30, 64, 64, 3)\n",
      "0.8216744\n",
      "[Epoch 1/5] [Batch 59/360] [D loss: 0.006150] [G loss: 13.954692] time: 0:03:19.249735\n",
      "(30, 64, 64, 3)\n",
      "0.76838106\n",
      "[Epoch 1/5] [Batch 60/360] [D loss: 0.005701] [G loss: 14.046259] time: 0:03:19.418304\n",
      "(30, 64, 64, 3)\n",
      "0.7313548\n",
      "[Epoch 1/5] [Batch 61/360] [D loss: 0.005480] [G loss: 13.754184] time: 0:03:19.616824\n",
      "(30, 64, 64, 3)\n",
      "0.8168335\n",
      "[Epoch 1/5] [Batch 62/360] [D loss: 0.005812] [G loss: 13.839202] time: 0:03:19.778477\n",
      "(30, 64, 64, 3)\n",
      "0.741385\n",
      "[Epoch 1/5] [Batch 63/360] [D loss: 0.004708] [G loss: 13.653640] time: 0:03:20.013266\n",
      "(30, 64, 64, 3)\n",
      "0.81051064\n",
      "[Epoch 1/5] [Batch 64/360] [D loss: 0.006105] [G loss: 14.182746] time: 0:03:20.207590\n",
      "(30, 64, 64, 3)\n",
      "0.7466337\n",
      "[Epoch 1/5] [Batch 65/360] [D loss: 0.006972] [G loss: 13.594400] time: 0:03:20.410255\n",
      "(30, 64, 64, 3)\n",
      "0.87644124\n",
      "[Epoch 1/5] [Batch 66/360] [D loss: 0.005716] [G loss: 13.359414] time: 0:03:20.637071\n",
      "(30, 64, 64, 3)\n",
      "0.7799192\n",
      "[Epoch 1/5] [Batch 67/360] [D loss: 0.006883] [G loss: 13.616696] time: 0:03:20.804424\n",
      "(30, 64, 64, 3)\n",
      "0.81083935\n",
      "[Epoch 1/5] [Batch 68/360] [D loss: 0.003727] [G loss: 14.271995] time: 0:03:21.008397\n",
      "(30, 64, 64, 3)\n",
      "0.87601906\n",
      "[Epoch 1/5] [Batch 69/360] [D loss: 0.007026] [G loss: 13.863400] time: 0:03:21.233357\n",
      "(30, 64, 64, 3)\n",
      "0.7976043\n",
      "[Epoch 1/5] [Batch 70/360] [D loss: 0.004613] [G loss: 13.490416] time: 0:03:21.429723\n",
      "(30, 64, 64, 3)\n",
      "0.8078081\n",
      "[Epoch 1/5] [Batch 71/360] [D loss: 0.003443] [G loss: 14.608051] time: 0:03:21.592887\n",
      "(30, 64, 64, 3)\n",
      "0.80752164\n",
      "[Epoch 1/5] [Batch 72/360] [D loss: 0.004625] [G loss: 13.739341] time: 0:03:21.779655\n",
      "(30, 64, 64, 3)\n",
      "0.8336752\n",
      "[Epoch 1/5] [Batch 73/360] [D loss: 0.003536] [G loss: 14.148263] time: 0:03:21.989420\n",
      "(30, 64, 64, 3)\n",
      "0.8286495\n",
      "[Epoch 1/5] [Batch 74/360] [D loss: 0.007663] [G loss: 13.853058] time: 0:03:22.209041\n",
      "(30, 64, 64, 3)\n",
      "0.7999466\n",
      "[Epoch 1/5] [Batch 75/360] [D loss: 0.003670] [G loss: 13.524625] time: 0:03:22.407223\n",
      "(30, 64, 64, 3)\n",
      "0.7856643\n",
      "[Epoch 1/5] [Batch 76/360] [D loss: 0.004282] [G loss: 13.565998] time: 0:03:22.605992\n",
      "(30, 64, 64, 3)\n",
      "0.77128506\n",
      "[Epoch 1/5] [Batch 77/360] [D loss: 0.003955] [G loss: 13.443272] time: 0:03:22.827869\n",
      "(30, 64, 64, 3)\n",
      "0.7544527\n",
      "[Epoch 1/5] [Batch 78/360] [D loss: 0.011798] [G loss: 12.824796] time: 0:03:23.070845\n",
      "(30, 64, 64, 3)\n",
      "0.7871955\n",
      "[Epoch 1/5] [Batch 79/360] [D loss: 0.005184] [G loss: 13.538568] time: 0:03:23.300259\n",
      "(30, 64, 64, 3)\n",
      "0.79778117\n",
      "[Epoch 1/5] [Batch 80/360] [D loss: 0.003580] [G loss: 13.597935] time: 0:03:23.482715\n",
      "(30, 64, 64, 3)\n",
      "0.78380233\n",
      "[Epoch 1/5] [Batch 81/360] [D loss: 0.003095] [G loss: 13.284327] time: 0:03:23.664167\n",
      "(30, 64, 64, 3)\n",
      "0.80416924\n",
      "[Epoch 1/5] [Batch 82/360] [D loss: 0.004561] [G loss: 13.097714] time: 0:03:23.864188\n",
      "(30, 64, 64, 3)\n",
      "0.8022144\n",
      "[Epoch 1/5] [Batch 83/360] [D loss: 0.004483] [G loss: 13.580516] time: 0:03:24.060822\n",
      "(30, 64, 64, 3)\n",
      "0.8059404\n",
      "[Epoch 1/5] [Batch 84/360] [D loss: 0.006900] [G loss: 13.409391] time: 0:03:24.243552\n",
      "(30, 64, 64, 3)\n",
      "0.82303524\n",
      "[Epoch 1/5] [Batch 85/360] [D loss: 0.003064] [G loss: 13.746194] time: 0:03:24.402623\n",
      "(30, 64, 64, 3)\n",
      "0.8435111\n",
      "[Epoch 1/5] [Batch 86/360] [D loss: 0.004421] [G loss: 13.000741] time: 0:03:24.576635\n",
      "(30, 64, 64, 3)\n",
      "0.8780031\n",
      "[Epoch 1/5] [Batch 87/360] [D loss: 0.004733] [G loss: 13.790207] time: 0:03:24.756209\n",
      "(30, 64, 64, 3)\n",
      "0.7966905\n",
      "[Epoch 1/5] [Batch 88/360] [D loss: 0.011769] [G loss: 13.269823] time: 0:03:24.949712\n",
      "(30, 64, 64, 3)\n",
      "0.80486673\n",
      "[Epoch 1/5] [Batch 89/360] [D loss: 0.005848] [G loss: 12.871856] time: 0:03:25.160502\n",
      "(30, 64, 64, 3)\n",
      "0.7741828\n",
      "[Epoch 1/5] [Batch 90/360] [D loss: 0.041028] [G loss: 13.566634] time: 0:03:25.350634\n",
      "(30, 64, 64, 3)\n",
      "0.82941943\n",
      "[Epoch 1/5] [Batch 91/360] [D loss: 0.092614] [G loss: 14.344952] time: 0:03:25.541789\n",
      "(30, 64, 64, 3)\n",
      "0.82741326\n",
      "[Epoch 1/5] [Batch 92/360] [D loss: 0.026318] [G loss: 13.154767] time: 0:03:25.736517\n",
      "(30, 64, 64, 3)\n",
      "0.81286985\n",
      "[Epoch 1/5] [Batch 93/360] [D loss: 0.026691] [G loss: 13.452489] time: 0:03:25.914552\n",
      "(30, 64, 64, 3)\n",
      "0.7749383\n",
      "[Epoch 1/5] [Batch 94/360] [D loss: 0.109876] [G loss: 13.732473] time: 0:03:26.142057\n",
      "(30, 64, 64, 3)\n",
      "0.8169725\n",
      "[Epoch 1/5] [Batch 95/360] [D loss: 0.027104] [G loss: 13.572575] time: 0:03:26.315476\n",
      "(30, 64, 64, 3)\n",
      "0.81709915\n",
      "[Epoch 1/5] [Batch 96/360] [D loss: 0.007089] [G loss: 12.980503] time: 0:03:26.494691\n",
      "(30, 64, 64, 3)\n",
      "0.8281421\n",
      "[Epoch 1/5] [Batch 97/360] [D loss: 0.009861] [G loss: 12.668849] time: 0:03:26.687145\n",
      "(30, 64, 64, 3)\n",
      "0.8463332\n",
      "[Epoch 1/5] [Batch 98/360] [D loss: 0.003876] [G loss: 13.420447] time: 0:03:26.865030\n",
      "(30, 64, 64, 3)\n",
      "0.81432706\n",
      "[Epoch 1/5] [Batch 99/360] [D loss: 0.004154] [G loss: 13.449734] time: 0:03:27.059086\n",
      "(30, 64, 64, 3)\n",
      "0.8068023\n",
      "[Epoch 1/5] [Batch 100/360] [D loss: 0.005605] [G loss: 13.575326] time: 0:03:27.256697\n",
      "(30, 64, 64, 3)\n",
      "0.7978513\n",
      "[Epoch 1/5] [Batch 101/360] [D loss: 0.018861] [G loss: 13.027775] time: 0:03:27.444445\n",
      "(30, 64, 64, 3)\n",
      "0.8193671\n",
      "[Epoch 1/5] [Batch 102/360] [D loss: 0.005463] [G loss: 13.085851] time: 0:03:27.620095\n",
      "(30, 64, 64, 3)\n",
      "0.7766083\n",
      "[Epoch 1/5] [Batch 103/360] [D loss: 0.005597] [G loss: 13.204276] time: 0:03:27.831305\n",
      "(30, 64, 64, 3)\n",
      "0.81013554\n",
      "[Epoch 1/5] [Batch 104/360] [D loss: 0.005772] [G loss: 13.052438] time: 0:03:28.035119\n",
      "(30, 64, 64, 3)\n",
      "0.8282826\n",
      "[Epoch 1/5] [Batch 105/360] [D loss: 0.003583] [G loss: 13.110657] time: 0:03:28.210148\n",
      "(30, 64, 64, 3)\n",
      "0.7649177\n",
      "[Epoch 1/5] [Batch 106/360] [D loss: 0.004761] [G loss: 12.839192] time: 0:03:28.401815\n",
      "(30, 64, 64, 3)\n",
      "0.78966594\n",
      "[Epoch 1/5] [Batch 107/360] [D loss: 0.003348] [G loss: 12.967223] time: 0:03:28.635429\n",
      "(30, 64, 64, 3)\n",
      "0.8359496\n",
      "[Epoch 1/5] [Batch 108/360] [D loss: 0.006995] [G loss: 13.339203] time: 0:03:28.847655\n",
      "(30, 64, 64, 3)\n",
      "0.81503254\n",
      "[Epoch 1/5] [Batch 109/360] [D loss: 0.005099] [G loss: 13.093711] time: 0:03:29.091178\n",
      "(30, 64, 64, 3)\n",
      "0.8498362\n",
      "[Epoch 1/5] [Batch 110/360] [D loss: 0.005878] [G loss: 13.218973] time: 0:03:29.250071\n",
      "(30, 64, 64, 3)\n",
      "0.85968584\n",
      "[Epoch 1/5] [Batch 111/360] [D loss: 0.003662] [G loss: 13.013436] time: 0:03:29.441423\n",
      "(30, 64, 64, 3)\n",
      "0.8327169\n",
      "[Epoch 1/5] [Batch 112/360] [D loss: 0.008424] [G loss: 12.242448] time: 0:03:29.617363\n",
      "(30, 64, 64, 3)\n",
      "0.8164441\n",
      "[Epoch 1/5] [Batch 113/360] [D loss: 0.004292] [G loss: 12.919320] time: 0:03:29.842995\n",
      "(30, 64, 64, 3)\n",
      "0.8868975\n",
      "[Epoch 1/5] [Batch 114/360] [D loss: 0.029329] [G loss: 13.125769] time: 0:03:30.017304\n",
      "(30, 64, 64, 3)\n",
      "0.8795096\n",
      "[Epoch 1/5] [Batch 115/360] [D loss: 0.009961] [G loss: 12.830725] time: 0:03:30.239023\n",
      "(30, 64, 64, 3)\n",
      "0.82591933\n",
      "[Epoch 1/5] [Batch 116/360] [D loss: 0.016524] [G loss: 13.459532] time: 0:03:30.432339\n",
      "(30, 64, 64, 3)\n",
      "0.7775688\n",
      "[Epoch 1/5] [Batch 117/360] [D loss: 0.040664] [G loss: 12.609166] time: 0:03:30.625711\n",
      "(30, 64, 64, 3)\n",
      "0.80332136\n",
      "[Epoch 1/5] [Batch 118/360] [D loss: 0.005713] [G loss: 12.914264] time: 0:03:30.874351\n",
      "(30, 64, 64, 3)\n",
      "0.78897816\n",
      "[Epoch 1/5] [Batch 119/360] [D loss: 0.010015] [G loss: 13.704987] time: 0:03:31.090423\n",
      "(30, 64, 64, 3)\n",
      "0.8020775\n",
      "[Epoch 1/5] [Batch 120/360] [D loss: 0.005027] [G loss: 12.394193] time: 0:03:31.282883\n",
      "(30, 64, 64, 3)\n",
      "0.7988329\n",
      "[Epoch 1/5] [Batch 121/360] [D loss: 0.004193] [G loss: 12.785321] time: 0:03:31.511078\n",
      "(30, 64, 64, 3)\n",
      "0.8049826\n",
      "[Epoch 1/5] [Batch 122/360] [D loss: 0.005529] [G loss: 12.258134] time: 0:03:31.742861\n",
      "(30, 64, 64, 3)\n",
      "0.79788584\n",
      "[Epoch 1/5] [Batch 123/360] [D loss: 0.021729] [G loss: 12.585679] time: 0:03:31.938157\n",
      "(30, 64, 64, 3)\n",
      "0.7902813\n",
      "[Epoch 1/5] [Batch 124/360] [D loss: 0.004348] [G loss: 13.551263] time: 0:03:32.104004\n",
      "(30, 64, 64, 3)\n",
      "0.7887203\n",
      "[Epoch 1/5] [Batch 125/360] [D loss: 0.005539] [G loss: 12.384624] time: 0:03:32.291556\n",
      "(30, 64, 64, 3)\n",
      "0.87036353\n",
      "[Epoch 1/5] [Batch 126/360] [D loss: 0.010274] [G loss: 12.779136] time: 0:03:32.499670\n",
      "(30, 64, 64, 3)\n",
      "0.8299389\n",
      "[Epoch 1/5] [Batch 127/360] [D loss: 0.025036] [G loss: 12.663925] time: 0:03:32.669564\n",
      "(30, 64, 64, 3)\n",
      "0.8395443\n",
      "[Epoch 1/5] [Batch 128/360] [D loss: 0.008092] [G loss: 12.148825] time: 0:03:32.859135\n",
      "(30, 64, 64, 3)\n",
      "0.83413213\n",
      "[Epoch 1/5] [Batch 129/360] [D loss: 0.011177] [G loss: 12.419131] time: 0:03:33.054433\n",
      "(30, 64, 64, 3)\n",
      "0.8517696\n",
      "[Epoch 1/5] [Batch 130/360] [D loss: 0.082613] [G loss: 12.921801] time: 0:03:33.264316\n",
      "(30, 64, 64, 3)\n",
      "0.7970083\n",
      "[Epoch 1/5] [Batch 131/360] [D loss: 0.272662] [G loss: 12.158792] time: 0:03:33.463867\n",
      "(30, 64, 64, 3)\n",
      "0.8089649\n",
      "[Epoch 1/5] [Batch 132/360] [D loss: 0.015730] [G loss: 12.329244] time: 0:03:33.674036\n",
      "(30, 64, 64, 3)\n",
      "0.81961125\n",
      "[Epoch 1/5] [Batch 133/360] [D loss: 0.059893] [G loss: 12.844100] time: 0:03:33.837209\n",
      "(30, 64, 64, 3)\n",
      "0.84358805\n",
      "[Epoch 1/5] [Batch 134/360] [D loss: 0.010483] [G loss: 12.661746] time: 0:03:34.000835\n",
      "(30, 64, 64, 3)\n",
      "0.82798356\n",
      "[Epoch 1/5] [Batch 135/360] [D loss: 0.007264] [G loss: 12.226956] time: 0:03:34.159942\n",
      "(30, 64, 64, 3)\n",
      "0.72692823\n",
      "[Epoch 1/5] [Batch 136/360] [D loss: 0.005123] [G loss: 12.478705] time: 0:03:34.364017\n",
      "(30, 64, 64, 3)\n",
      "0.8521328\n",
      "[Epoch 1/5] [Batch 137/360] [D loss: 0.007339] [G loss: 12.309030] time: 0:03:34.564308\n",
      "(30, 64, 64, 3)\n",
      "0.83993655\n",
      "[Epoch 1/5] [Batch 138/360] [D loss: 0.004879] [G loss: 12.473965] time: 0:03:34.758656\n",
      "(30, 64, 64, 3)\n",
      "0.8485617\n",
      "[Epoch 1/5] [Batch 139/360] [D loss: 0.004191] [G loss: 12.751367] time: 0:03:34.946554\n",
      "(30, 64, 64, 3)\n",
      "0.88536674\n",
      "[Epoch 1/5] [Batch 140/360] [D loss: 0.002597] [G loss: 12.589905] time: 0:03:35.161658\n",
      "(30, 64, 64, 3)\n",
      "0.80859536\n",
      "[Epoch 1/5] [Batch 141/360] [D loss: 0.005576] [G loss: 12.557035] time: 0:03:35.358315\n",
      "(30, 64, 64, 3)\n",
      "0.810749\n",
      "[Epoch 1/5] [Batch 142/360] [D loss: 0.003215] [G loss: 12.639303] time: 0:03:35.513778\n",
      "(30, 64, 64, 3)\n",
      "0.8462177\n",
      "[Epoch 1/5] [Batch 143/360] [D loss: 0.004461] [G loss: 12.636027] time: 0:03:35.678791\n",
      "(30, 64, 64, 3)\n",
      "0.8186379\n",
      "[Epoch 1/5] [Batch 144/360] [D loss: 0.003779] [G loss: 12.117444] time: 0:03:35.903724\n",
      "(30, 64, 64, 3)\n",
      "0.8032016\n",
      "[Epoch 1/5] [Batch 145/360] [D loss: 0.005458] [G loss: 12.096737] time: 0:03:36.118955\n",
      "(30, 64, 64, 3)\n",
      "0.8755428\n",
      "[Epoch 1/5] [Batch 146/360] [D loss: 0.006160] [G loss: 12.398019] time: 0:03:36.381322\n",
      "(30, 64, 64, 3)\n",
      "0.8024311\n",
      "[Epoch 1/5] [Batch 147/360] [D loss: 0.004877] [G loss: 12.139889] time: 0:03:36.571440\n",
      "(30, 64, 64, 3)\n",
      "0.86419415\n",
      "[Epoch 1/5] [Batch 148/360] [D loss: 0.003585] [G loss: 12.029099] time: 0:03:36.788537\n",
      "(30, 64, 64, 3)\n",
      "0.8151594\n",
      "[Epoch 1/5] [Batch 149/360] [D loss: 0.005298] [G loss: 11.747252] time: 0:03:36.987547\n",
      "(30, 64, 64, 3)\n",
      "0.8240795\n",
      "[Epoch 1/5] [Batch 150/360] [D loss: 0.005510] [G loss: 12.533697] time: 0:03:37.184866\n",
      "(30, 64, 64, 3)\n",
      "0.81155235\n",
      "[Epoch 1/5] [Batch 151/360] [D loss: 0.003612] [G loss: 12.752323] time: 0:03:37.375959\n",
      "(30, 64, 64, 3)\n",
      "0.7487747\n",
      "[Epoch 1/5] [Batch 152/360] [D loss: 0.005545] [G loss: 11.824879] time: 0:03:37.588824\n",
      "(30, 64, 64, 3)\n",
      "0.856425\n",
      "[Epoch 1/5] [Batch 153/360] [D loss: 0.007183] [G loss: 11.867030] time: 0:03:37.755706\n",
      "(30, 64, 64, 3)\n",
      "0.77885675\n",
      "[Epoch 1/5] [Batch 154/360] [D loss: 0.022936] [G loss: 11.735703] time: 0:03:37.938701\n",
      "(30, 64, 64, 3)\n",
      "0.7546541\n",
      "[Epoch 1/5] [Batch 155/360] [D loss: 0.007459] [G loss: 11.953355] time: 0:03:38.126665\n",
      "(30, 64, 64, 3)\n",
      "0.84018975\n",
      "[Epoch 1/5] [Batch 156/360] [D loss: 0.005025] [G loss: 12.023451] time: 0:03:38.380711\n",
      "(30, 64, 64, 3)\n",
      "0.784975\n",
      "[Epoch 1/5] [Batch 157/360] [D loss: 0.007578] [G loss: 11.177072] time: 0:03:38.587433\n",
      "(30, 64, 64, 3)\n",
      "0.84245163\n",
      "[Epoch 1/5] [Batch 158/360] [D loss: 0.022623] [G loss: 11.924679] time: 0:03:38.833305\n",
      "(30, 64, 64, 3)\n",
      "0.8204338\n",
      "[Epoch 1/5] [Batch 159/360] [D loss: 0.017541] [G loss: 11.896173] time: 0:03:39.020855\n",
      "(30, 64, 64, 3)\n",
      "0.80628514\n",
      "[Epoch 1/5] [Batch 160/360] [D loss: 0.011790] [G loss: 11.786216] time: 0:03:39.185592\n",
      "(30, 64, 64, 3)\n",
      "0.80450726\n",
      "[Epoch 1/5] [Batch 161/360] [D loss: 0.011067] [G loss: 11.482999] time: 0:03:39.380856\n",
      "(30, 64, 64, 3)\n",
      "0.8426876\n",
      "[Epoch 1/5] [Batch 162/360] [D loss: 0.004603] [G loss: 12.173666] time: 0:03:39.577559\n",
      "(30, 64, 64, 3)\n",
      "0.8018195\n",
      "[Epoch 1/5] [Batch 163/360] [D loss: 0.027077] [G loss: 12.004983] time: 0:03:39.736731\n",
      "(30, 64, 64, 3)\n",
      "0.8074749\n",
      "[Epoch 1/5] [Batch 164/360] [D loss: 0.031395] [G loss: 11.739624] time: 0:03:39.919980\n",
      "(30, 64, 64, 3)\n",
      "0.8538169\n",
      "[Epoch 1/5] [Batch 165/360] [D loss: 0.010873] [G loss: 12.017669] time: 0:03:40.102936\n",
      "(30, 64, 64, 3)\n",
      "0.80034447\n",
      "[Epoch 1/5] [Batch 166/360] [D loss: 0.055070] [G loss: 11.403998] time: 0:03:40.314285\n",
      "(30, 64, 64, 3)\n",
      "0.8348201\n",
      "[Epoch 1/5] [Batch 167/360] [D loss: 0.106557] [G loss: 11.488236] time: 0:03:40.518544\n",
      "(30, 64, 64, 3)\n",
      "0.8493441\n",
      "[Epoch 1/5] [Batch 168/360] [D loss: 0.191730] [G loss: 11.947191] time: 0:03:40.751757\n",
      "(30, 64, 64, 3)\n",
      "0.777194\n",
      "[Epoch 1/5] [Batch 169/360] [D loss: 0.036443] [G loss: 11.516825] time: 0:03:40.950967\n",
      "(30, 64, 64, 3)\n",
      "0.84170836\n",
      "[Epoch 1/5] [Batch 170/360] [D loss: 0.027564] [G loss: 12.264881] time: 0:03:41.146103\n",
      "(30, 64, 64, 3)\n",
      "0.85101455\n",
      "[Epoch 1/5] [Batch 171/360] [D loss: 0.031120] [G loss: 12.236696] time: 0:03:41.349052\n",
      "(30, 64, 64, 3)\n",
      "0.7862005\n",
      "[Epoch 1/5] [Batch 172/360] [D loss: 0.006348] [G loss: 11.596332] time: 0:03:41.548064\n",
      "(30, 64, 64, 3)\n",
      "0.8310895\n",
      "[Epoch 1/5] [Batch 173/360] [D loss: 0.006552] [G loss: 11.326175] time: 0:03:41.770921\n",
      "(30, 64, 64, 3)\n",
      "0.847408\n",
      "[Epoch 1/5] [Batch 174/360] [D loss: 0.008666] [G loss: 11.908268] time: 0:03:41.942708\n",
      "(30, 64, 64, 3)\n",
      "0.7845004\n",
      "[Epoch 1/5] [Batch 175/360] [D loss: 0.013693] [G loss: 11.351649] time: 0:03:42.122863\n",
      "(30, 64, 64, 3)\n",
      "0.78914434\n",
      "[Epoch 1/5] [Batch 176/360] [D loss: 0.007149] [G loss: 11.575963] time: 0:03:42.312298\n",
      "(30, 64, 64, 3)\n",
      "0.87648195\n",
      "[Epoch 1/5] [Batch 177/360] [D loss: 0.006843] [G loss: 12.065296] time: 0:03:42.532926\n",
      "(30, 64, 64, 3)\n",
      "0.8715666\n",
      "[Epoch 1/5] [Batch 178/360] [D loss: 0.004949] [G loss: 11.219635] time: 0:03:42.775080\n",
      "(30, 64, 64, 3)\n",
      "0.8053913\n",
      "[Epoch 1/5] [Batch 179/360] [D loss: 0.005005] [G loss: 10.989188] time: 0:03:43.016202\n",
      "(30, 64, 64, 3)\n",
      "0.8402841\n",
      "[Epoch 1/5] [Batch 180/360] [D loss: 0.006250] [G loss: 10.872471] time: 0:03:43.223109\n",
      "(30, 64, 64, 3)\n",
      "0.8658843\n",
      "[Epoch 1/5] [Batch 181/360] [D loss: 0.006382] [G loss: 12.428500] time: 0:03:43.427041\n",
      "(30, 64, 64, 3)\n",
      "0.8146672\n",
      "[Epoch 1/5] [Batch 182/360] [D loss: 0.004607] [G loss: 11.920047] time: 0:03:43.614169\n",
      "(30, 64, 64, 3)\n",
      "0.7631375\n",
      "[Epoch 1/5] [Batch 183/360] [D loss: 0.003333] [G loss: 12.043550] time: 0:03:43.807979\n",
      "(30, 64, 64, 3)\n",
      "0.84589654\n",
      "[Epoch 1/5] [Batch 184/360] [D loss: 0.005835] [G loss: 11.728181] time: 0:03:44.030058\n",
      "(30, 64, 64, 3)\n",
      "0.83128256\n",
      "[Epoch 1/5] [Batch 185/360] [D loss: 0.004324] [G loss: 11.040371] time: 0:03:44.216090\n",
      "(30, 64, 64, 3)\n",
      "0.84970146\n",
      "[Epoch 1/5] [Batch 186/360] [D loss: 0.004548] [G loss: 11.129046] time: 0:03:44.386784\n",
      "(30, 64, 64, 3)\n",
      "0.8357101\n",
      "[Epoch 1/5] [Batch 187/360] [D loss: 0.006041] [G loss: 10.575469] time: 0:03:44.566056\n",
      "(30, 64, 64, 3)\n",
      "0.81274134\n",
      "[Epoch 1/5] [Batch 188/360] [D loss: 0.005158] [G loss: 12.169154] time: 0:03:44.756912\n",
      "(30, 64, 64, 3)\n",
      "0.81307936\n",
      "[Epoch 1/5] [Batch 189/360] [D loss: 0.003359] [G loss: 11.402562] time: 0:03:44.963995\n",
      "(30, 64, 64, 3)\n",
      "0.81068945\n",
      "[Epoch 1/5] [Batch 190/360] [D loss: 0.002968] [G loss: 11.759844] time: 0:03:45.197799\n",
      "(30, 64, 64, 3)\n",
      "0.8497974\n",
      "[Epoch 1/5] [Batch 191/360] [D loss: 0.004849] [G loss: 11.505380] time: 0:03:45.431693\n",
      "(30, 64, 64, 3)\n",
      "0.81417066\n",
      "[Epoch 1/5] [Batch 192/360] [D loss: 0.003829] [G loss: 11.191339] time: 0:03:45.614126\n",
      "(30, 64, 64, 3)\n",
      "0.8293869\n",
      "[Epoch 1/5] [Batch 193/360] [D loss: 0.004327] [G loss: 11.464327] time: 0:03:45.832049\n",
      "(30, 64, 64, 3)\n",
      "0.76964664\n",
      "[Epoch 1/5] [Batch 194/360] [D loss: 0.003730] [G loss: 11.659084] time: 0:03:46.017316\n",
      "(30, 64, 64, 3)\n",
      "0.8349616\n",
      "[Epoch 1/5] [Batch 195/360] [D loss: 0.004563] [G loss: 11.362738] time: 0:03:46.204148\n",
      "(30, 64, 64, 3)\n",
      "0.7931688\n",
      "[Epoch 1/5] [Batch 196/360] [D loss: 0.003988] [G loss: 11.232528] time: 0:03:46.441038\n",
      "(30, 64, 64, 3)\n",
      "0.8221705\n",
      "[Epoch 1/5] [Batch 197/360] [D loss: 0.004602] [G loss: 11.483027] time: 0:03:46.669040\n",
      "(30, 64, 64, 3)\n",
      "0.85056895\n",
      "[Epoch 1/5] [Batch 198/360] [D loss: 0.003855] [G loss: 11.080771] time: 0:03:46.871756\n",
      "(30, 64, 64, 3)\n",
      "0.83693314\n",
      "[Epoch 1/5] [Batch 199/360] [D loss: 0.003965] [G loss: 11.217344] time: 0:03:47.101848\n",
      "(30, 64, 64, 3)\n",
      "0.84461397\n",
      "[Epoch 1/5] [Batch 200/360] [D loss: 0.004671] [G loss: 10.845703] time: 0:03:47.309942\n",
      "(30, 64, 64, 3)\n",
      "0.7938381\n",
      "[Epoch 1/5] [Batch 201/360] [D loss: 0.003704] [G loss: 11.446568] time: 0:03:47.536925\n",
      "(30, 64, 64, 3)\n",
      "0.8164611\n",
      "[Epoch 1/5] [Batch 202/360] [D loss: 0.004154] [G loss: 11.407430] time: 0:03:47.710379\n",
      "(30, 64, 64, 3)\n",
      "0.820045\n",
      "[Epoch 1/5] [Batch 203/360] [D loss: 0.005704] [G loss: 11.272894] time: 0:03:47.885960\n",
      "(30, 64, 64, 3)\n",
      "0.8114879\n",
      "[Epoch 1/5] [Batch 204/360] [D loss: 0.004627] [G loss: 11.242739] time: 0:03:48.075707\n",
      "(30, 64, 64, 3)\n",
      "0.79184026\n",
      "[Epoch 1/5] [Batch 205/360] [D loss: 0.006308] [G loss: 11.096220] time: 0:03:48.256584\n",
      "(30, 64, 64, 3)\n",
      "0.82559896\n",
      "[Epoch 1/5] [Batch 206/360] [D loss: 0.005762] [G loss: 10.710133] time: 0:03:48.432607\n",
      "(30, 64, 64, 3)\n",
      "0.8584368\n",
      "[Epoch 1/5] [Batch 207/360] [D loss: 0.005619] [G loss: 10.934218] time: 0:03:48.642900\n",
      "(30, 64, 64, 3)\n",
      "0.8083852\n",
      "[Epoch 1/5] [Batch 208/360] [D loss: 0.012089] [G loss: 11.061778] time: 0:03:48.832738\n",
      "(30, 64, 64, 3)\n",
      "0.77709883\n",
      "[Epoch 1/5] [Batch 209/360] [D loss: 0.028082] [G loss: 11.983307] time: 0:03:49.034816\n",
      "(30, 64, 64, 3)\n",
      "0.88237077\n",
      "[Epoch 1/5] [Batch 210/360] [D loss: 0.019804] [G loss: 10.954106] time: 0:03:49.234560\n",
      "(30, 64, 64, 3)\n",
      "0.8284592\n",
      "[Epoch 1/5] [Batch 211/360] [D loss: 0.016734] [G loss: 11.177935] time: 0:03:49.449783\n",
      "(30, 64, 64, 3)\n",
      "0.844021\n",
      "[Epoch 1/5] [Batch 212/360] [D loss: 0.065604] [G loss: 10.751602] time: 0:03:49.621768\n",
      "(30, 64, 64, 3)\n",
      "0.8414693\n",
      "[Epoch 1/5] [Batch 213/360] [D loss: 0.390641] [G loss: 11.188334] time: 0:03:49.796626\n",
      "(30, 64, 64, 3)\n",
      "0.81867486\n",
      "[Epoch 1/5] [Batch 214/360] [D loss: 0.062467] [G loss: 10.786057] time: 0:03:50.020891\n",
      "(30, 64, 64, 3)\n",
      "0.8164211\n",
      "[Epoch 1/5] [Batch 215/360] [D loss: 0.108971] [G loss: 11.018533] time: 0:03:50.222546\n",
      "(30, 64, 64, 3)\n",
      "0.82326984\n",
      "[Epoch 1/5] [Batch 216/360] [D loss: 0.045483] [G loss: 11.569798] time: 0:03:50.450981\n",
      "(30, 64, 64, 3)\n",
      "0.7634349\n",
      "[Epoch 1/5] [Batch 217/360] [D loss: 0.037591] [G loss: 11.794821] time: 0:03:50.629910\n",
      "(30, 64, 64, 3)\n",
      "0.8727929\n",
      "[Epoch 1/5] [Batch 218/360] [D loss: 0.016657] [G loss: 11.732834] time: 0:03:50.821008\n",
      "(30, 64, 64, 3)\n",
      "0.8244829\n",
      "[Epoch 1/5] [Batch 219/360] [D loss: 0.013979] [G loss: 10.612988] time: 0:03:51.037461\n",
      "(30, 64, 64, 3)\n",
      "0.81053406\n",
      "[Epoch 1/5] [Batch 220/360] [D loss: 0.012340] [G loss: 10.836810] time: 0:03:51.274388\n",
      "(30, 64, 64, 3)\n",
      "0.8523636\n",
      "[Epoch 1/5] [Batch 221/360] [D loss: 0.015582] [G loss: 10.668518] time: 0:03:51.504229\n",
      "(30, 64, 64, 3)\n",
      "0.8421056\n",
      "[Epoch 1/5] [Batch 222/360] [D loss: 0.010505] [G loss: 10.451117] time: 0:03:51.691085\n",
      "(30, 64, 64, 3)\n",
      "0.8411746\n",
      "[Epoch 1/5] [Batch 223/360] [D loss: 0.015488] [G loss: 10.693519] time: 0:03:51.900670\n",
      "(30, 64, 64, 3)\n",
      "0.8371642\n",
      "[Epoch 1/5] [Batch 224/360] [D loss: 0.008518] [G loss: 11.573159] time: 0:03:52.280648\n",
      "(30, 64, 64, 3)\n",
      "0.86913425\n",
      "[Epoch 1/5] [Batch 225/360] [D loss: 0.007682] [G loss: 10.918625] time: 0:03:52.471404\n",
      "(30, 64, 64, 3)\n",
      "0.79702264\n",
      "[Epoch 1/5] [Batch 226/360] [D loss: 0.005203] [G loss: 11.366698] time: 0:03:52.628329\n",
      "(30, 64, 64, 3)\n",
      "0.85421515\n",
      "[Epoch 1/5] [Batch 227/360] [D loss: 0.007682] [G loss: 10.632730] time: 0:03:52.804370\n",
      "(30, 64, 64, 3)\n",
      "0.8942564\n",
      "[Epoch 1/5] [Batch 228/360] [D loss: 0.006209] [G loss: 10.712003] time: 0:03:53.032907\n",
      "(30, 64, 64, 3)\n",
      "0.8331427\n",
      "[Epoch 1/5] [Batch 229/360] [D loss: 0.010599] [G loss: 11.293878] time: 0:03:53.246709\n",
      "(30, 64, 64, 3)\n",
      "0.8448376\n",
      "[Epoch 1/5] [Batch 230/360] [D loss: 0.006971] [G loss: 10.480639] time: 0:03:53.408665\n",
      "(30, 64, 64, 3)\n",
      "0.8104933\n",
      "[Epoch 1/5] [Batch 231/360] [D loss: 0.004738] [G loss: 11.335039] time: 0:03:53.581124\n",
      "(30, 64, 64, 3)\n",
      "0.82414657\n",
      "[Epoch 1/5] [Batch 232/360] [D loss: 0.004594] [G loss: 11.025922] time: 0:03:53.758494\n",
      "(30, 64, 64, 3)\n",
      "0.76632386\n",
      "[Epoch 1/5] [Batch 233/360] [D loss: 0.004605] [G loss: 10.548059] time: 0:03:53.953034\n",
      "(30, 64, 64, 3)\n",
      "0.80033606\n",
      "[Epoch 1/5] [Batch 234/360] [D loss: 0.007422] [G loss: 10.734185] time: 0:03:54.189057\n",
      "(30, 64, 64, 3)\n",
      "0.8526041\n",
      "[Epoch 1/5] [Batch 235/360] [D loss: 0.008650] [G loss: 10.790258] time: 0:03:54.345609\n",
      "(30, 64, 64, 3)\n",
      "0.85357594\n",
      "[Epoch 1/5] [Batch 236/360] [D loss: 0.004689] [G loss: 10.591612] time: 0:03:54.503752\n",
      "(30, 64, 64, 3)\n",
      "0.848012\n",
      "[Epoch 1/5] [Batch 237/360] [D loss: 0.004162] [G loss: 11.245341] time: 0:03:54.690497\n",
      "(30, 64, 64, 3)\n",
      "0.7842874\n",
      "[Epoch 1/5] [Batch 238/360] [D loss: 0.007006] [G loss: 10.824711] time: 0:03:54.858046\n",
      "(30, 64, 64, 3)\n",
      "0.76968\n",
      "[Epoch 1/5] [Batch 239/360] [D loss: 0.006981] [G loss: 10.106206] time: 0:03:55.076678\n",
      "(30, 64, 64, 3)\n",
      "0.8241403\n",
      "[Epoch 1/5] [Batch 240/360] [D loss: 0.006017] [G loss: 10.926955] time: 0:03:55.304229\n",
      "(30, 64, 64, 3)\n",
      "0.86601895\n",
      "[Epoch 1/5] [Batch 241/360] [D loss: 0.005336] [G loss: 10.669475] time: 0:03:55.528409\n",
      "(30, 64, 64, 3)\n",
      "0.8543172\n",
      "[Epoch 1/5] [Batch 242/360] [D loss: 0.008515] [G loss: 10.154014] time: 0:03:55.709349\n",
      "(30, 64, 64, 3)\n",
      "0.8390899\n",
      "[Epoch 1/5] [Batch 243/360] [D loss: 0.005094] [G loss: 10.414115] time: 0:03:55.924999\n",
      "(30, 64, 64, 3)\n",
      "0.8346613\n",
      "[Epoch 1/5] [Batch 244/360] [D loss: 0.006838] [G loss: 10.534036] time: 0:03:56.105217\n",
      "(30, 64, 64, 3)\n",
      "0.8545585\n",
      "[Epoch 1/5] [Batch 245/360] [D loss: 0.006723] [G loss: 10.238903] time: 0:03:56.271507\n",
      "(30, 64, 64, 3)\n",
      "0.8360246\n",
      "[Epoch 1/5] [Batch 246/360] [D loss: 0.007962] [G loss: 10.099887] time: 0:03:56.439467\n",
      "(30, 64, 64, 3)\n",
      "0.86775285\n",
      "[Epoch 1/5] [Batch 247/360] [D loss: 0.006587] [G loss: 10.893668] time: 0:03:56.600820\n",
      "(30, 64, 64, 3)\n",
      "0.8490713\n",
      "[Epoch 1/5] [Batch 248/360] [D loss: 0.006673] [G loss: 10.679172] time: 0:03:56.795770\n",
      "(30, 64, 64, 3)\n",
      "0.7946493\n",
      "[Epoch 1/5] [Batch 249/360] [D loss: 0.007295] [G loss: 10.585209] time: 0:03:57.000760\n",
      "(30, 64, 64, 3)\n",
      "0.82809037\n",
      "[Epoch 1/5] [Batch 250/360] [D loss: 0.007543] [G loss: 10.720749] time: 0:03:57.193791\n",
      "(30, 64, 64, 3)\n",
      "0.8158018\n",
      "[Epoch 1/5] [Batch 251/360] [D loss: 0.008437] [G loss: 10.717311] time: 0:03:57.397187\n",
      "(30, 64, 64, 3)\n",
      "0.7849365\n",
      "[Epoch 1/5] [Batch 252/360] [D loss: 0.006327] [G loss: 10.436175] time: 0:03:57.571625\n",
      "(30, 64, 64, 3)\n",
      "0.8828075\n",
      "[Epoch 1/5] [Batch 253/360] [D loss: 0.009658] [G loss: 11.037832] time: 0:03:57.758599\n",
      "(30, 64, 64, 3)\n",
      "0.8304696\n",
      "[Epoch 1/5] [Batch 254/360] [D loss: 0.123655] [G loss: 10.831046] time: 0:03:57.950985\n",
      "(30, 64, 64, 3)\n",
      "0.8968115\n",
      "[Epoch 1/5] [Batch 255/360] [D loss: 0.587557] [G loss: 10.395963] time: 0:03:58.131762\n",
      "(30, 64, 64, 3)\n",
      "0.86463475\n",
      "[Epoch 1/5] [Batch 256/360] [D loss: 0.079988] [G loss: 9.902197] time: 0:03:58.365270\n",
      "(30, 64, 64, 3)\n",
      "0.7433856\n",
      "[Epoch 1/5] [Batch 257/360] [D loss: 0.167815] [G loss: 10.330790] time: 0:03:58.610742\n",
      "(30, 64, 64, 3)\n",
      "0.8385064\n",
      "[Epoch 1/5] [Batch 258/360] [D loss: 0.097172] [G loss: 10.326024] time: 0:03:58.805057\n",
      "(30, 64, 64, 3)\n",
      "0.8190561\n",
      "[Epoch 1/5] [Batch 259/360] [D loss: 0.073077] [G loss: 10.030618] time: 0:03:59.010293\n",
      "(30, 64, 64, 3)\n",
      "0.77344567\n",
      "[Epoch 1/5] [Batch 260/360] [D loss: 0.066750] [G loss: 9.717032] time: 0:03:59.212223\n",
      "(30, 64, 64, 3)\n",
      "0.8265193\n",
      "[Epoch 1/5] [Batch 261/360] [D loss: 0.062516] [G loss: 11.342500] time: 0:03:59.395908\n",
      "(30, 64, 64, 3)\n",
      "0.82663316\n",
      "[Epoch 1/5] [Batch 262/360] [D loss: 0.053634] [G loss: 10.420810] time: 0:03:59.583146\n",
      "(30, 64, 64, 3)\n",
      "0.87607145\n",
      "[Epoch 1/5] [Batch 263/360] [D loss: 0.042036] [G loss: 10.106339] time: 0:03:59.791533\n",
      "(30, 64, 64, 3)\n",
      "0.84933424\n",
      "[Epoch 1/5] [Batch 264/360] [D loss: 0.057798] [G loss: 10.872731] time: 0:03:59.956915\n",
      "(30, 64, 64, 3)\n",
      "0.8504629\n",
      "[Epoch 1/5] [Batch 265/360] [D loss: 0.044103] [G loss: 10.395633] time: 0:04:00.140990\n",
      "(30, 64, 64, 3)\n",
      "0.8827295\n",
      "[Epoch 1/5] [Batch 266/360] [D loss: 0.060067] [G loss: 10.078870] time: 0:04:00.366136\n",
      "(30, 64, 64, 3)\n",
      "0.77808255\n",
      "[Epoch 1/5] [Batch 267/360] [D loss: 0.103961] [G loss: 10.553190] time: 0:04:00.542981\n",
      "(30, 64, 64, 3)\n",
      "0.79479367\n",
      "[Epoch 1/5] [Batch 268/360] [D loss: 0.069263] [G loss: 10.224995] time: 0:04:00.709393\n",
      "(30, 64, 64, 3)\n",
      "0.8409979\n",
      "[Epoch 1/5] [Batch 269/360] [D loss: 0.021120] [G loss: 10.481665] time: 0:04:00.864397\n",
      "(30, 64, 64, 3)\n",
      "0.7630046\n",
      "[Epoch 1/5] [Batch 270/360] [D loss: 0.033381] [G loss: 10.427011] time: 0:04:01.045442\n",
      "(30, 64, 64, 3)\n",
      "0.8542405\n",
      "[Epoch 1/5] [Batch 271/360] [D loss: 0.061728] [G loss: 10.948812] time: 0:04:01.228900\n",
      "(30, 64, 64, 3)\n",
      "0.850346\n",
      "[Epoch 1/5] [Batch 272/360] [D loss: 0.031743] [G loss: 9.926161] time: 0:04:01.426848\n",
      "(30, 64, 64, 3)\n",
      "0.8548939\n",
      "[Epoch 1/5] [Batch 273/360] [D loss: 0.014210] [G loss: 10.155290] time: 0:04:01.636487\n",
      "(30, 64, 64, 3)\n",
      "0.86145073\n",
      "[Epoch 1/5] [Batch 274/360] [D loss: 0.019682] [G loss: 9.969718] time: 0:04:01.820167\n",
      "(30, 64, 64, 3)\n",
      "0.85327315\n",
      "[Epoch 1/5] [Batch 275/360] [D loss: 0.014902] [G loss: 10.588321] time: 0:04:02.004280\n",
      "(30, 64, 64, 3)\n",
      "0.7583801\n",
      "[Epoch 1/5] [Batch 276/360] [D loss: 0.011988] [G loss: 10.240867] time: 0:04:02.200818\n",
      "(30, 64, 64, 3)\n",
      "0.8232886\n",
      "[Epoch 1/5] [Batch 277/360] [D loss: 0.026031] [G loss: 10.796651] time: 0:04:02.390134\n",
      "(30, 64, 64, 3)\n",
      "0.80368835\n",
      "[Epoch 1/5] [Batch 278/360] [D loss: 0.229851] [G loss: 10.460450] time: 0:04:02.594466\n",
      "(30, 64, 64, 3)\n",
      "0.86693764\n",
      "[Epoch 1/5] [Batch 279/360] [D loss: 0.032417] [G loss: 10.208077] time: 0:04:02.759544\n",
      "(30, 64, 64, 3)\n",
      "0.76625586\n",
      "[Epoch 1/5] [Batch 280/360] [D loss: 0.030090] [G loss: 10.444667] time: 0:04:02.972930\n",
      "(30, 64, 64, 3)\n",
      "0.85453826\n",
      "[Epoch 1/5] [Batch 281/360] [D loss: 0.091154] [G loss: 10.533751] time: 0:04:03.198317\n",
      "(30, 64, 64, 3)\n",
      "0.86513454\n",
      "[Epoch 1/5] [Batch 282/360] [D loss: 0.338101] [G loss: 9.724272] time: 0:04:03.431808\n",
      "(30, 64, 64, 3)\n",
      "0.86318064\n",
      "[Epoch 1/5] [Batch 283/360] [D loss: 0.035633] [G loss: 9.609518] time: 0:04:03.620294\n",
      "(30, 64, 64, 3)\n",
      "0.8698616\n",
      "[Epoch 1/5] [Batch 284/360] [D loss: 0.142910] [G loss: 10.232991] time: 0:04:03.838968\n",
      "(30, 64, 64, 3)\n",
      "0.8265195\n",
      "[Epoch 1/5] [Batch 285/360] [D loss: 0.060264] [G loss: 9.849954] time: 0:04:04.016373\n",
      "(30, 64, 64, 3)\n",
      "0.86203694\n",
      "[Epoch 1/5] [Batch 286/360] [D loss: 0.072731] [G loss: 9.530831] time: 0:04:04.248425\n",
      "(30, 64, 64, 3)\n",
      "0.8459889\n",
      "[Epoch 1/5] [Batch 287/360] [D loss: 0.025982] [G loss: 10.008951] time: 0:04:04.504029\n",
      "(30, 64, 64, 3)\n",
      "0.8367157\n",
      "[Epoch 1/5] [Batch 288/360] [D loss: 0.016407] [G loss: 10.102016] time: 0:04:04.722146\n",
      "(30, 64, 64, 3)\n",
      "0.81571674\n",
      "[Epoch 1/5] [Batch 289/360] [D loss: 0.012911] [G loss: 9.846527] time: 0:04:04.921231\n",
      "(30, 64, 64, 3)\n",
      "0.8448979\n",
      "[Epoch 1/5] [Batch 290/360] [D loss: 0.016509] [G loss: 9.175504] time: 0:04:05.145568\n",
      "(30, 64, 64, 3)\n",
      "0.86925656\n",
      "[Epoch 1/5] [Batch 291/360] [D loss: 0.017136] [G loss: 10.269197] time: 0:04:05.336007\n",
      "(30, 64, 64, 3)\n",
      "0.77197117\n",
      "[Epoch 1/5] [Batch 292/360] [D loss: 0.011845] [G loss: 10.141925] time: 0:04:05.588030\n",
      "(30, 64, 64, 3)\n",
      "0.8749241\n",
      "[Epoch 1/5] [Batch 293/360] [D loss: 0.015178] [G loss: 10.168641] time: 0:04:05.795908\n",
      "(30, 64, 64, 3)\n",
      "0.8586147\n",
      "[Epoch 1/5] [Batch 294/360] [D loss: 0.015166] [G loss: 9.684262] time: 0:04:05.971017\n",
      "(30, 64, 64, 3)\n",
      "0.8606779\n",
      "[Epoch 1/5] [Batch 295/360] [D loss: 0.010259] [G loss: 9.961617] time: 0:04:06.164514\n",
      "(30, 64, 64, 3)\n",
      "0.85695505\n",
      "[Epoch 1/5] [Batch 296/360] [D loss: 0.017478] [G loss: 9.787992] time: 0:04:06.367534\n",
      "(30, 64, 64, 3)\n",
      "0.8134103\n",
      "[Epoch 1/5] [Batch 297/360] [D loss: 0.010546] [G loss: 9.570606] time: 0:04:06.545068\n",
      "(30, 64, 64, 3)\n",
      "0.8790104\n",
      "[Epoch 1/5] [Batch 298/360] [D loss: 0.009935] [G loss: 9.601557] time: 0:04:06.750606\n",
      "(30, 64, 64, 3)\n",
      "0.8570957\n",
      "[Epoch 1/5] [Batch 299/360] [D loss: 0.011854] [G loss: 9.318510] time: 0:04:06.931975\n",
      "(30, 64, 64, 3)\n",
      "0.8490071\n",
      "[Epoch 1/5] [Batch 300/360] [D loss: 0.008965] [G loss: 9.423849] time: 0:04:07.133634\n",
      "(30, 64, 64, 3)\n",
      "0.82834965\n",
      "[Epoch 1/5] [Batch 301/360] [D loss: 0.010441] [G loss: 9.954537] time: 0:04:07.327693\n",
      "(30, 64, 64, 3)\n",
      "0.8177387\n",
      "[Epoch 1/5] [Batch 302/360] [D loss: 0.016110] [G loss: 9.893490] time: 0:04:07.520030\n",
      "(30, 64, 64, 3)\n",
      "0.86400527\n",
      "[Epoch 1/5] [Batch 303/360] [D loss: 0.010248] [G loss: 10.005666] time: 0:04:07.719081\n",
      "(30, 64, 64, 3)\n",
      "0.84561723\n",
      "[Epoch 1/5] [Batch 304/360] [D loss: 0.014111] [G loss: 9.567921] time: 0:04:07.886906\n",
      "(30, 64, 64, 3)\n",
      "0.8657382\n",
      "[Epoch 1/5] [Batch 305/360] [D loss: 0.009621] [G loss: 9.948355] time: 0:04:08.079149\n",
      "(30, 64, 64, 3)\n",
      "0.8916456\n",
      "[Epoch 1/5] [Batch 306/360] [D loss: 0.010650] [G loss: 9.731984] time: 0:04:08.251759\n",
      "(30, 64, 64, 3)\n",
      "0.8005437\n",
      "[Epoch 1/5] [Batch 307/360] [D loss: 0.010421] [G loss: 9.114942] time: 0:04:08.417150\n",
      "(30, 64, 64, 3)\n",
      "0.8771486\n",
      "[Epoch 1/5] [Batch 308/360] [D loss: 0.008919] [G loss: 10.200412] time: 0:04:08.624181\n",
      "(30, 64, 64, 3)\n",
      "0.78744227\n",
      "[Epoch 1/5] [Batch 309/360] [D loss: 0.014289] [G loss: 9.884434] time: 0:04:08.811586\n",
      "(30, 64, 64, 3)\n",
      "0.8820637\n",
      "[Epoch 1/5] [Batch 310/360] [D loss: 0.009872] [G loss: 9.688173] time: 0:04:08.994218\n",
      "(30, 64, 64, 3)\n",
      "0.8635289\n",
      "[Epoch 1/5] [Batch 311/360] [D loss: 0.010631] [G loss: 9.653303] time: 0:04:09.174247\n",
      "(30, 64, 64, 3)\n",
      "0.8739145\n",
      "[Epoch 1/5] [Batch 312/360] [D loss: 0.008761] [G loss: 10.195224] time: 0:04:09.349457\n",
      "(30, 64, 64, 3)\n",
      "0.85810536\n",
      "[Epoch 1/5] [Batch 313/360] [D loss: 0.018708] [G loss: 9.133027] time: 0:04:09.560804\n",
      "(30, 64, 64, 3)\n",
      "0.8252112\n",
      "[Epoch 1/5] [Batch 314/360] [D loss: 0.013856] [G loss: 9.663775] time: 0:04:09.791802\n",
      "(30, 64, 64, 3)\n",
      "0.8081855\n",
      "[Epoch 1/5] [Batch 315/360] [D loss: 0.010746] [G loss: 9.803483] time: 0:04:09.986347\n",
      "(30, 64, 64, 3)\n",
      "0.8662526\n",
      "[Epoch 1/5] [Batch 316/360] [D loss: 0.011182] [G loss: 8.969952] time: 0:04:10.195983\n",
      "(30, 64, 64, 3)\n",
      "0.84285235\n",
      "[Epoch 1/5] [Batch 317/360] [D loss: 0.014532] [G loss: 9.111957] time: 0:04:10.396044\n",
      "(30, 64, 64, 3)\n",
      "0.871247\n",
      "[Epoch 1/5] [Batch 318/360] [D loss: 0.024013] [G loss: 9.273561] time: 0:04:10.619569\n",
      "(30, 64, 64, 3)\n",
      "0.8339582\n",
      "[Epoch 1/5] [Batch 319/360] [D loss: 0.036596] [G loss: 10.199162] time: 0:04:10.809547\n",
      "(30, 64, 64, 3)\n",
      "0.8593158\n",
      "[Epoch 1/5] [Batch 320/360] [D loss: 0.358910] [G loss: 9.479290] time: 0:04:11.015764\n",
      "(30, 64, 64, 3)\n",
      "0.868112\n",
      "[Epoch 1/5] [Batch 321/360] [D loss: 0.015463] [G loss: 10.061769] time: 0:04:11.215445\n",
      "(30, 64, 64, 3)\n",
      "0.89918685\n",
      "[Epoch 1/5] [Batch 322/360] [D loss: 0.145030] [G loss: 9.553054] time: 0:04:11.405140\n",
      "(30, 64, 64, 3)\n",
      "0.8261234\n",
      "[Epoch 1/5] [Batch 323/360] [D loss: 0.044098] [G loss: 9.681189] time: 0:04:11.564693\n",
      "(30, 64, 64, 3)\n",
      "0.77894616\n",
      "[Epoch 1/5] [Batch 324/360] [D loss: 0.032809] [G loss: 9.236493] time: 0:04:11.733951\n",
      "(30, 64, 64, 3)\n",
      "0.7983187\n",
      "[Epoch 1/5] [Batch 325/360] [D loss: 0.054276] [G loss: 10.177660] time: 0:04:11.938785\n",
      "(30, 64, 64, 3)\n",
      "0.85794187\n",
      "[Epoch 1/5] [Batch 326/360] [D loss: 0.263344] [G loss: 8.969471] time: 0:04:12.086988\n",
      "(30, 64, 64, 3)\n",
      "0.7628643\n",
      "[Epoch 1/5] [Batch 327/360] [D loss: 0.018382] [G loss: 9.018636] time: 0:04:12.278133\n",
      "(30, 64, 64, 3)\n",
      "0.8405602\n",
      "[Epoch 1/5] [Batch 328/360] [D loss: 0.057176] [G loss: 9.870684] time: 0:04:12.458885\n",
      "(30, 64, 64, 3)\n",
      "0.88723445\n",
      "[Epoch 1/5] [Batch 329/360] [D loss: 0.052193] [G loss: 9.416790] time: 0:04:12.649658\n",
      "(30, 64, 64, 3)\n",
      "0.8258731\n",
      "[Epoch 1/5] [Batch 330/360] [D loss: 0.023146] [G loss: 9.685861] time: 0:04:12.853154\n",
      "(30, 64, 64, 3)\n",
      "0.86963934\n",
      "[Epoch 1/5] [Batch 331/360] [D loss: 0.016217] [G loss: 9.994110] time: 0:04:13.038078\n",
      "(30, 64, 64, 3)\n",
      "0.87890834\n",
      "[Epoch 1/5] [Batch 332/360] [D loss: 0.021493] [G loss: 9.786473] time: 0:04:13.292944\n",
      "(30, 64, 64, 3)\n",
      "0.87018746\n",
      "[Epoch 1/5] [Batch 333/360] [D loss: 0.011359] [G loss: 9.711921] time: 0:04:13.474227\n",
      "(30, 64, 64, 3)\n",
      "0.85005695\n",
      "[Epoch 1/5] [Batch 334/360] [D loss: 0.016996] [G loss: 9.207345] time: 0:04:13.640864\n",
      "(30, 64, 64, 3)\n",
      "0.8903656\n",
      "[Epoch 1/5] [Batch 335/360] [D loss: 0.029114] [G loss: 9.582791] time: 0:04:13.855188\n",
      "(30, 64, 64, 3)\n",
      "0.8578474\n",
      "[Epoch 1/5] [Batch 336/360] [D loss: 0.011942] [G loss: 9.537892] time: 0:04:14.052074\n",
      "(30, 64, 64, 3)\n",
      "0.81200266\n",
      "[Epoch 1/5] [Batch 337/360] [D loss: 0.021977] [G loss: 9.290660] time: 0:04:14.272451\n",
      "(30, 64, 64, 3)\n",
      "0.91141015\n",
      "[Epoch 1/5] [Batch 338/360] [D loss: 0.088601] [G loss: 9.943697] time: 0:04:14.477989\n",
      "(30, 64, 64, 3)\n",
      "0.887764\n",
      "[Epoch 1/5] [Batch 339/360] [D loss: 0.756370] [G loss: 8.479629] time: 0:04:14.673202\n",
      "(30, 64, 64, 3)\n",
      "0.8622052\n",
      "[Epoch 1/5] [Batch 340/360] [D loss: 0.100634] [G loss: 9.203760] time: 0:04:14.870793\n",
      "(30, 64, 64, 3)\n",
      "0.8026719\n",
      "[Epoch 1/5] [Batch 341/360] [D loss: 0.120592] [G loss: 9.414018] time: 0:04:15.085895\n",
      "(30, 64, 64, 3)\n",
      "0.8497017\n",
      "[Epoch 1/5] [Batch 342/360] [D loss: 0.133374] [G loss: 9.566379] time: 0:04:15.264247\n",
      "(30, 64, 64, 3)\n",
      "0.8314788\n",
      "[Epoch 1/5] [Batch 343/360] [D loss: 0.097669] [G loss: 9.339572] time: 0:04:15.427606\n",
      "(30, 64, 64, 3)\n",
      "0.83715105\n",
      "[Epoch 1/5] [Batch 344/360] [D loss: 0.082967] [G loss: 9.395182] time: 0:04:15.600867\n",
      "(30, 64, 64, 3)\n",
      "0.80178404\n",
      "[Epoch 1/5] [Batch 345/360] [D loss: 0.066095] [G loss: 9.608442] time: 0:04:15.795252\n",
      "(30, 64, 64, 3)\n",
      "0.7916255\n",
      "[Epoch 1/5] [Batch 346/360] [D loss: 0.073346] [G loss: 9.741716] time: 0:04:15.976388\n",
      "(30, 64, 64, 3)\n",
      "0.86604214\n",
      "[Epoch 1/5] [Batch 347/360] [D loss: 0.095986] [G loss: 9.383099] time: 0:04:16.162498\n",
      "(30, 64, 64, 3)\n",
      "0.8932646\n",
      "[Epoch 1/5] [Batch 348/360] [D loss: 0.065657] [G loss: 9.468138] time: 0:04:16.348149\n",
      "(30, 64, 64, 3)\n",
      "0.83184963\n",
      "[Epoch 1/5] [Batch 349/360] [D loss: 0.053187] [G loss: 9.602016] time: 0:04:16.548460\n",
      "(30, 64, 64, 3)\n",
      "0.8535962\n",
      "[Epoch 1/5] [Batch 350/360] [D loss: 0.125609] [G loss: 10.279267] time: 0:04:16.718065\n",
      "(30, 64, 64, 3)\n",
      "0.84717625\n",
      "[Epoch 1/5] [Batch 351/360] [D loss: 0.060853] [G loss: 8.818589] time: 0:04:16.885317\n",
      "(30, 64, 64, 3)\n",
      "0.8443915\n",
      "[Epoch 1/5] [Batch 352/360] [D loss: 0.022951] [G loss: 9.313452] time: 0:04:17.050921\n",
      "(30, 64, 64, 3)\n",
      "0.8836729\n",
      "[Epoch 1/5] [Batch 353/360] [D loss: 0.035671] [G loss: 9.431663] time: 0:04:17.262164\n",
      "(30, 64, 64, 3)\n",
      "0.895371\n",
      "[Epoch 1/5] [Batch 354/360] [D loss: 0.038570] [G loss: 9.382573] time: 0:04:17.456217\n",
      "(30, 64, 64, 3)\n",
      "0.8812229\n",
      "[Epoch 1/5] [Batch 355/360] [D loss: 0.019787] [G loss: 9.328539] time: 0:04:17.645044\n",
      "(30, 64, 64, 3)\n",
      "0.8426308\n",
      "[Epoch 1/5] [Batch 356/360] [D loss: 0.073235] [G loss: 9.290811] time: 0:04:17.827089\n",
      "(30, 64, 64, 3)\n",
      "0.91351205\n",
      "[Epoch 1/5] [Batch 357/360] [D loss: 0.331272] [G loss: 9.241428] time: 0:04:17.997274\n",
      "(30, 64, 64, 3)\n",
      "0.8684637\n",
      "[Epoch 1/5] [Batch 358/360] [D loss: 0.050683] [G loss: 9.578181] time: 0:04:18.205641\n",
      "(30, 64, 64, 3)\n",
      "0.8668199\n",
      "[Epoch 1/5] [Batch 359/360] [D loss: 0.142009] [G loss: 9.391651] time: 0:04:18.421070\n",
      "(30, 64, 64, 3)\n",
      "0.8294371\n",
      "[Epoch 2/5] [Batch 1/360] [D loss: 0.061276] [G loss: 9.861098] time: 0:04:18.704457\n",
      "(30, 64, 64, 3)\n",
      "0.83730143\n",
      "[Epoch 2/5] [Batch 2/360] [D loss: 0.080587] [G loss: 8.937701] time: 0:04:18.948023\n",
      "(30, 64, 64, 3)\n",
      "0.8337353\n",
      "[Epoch 2/5] [Batch 3/360] [D loss: 0.031199] [G loss: 8.686855] time: 0:04:19.129862\n",
      "(30, 64, 64, 3)\n",
      "0.9096491\n",
      "[Epoch 2/5] [Batch 4/360] [D loss: 0.059155] [G loss: 9.667027] time: 0:04:19.318870\n",
      "(30, 64, 64, 3)\n",
      "0.85804003\n",
      "[Epoch 2/5] [Batch 5/360] [D loss: 0.185219] [G loss: 9.123701] time: 0:04:19.495983\n",
      "(30, 64, 64, 3)\n",
      "0.8227399\n",
      "[Epoch 2/5] [Batch 6/360] [D loss: 0.097509] [G loss: 9.491867] time: 0:04:19.695662\n",
      "(30, 64, 64, 3)\n",
      "0.82282\n",
      "[Epoch 2/5] [Batch 7/360] [D loss: 0.038111] [G loss: 8.957864] time: 0:04:19.924630\n",
      "(30, 64, 64, 3)\n",
      "0.803953\n",
      "[Epoch 2/5] [Batch 8/360] [D loss: 0.032373] [G loss: 9.594410] time: 0:04:20.132094\n",
      "(30, 64, 64, 3)\n",
      "0.8145438\n",
      "[Epoch 2/5] [Batch 9/360] [D loss: 0.093652] [G loss: 9.467355] time: 0:04:20.328311\n",
      "(30, 64, 64, 3)\n",
      "0.86114645\n",
      "[Epoch 2/5] [Batch 10/360] [D loss: 0.062372] [G loss: 9.118885] time: 0:04:20.487447\n",
      "(30, 64, 64, 3)\n",
      "0.86137944\n",
      "[Epoch 2/5] [Batch 11/360] [D loss: 0.024191] [G loss: 8.738434] time: 0:04:20.677588\n",
      "(30, 64, 64, 3)\n",
      "0.87145025\n",
      "[Epoch 2/5] [Batch 12/360] [D loss: 0.041738] [G loss: 9.508257] time: 0:04:20.885163\n",
      "(30, 64, 64, 3)\n",
      "0.8279557\n",
      "[Epoch 2/5] [Batch 13/360] [D loss: 0.384288] [G loss: 8.192307] time: 0:04:21.118258\n",
      "(30, 64, 64, 3)\n",
      "0.8775689\n",
      "[Epoch 2/5] [Batch 14/360] [D loss: 0.108918] [G loss: 9.077845] time: 0:04:21.330480\n",
      "(30, 64, 64, 3)\n",
      "0.8736539\n",
      "[Epoch 2/5] [Batch 15/360] [D loss: 0.081844] [G loss: 8.806483] time: 0:04:21.506580\n",
      "(30, 64, 64, 3)\n",
      "0.85935575\n",
      "[Epoch 2/5] [Batch 16/360] [D loss: 0.103853] [G loss: 8.969625] time: 0:04:21.701719\n",
      "(30, 64, 64, 3)\n",
      "0.83506036\n",
      "[Epoch 2/5] [Batch 17/360] [D loss: 0.063200] [G loss: 9.286945] time: 0:04:21.937248\n",
      "(30, 64, 64, 3)\n",
      "0.9071718\n",
      "[Epoch 2/5] [Batch 18/360] [D loss: 0.048177] [G loss: 9.060016] time: 0:04:22.140566\n",
      "(30, 64, 64, 3)\n",
      "0.828572\n",
      "[Epoch 2/5] [Batch 19/360] [D loss: 0.046040] [G loss: 9.080580] time: 0:04:22.326751\n",
      "(30, 64, 64, 3)\n",
      "0.8288958\n",
      "[Epoch 2/5] [Batch 20/360] [D loss: 0.030770] [G loss: 8.615175] time: 0:04:22.531240\n",
      "(30, 64, 64, 3)\n",
      "0.7948206\n",
      "[Epoch 2/5] [Batch 21/360] [D loss: 0.035842] [G loss: 8.750318] time: 0:04:22.694772\n",
      "(30, 64, 64, 3)\n",
      "0.827998\n",
      "[Epoch 2/5] [Batch 22/360] [D loss: 0.040382] [G loss: 9.056130] time: 0:04:22.913691\n",
      "(30, 64, 64, 3)\n",
      "0.87607694\n",
      "[Epoch 2/5] [Batch 23/360] [D loss: 0.098341] [G loss: 9.541148] time: 0:04:23.150447\n",
      "(30, 64, 64, 3)\n",
      "0.81524295\n",
      "[Epoch 2/5] [Batch 24/360] [D loss: 0.108545] [G loss: 9.222827] time: 0:04:23.391655\n",
      "(30, 64, 64, 3)\n",
      "0.8741928\n",
      "[Epoch 2/5] [Batch 25/360] [D loss: 0.026729] [G loss: 8.765711] time: 0:04:23.595719\n",
      "(30, 64, 64, 3)\n",
      "0.85616106\n",
      "[Epoch 2/5] [Batch 26/360] [D loss: 0.036417] [G loss: 8.667024] time: 0:04:23.827624\n",
      "(30, 64, 64, 3)\n",
      "0.86431545\n",
      "[Epoch 2/5] [Batch 27/360] [D loss: 0.185494] [G loss: 9.737656] time: 0:04:23.994096\n",
      "(30, 64, 64, 3)\n",
      "0.83272004\n",
      "[Epoch 2/5] [Batch 28/360] [D loss: 0.151712] [G loss: 9.094715] time: 0:04:24.177777\n",
      "(30, 64, 64, 3)\n",
      "0.8879122\n",
      "[Epoch 2/5] [Batch 29/360] [D loss: 0.230292] [G loss: 8.728302] time: 0:04:24.391762\n",
      "(30, 64, 64, 3)\n",
      "0.84081906\n",
      "[Epoch 2/5] [Batch 30/360] [D loss: 0.264609] [G loss: 8.596226] time: 0:04:24.657625\n",
      "(30, 64, 64, 3)\n",
      "0.8684618\n",
      "[Epoch 2/5] [Batch 31/360] [D loss: 0.026105] [G loss: 8.608868] time: 0:04:24.861753\n",
      "(30, 64, 64, 3)\n",
      "0.8063553\n",
      "[Epoch 2/5] [Batch 32/360] [D loss: 0.049213] [G loss: 8.651999] time: 0:04:25.069543\n",
      "(30, 64, 64, 3)\n",
      "0.8624727\n",
      "[Epoch 2/5] [Batch 33/360] [D loss: 0.041897] [G loss: 8.769027] time: 0:04:25.272497\n",
      "(30, 64, 64, 3)\n",
      "0.83221406\n",
      "[Epoch 2/5] [Batch 34/360] [D loss: 0.024828] [G loss: 8.940800] time: 0:04:25.433860\n",
      "(30, 64, 64, 3)\n",
      "0.9134926\n",
      "[Epoch 2/5] [Batch 35/360] [D loss: 0.059735] [G loss: 8.464597] time: 0:04:25.635729\n",
      "(30, 64, 64, 3)\n",
      "0.8234108\n",
      "[Epoch 2/5] [Batch 36/360] [D loss: 0.088191] [G loss: 8.941156] time: 0:04:25.847151\n",
      "(30, 64, 64, 3)\n",
      "0.87437105\n",
      "[Epoch 2/5] [Batch 37/360] [D loss: 0.065803] [G loss: 8.625784] time: 0:04:26.044641\n",
      "(30, 64, 64, 3)\n",
      "0.79462034\n",
      "[Epoch 2/5] [Batch 38/360] [D loss: 0.023929] [G loss: 8.942017] time: 0:04:26.253228\n",
      "(30, 64, 64, 3)\n",
      "0.83828515\n",
      "[Epoch 2/5] [Batch 39/360] [D loss: 0.027190] [G loss: 8.392560] time: 0:04:26.459959\n",
      "(30, 64, 64, 3)\n",
      "0.83005947\n",
      "[Epoch 2/5] [Batch 40/360] [D loss: 0.037951] [G loss: 8.259995] time: 0:04:26.638218\n",
      "(30, 64, 64, 3)\n",
      "0.863347\n",
      "[Epoch 2/5] [Batch 41/360] [D loss: 0.018593] [G loss: 8.589247] time: 0:04:26.841852\n",
      "(30, 64, 64, 3)\n",
      "0.87389404\n",
      "[Epoch 2/5] [Batch 42/360] [D loss: 0.019721] [G loss: 9.009238] time: 0:04:27.038382\n",
      "(30, 64, 64, 3)\n",
      "0.83862686\n",
      "[Epoch 2/5] [Batch 43/360] [D loss: 0.122321] [G loss: 8.555246] time: 0:04:27.260180\n",
      "(30, 64, 64, 3)\n",
      "0.8123782\n",
      "[Epoch 2/5] [Batch 44/360] [D loss: 0.142779] [G loss: 8.698080] time: 0:04:27.432731\n",
      "(30, 64, 64, 3)\n",
      "0.8772841\n",
      "[Epoch 2/5] [Batch 45/360] [D loss: 0.122218] [G loss: 8.980123] time: 0:04:27.622237\n",
      "(30, 64, 64, 3)\n",
      "0.8551037\n",
      "[Epoch 2/5] [Batch 46/360] [D loss: 0.202212] [G loss: 8.785493] time: 0:04:27.822136\n",
      "(30, 64, 64, 3)\n",
      "0.81231433\n",
      "[Epoch 2/5] [Batch 47/360] [D loss: 0.038023] [G loss: 9.039211] time: 0:04:27.999455\n",
      "(30, 64, 64, 3)\n",
      "0.83084744\n",
      "[Epoch 2/5] [Batch 48/360] [D loss: 0.032479] [G loss: 9.176832] time: 0:04:28.177754\n",
      "(30, 64, 64, 3)\n",
      "0.8563331\n",
      "[Epoch 2/5] [Batch 49/360] [D loss: 0.028325] [G loss: 8.784700] time: 0:04:28.359779\n",
      "(30, 64, 64, 3)\n",
      "0.84865546\n",
      "[Epoch 2/5] [Batch 50/360] [D loss: 0.031620] [G loss: 8.918521] time: 0:04:28.587699\n",
      "(30, 64, 64, 3)\n",
      "0.85436773\n",
      "[Epoch 2/5] [Batch 51/360] [D loss: 0.118482] [G loss: 8.596505] time: 0:04:28.755832\n",
      "(30, 64, 64, 3)\n",
      "0.87928486\n",
      "[Epoch 2/5] [Batch 52/360] [D loss: 0.102048] [G loss: 7.996586] time: 0:04:28.939820\n",
      "(30, 64, 64, 3)\n",
      "0.82382745\n",
      "[Epoch 2/5] [Batch 53/360] [D loss: 0.028829] [G loss: 8.676611] time: 0:04:29.168952\n",
      "(30, 64, 64, 3)\n",
      "0.84727424\n",
      "[Epoch 2/5] [Batch 54/360] [D loss: 0.023605] [G loss: 9.285073] time: 0:04:29.388966\n",
      "(30, 64, 64, 3)\n",
      "0.854795\n",
      "[Epoch 2/5] [Batch 55/360] [D loss: 0.039181] [G loss: 7.959604] time: 0:04:29.575410\n",
      "(30, 64, 64, 3)\n",
      "0.8268151\n",
      "[Epoch 2/5] [Batch 56/360] [D loss: 0.019777] [G loss: 8.003026] time: 0:04:29.756693\n",
      "(30, 64, 64, 3)\n",
      "0.8718768\n",
      "[Epoch 2/5] [Batch 57/360] [D loss: 0.017651] [G loss: 8.558772] time: 0:04:29.992209\n",
      "(30, 64, 64, 3)\n",
      "0.8783547\n",
      "[Epoch 2/5] [Batch 58/360] [D loss: 0.013886] [G loss: 8.752698] time: 0:04:30.198418\n",
      "(30, 64, 64, 3)\n",
      "0.86127955\n",
      "[Epoch 2/5] [Batch 59/360] [D loss: 0.015672] [G loss: 8.273173] time: 0:04:30.391169\n",
      "(30, 64, 64, 3)\n",
      "0.87124205\n",
      "[Epoch 2/5] [Batch 60/360] [D loss: 0.013632] [G loss: 8.739828] time: 0:04:30.591809\n",
      "(30, 64, 64, 3)\n",
      "0.8463784\n",
      "[Epoch 2/5] [Batch 61/360] [D loss: 0.017148] [G loss: 8.582269] time: 0:04:30.772708\n",
      "(30, 64, 64, 3)\n",
      "0.8740074\n",
      "[Epoch 2/5] [Batch 62/360] [D loss: 0.024886] [G loss: 8.333685] time: 0:04:30.962075\n",
      "(30, 64, 64, 3)\n",
      "0.82229567\n",
      "[Epoch 2/5] [Batch 63/360] [D loss: 0.017092] [G loss: 8.569901] time: 0:04:31.143992\n",
      "(30, 64, 64, 3)\n",
      "0.8298368\n",
      "[Epoch 2/5] [Batch 64/360] [D loss: 0.020055] [G loss: 7.812581] time: 0:04:31.363736\n",
      "(30, 64, 64, 3)\n",
      "0.83626026\n",
      "[Epoch 2/5] [Batch 65/360] [D loss: 0.019750] [G loss: 8.160746] time: 0:04:31.571871\n",
      "(30, 64, 64, 3)\n",
      "0.82567215\n",
      "[Epoch 2/5] [Batch 66/360] [D loss: 0.011548] [G loss: 8.319510] time: 0:04:31.775177\n",
      "(30, 64, 64, 3)\n",
      "0.86699325\n",
      "[Epoch 2/5] [Batch 67/360] [D loss: 0.014667] [G loss: 7.807200] time: 0:04:31.968717\n",
      "(30, 64, 64, 3)\n",
      "0.8357032\n",
      "[Epoch 2/5] [Batch 68/360] [D loss: 0.017873] [G loss: 7.814360] time: 0:04:32.140591\n",
      "(30, 64, 64, 3)\n",
      "0.88638407\n",
      "[Epoch 2/5] [Batch 69/360] [D loss: 0.041268] [G loss: 8.074452] time: 0:04:32.393861\n",
      "(30, 64, 64, 3)\n",
      "0.8213458\n",
      "[Epoch 2/5] [Batch 70/360] [D loss: 0.112125] [G loss: 8.824152] time: 0:04:32.614249\n",
      "(30, 64, 64, 3)\n",
      "0.9060438\n",
      "[Epoch 2/5] [Batch 71/360] [D loss: 0.279975] [G loss: 8.601279] time: 0:04:32.837820\n",
      "(30, 64, 64, 3)\n",
      "0.86965466\n",
      "[Epoch 2/5] [Batch 72/360] [D loss: 0.470416] [G loss: 8.093515] time: 0:04:33.041484\n",
      "(30, 64, 64, 3)\n",
      "0.911088\n",
      "[Epoch 2/5] [Batch 73/360] [D loss: 0.026531] [G loss: 8.244648] time: 0:04:33.244698\n",
      "(30, 64, 64, 3)\n",
      "0.8982093\n",
      "[Epoch 2/5] [Batch 74/360] [D loss: 0.095450] [G loss: 8.184966] time: 0:04:33.453221\n",
      "(30, 64, 64, 3)\n",
      "0.82437104\n",
      "[Epoch 2/5] [Batch 75/360] [D loss: 0.029930] [G loss: 8.052133] time: 0:04:33.678241\n",
      "(30, 64, 64, 3)\n",
      "0.8637211\n",
      "[Epoch 2/5] [Batch 76/360] [D loss: 0.034493] [G loss: 8.102020] time: 0:04:33.874970\n",
      "(30, 64, 64, 3)\n",
      "0.88046616\n",
      "[Epoch 2/5] [Batch 77/360] [D loss: 0.034916] [G loss: 8.209344] time: 0:04:34.041855\n",
      "(30, 64, 64, 3)\n",
      "0.8104079\n",
      "[Epoch 2/5] [Batch 78/360] [D loss: 0.033981] [G loss: 8.522060] time: 0:04:34.234197\n",
      "(30, 64, 64, 3)\n",
      "0.8021083\n",
      "[Epoch 2/5] [Batch 79/360] [D loss: 0.019435] [G loss: 8.504379] time: 0:04:34.402176\n",
      "(30, 64, 64, 3)\n",
      "0.8565903\n",
      "[Epoch 2/5] [Batch 80/360] [D loss: 0.026692] [G loss: 8.019991] time: 0:04:34.603708\n",
      "(30, 64, 64, 3)\n",
      "0.905528\n",
      "[Epoch 2/5] [Batch 81/360] [D loss: 0.028049] [G loss: 8.670983] time: 0:04:34.806683\n",
      "(30, 64, 64, 3)\n",
      "0.8645107\n",
      "[Epoch 2/5] [Batch 82/360] [D loss: 0.019737] [G loss: 8.211490] time: 0:04:34.994737\n",
      "(30, 64, 64, 3)\n",
      "0.8126216\n",
      "[Epoch 2/5] [Batch 83/360] [D loss: 0.018376] [G loss: 8.558345] time: 0:04:35.220049\n",
      "(30, 64, 64, 3)\n",
      "0.8671157\n",
      "[Epoch 2/5] [Batch 84/360] [D loss: 0.020416] [G loss: 8.281677] time: 0:04:35.394125\n",
      "(30, 64, 64, 3)\n",
      "0.80764514\n",
      "[Epoch 2/5] [Batch 85/360] [D loss: 0.017598] [G loss: 7.964083] time: 0:04:35.575289\n",
      "(30, 64, 64, 3)\n",
      "0.8633423\n",
      "[Epoch 2/5] [Batch 86/360] [D loss: 0.031299] [G loss: 8.195501] time: 0:04:35.772742\n",
      "(30, 64, 64, 3)\n",
      "0.9173441\n",
      "[Epoch 2/5] [Batch 87/360] [D loss: 0.045679] [G loss: 8.686682] time: 0:04:35.992058\n",
      "(30, 64, 64, 3)\n",
      "0.90097827\n",
      "[Epoch 2/5] [Batch 88/360] [D loss: 0.063157] [G loss: 8.317509] time: 0:04:36.170713\n",
      "(30, 64, 64, 3)\n",
      "0.91806465\n",
      "[Epoch 2/5] [Batch 89/360] [D loss: 0.026445] [G loss: 8.057409] time: 0:04:36.369269\n",
      "(30, 64, 64, 3)\n",
      "0.8308328\n",
      "[Epoch 2/5] [Batch 90/360] [D loss: 0.018093] [G loss: 8.597827] time: 0:04:36.571562\n",
      "(30, 64, 64, 3)\n",
      "0.85440975\n",
      "[Epoch 2/5] [Batch 91/360] [D loss: 0.041015] [G loss: 8.205815] time: 0:04:36.799661\n",
      "(30, 64, 64, 3)\n",
      "0.8770712\n",
      "[Epoch 2/5] [Batch 92/360] [D loss: 0.066663] [G loss: 8.017258] time: 0:04:37.009404\n",
      "(30, 64, 64, 3)\n",
      "0.85368663\n",
      "[Epoch 2/5] [Batch 93/360] [D loss: 0.075796] [G loss: 7.783727] time: 0:04:37.201414\n",
      "(30, 64, 64, 3)\n",
      "0.80948\n",
      "[Epoch 2/5] [Batch 94/360] [D loss: 0.037665] [G loss: 8.116616] time: 0:04:37.451984\n",
      "(30, 64, 64, 3)\n",
      "0.8252127\n",
      "[Epoch 2/5] [Batch 95/360] [D loss: 0.018470] [G loss: 8.087255] time: 0:04:37.674791\n",
      "(30, 64, 64, 3)\n",
      "0.85213965\n",
      "[Epoch 2/5] [Batch 96/360] [D loss: 0.020403] [G loss: 8.713223] time: 0:04:37.861370\n",
      "(30, 64, 64, 3)\n",
      "0.84823674\n",
      "[Epoch 2/5] [Batch 97/360] [D loss: 0.030459] [G loss: 7.988657] time: 0:04:38.070175\n",
      "(30, 64, 64, 3)\n",
      "0.83792514\n",
      "[Epoch 2/5] [Batch 98/360] [D loss: 0.019041] [G loss: 7.867125] time: 0:04:38.291885\n",
      "(30, 64, 64, 3)\n",
      "0.8247053\n",
      "[Epoch 2/5] [Batch 99/360] [D loss: 0.026908] [G loss: 7.747796] time: 0:04:38.484760\n",
      "(30, 64, 64, 3)\n",
      "0.88085383\n",
      "[Epoch 2/5] [Batch 100/360] [D loss: 0.039353] [G loss: 8.311252] time: 0:04:38.696520\n",
      "(30, 64, 64, 3)\n",
      "0.91833705\n",
      "[Epoch 2/5] [Batch 101/360] [D loss: 0.073396] [G loss: 7.941414] time: 0:04:38.912136\n",
      "(30, 64, 64, 3)\n",
      "0.8298783\n",
      "[Epoch 2/5] [Batch 102/360] [D loss: 0.117036] [G loss: 8.071662] time: 0:04:39.155623\n",
      "(30, 64, 64, 3)\n",
      "0.8244327\n",
      "[Epoch 2/5] [Batch 103/360] [D loss: 0.497948] [G loss: 7.180910] time: 0:04:39.329252\n",
      "(30, 64, 64, 3)\n",
      "0.87057585\n",
      "[Epoch 2/5] [Batch 104/360] [D loss: 0.028174] [G loss: 7.889644] time: 0:04:39.498126\n",
      "(30, 64, 64, 3)\n",
      "0.86954236\n",
      "[Epoch 2/5] [Batch 105/360] [D loss: 0.219228] [G loss: 7.760309] time: 0:04:39.747655\n",
      "(30, 64, 64, 3)\n",
      "0.9014004\n",
      "[Epoch 2/5] [Batch 106/360] [D loss: 0.084501] [G loss: 7.318175] time: 0:04:39.920430\n",
      "(30, 64, 64, 3)\n",
      "0.8883131\n",
      "[Epoch 2/5] [Batch 107/360] [D loss: 0.056590] [G loss: 7.906036] time: 0:04:40.098853\n",
      "(30, 64, 64, 3)\n",
      "0.87138224\n",
      "[Epoch 2/5] [Batch 108/360] [D loss: 0.083923] [G loss: 8.070314] time: 0:04:40.285440\n",
      "(30, 64, 64, 3)\n",
      "0.84345865\n",
      "[Epoch 2/5] [Batch 109/360] [D loss: 0.092952] [G loss: 7.267732] time: 0:04:40.472162\n",
      "(30, 64, 64, 3)\n",
      "0.8847668\n",
      "[Epoch 2/5] [Batch 110/360] [D loss: 0.035258] [G loss: 7.635301] time: 0:04:40.668774\n",
      "(30, 64, 64, 3)\n",
      "0.8569835\n",
      "[Epoch 2/5] [Batch 111/360] [D loss: 0.031521] [G loss: 8.128624] time: 0:04:40.843742\n",
      "(30, 64, 64, 3)\n",
      "0.92863846\n",
      "[Epoch 2/5] [Batch 112/360] [D loss: 0.034099] [G loss: 8.209901] time: 0:04:41.062322\n",
      "(30, 64, 64, 3)\n",
      "0.8874466\n",
      "[Epoch 2/5] [Batch 113/360] [D loss: 0.035507] [G loss: 7.901157] time: 0:04:41.254446\n",
      "(30, 64, 64, 3)\n",
      "0.7769943\n",
      "[Epoch 2/5] [Batch 114/360] [D loss: 0.020904] [G loss: 7.889304] time: 0:04:41.451500\n",
      "(30, 64, 64, 3)\n",
      "0.82685477\n",
      "[Epoch 2/5] [Batch 115/360] [D loss: 0.017803] [G loss: 8.122330] time: 0:04:41.645122\n",
      "(30, 64, 64, 3)\n",
      "0.8492196\n",
      "[Epoch 2/5] [Batch 116/360] [D loss: 0.018271] [G loss: 7.419212] time: 0:04:41.814243\n",
      "(30, 64, 64, 3)\n",
      "0.7971919\n",
      "[Epoch 2/5] [Batch 117/360] [D loss: 0.017901] [G loss: 7.835560] time: 0:04:42.010633\n",
      "(30, 64, 64, 3)\n",
      "0.84625524\n",
      "[Epoch 2/5] [Batch 118/360] [D loss: 0.020640] [G loss: 7.823187] time: 0:04:42.202010\n",
      "(30, 64, 64, 3)\n",
      "0.85285586\n",
      "[Epoch 2/5] [Batch 119/360] [D loss: 0.015705] [G loss: 7.489946] time: 0:04:42.416801\n",
      "(30, 64, 64, 3)\n",
      "0.8988684\n",
      "[Epoch 2/5] [Batch 120/360] [D loss: 0.011822] [G loss: 8.218699] time: 0:04:42.624684\n",
      "(30, 64, 64, 3)\n",
      "0.89731413\n",
      "[Epoch 2/5] [Batch 121/360] [D loss: 0.026416] [G loss: 7.453041] time: 0:04:42.845119\n",
      "(30, 64, 64, 3)\n",
      "0.85965544\n",
      "[Epoch 2/5] [Batch 122/360] [D loss: 0.021117] [G loss: 7.768237] time: 0:04:43.042097\n",
      "(30, 64, 64, 3)\n",
      "0.8826852\n",
      "[Epoch 2/5] [Batch 123/360] [D loss: 0.020932] [G loss: 7.426774] time: 0:04:43.262170\n",
      "(30, 64, 64, 3)\n",
      "0.8511303\n",
      "[Epoch 2/5] [Batch 124/360] [D loss: 0.038393] [G loss: 7.547175] time: 0:04:43.443377\n",
      "(30, 64, 64, 3)\n",
      "0.84673613\n",
      "[Epoch 2/5] [Batch 125/360] [D loss: 0.086005] [G loss: 7.861975] time: 0:04:43.664502\n",
      "(30, 64, 64, 3)\n",
      "0.8948348\n",
      "[Epoch 2/5] [Batch 126/360] [D loss: 0.378372] [G loss: 7.846583] time: 0:04:43.856382\n",
      "(30, 64, 64, 3)\n",
      "0.835969\n",
      "[Epoch 2/5] [Batch 127/360] [D loss: 0.019718] [G loss: 7.881302] time: 0:04:44.093134\n",
      "(30, 64, 64, 3)\n",
      "0.89058524\n",
      "[Epoch 2/5] [Batch 128/360] [D loss: 0.132774] [G loss: 8.006011] time: 0:04:44.291053\n",
      "(30, 64, 64, 3)\n",
      "0.8730231\n",
      "[Epoch 2/5] [Batch 129/360] [D loss: 0.044770] [G loss: 7.601029] time: 0:04:44.453418\n",
      "(30, 64, 64, 3)\n",
      "0.883262\n",
      "[Epoch 2/5] [Batch 130/360] [D loss: 0.042901] [G loss: 7.572725] time: 0:04:44.624768\n",
      "(30, 64, 64, 3)\n",
      "0.8860381\n",
      "[Epoch 2/5] [Batch 131/360] [D loss: 0.398797] [G loss: 7.333388] time: 0:04:44.805264\n",
      "(30, 64, 64, 3)\n",
      "0.85509473\n",
      "[Epoch 2/5] [Batch 132/360] [D loss: 0.070601] [G loss: 7.732456] time: 0:04:45.017821\n",
      "(30, 64, 64, 3)\n",
      "0.8181853\n",
      "[Epoch 2/5] [Batch 133/360] [D loss: 0.132033] [G loss: 7.658635] time: 0:04:45.201366\n",
      "(30, 64, 64, 3)\n",
      "0.88615435\n",
      "[Epoch 2/5] [Batch 134/360] [D loss: 0.060943] [G loss: 7.864050] time: 0:04:45.404843\n",
      "(30, 64, 64, 3)\n",
      "0.8955144\n",
      "[Epoch 2/5] [Batch 135/360] [D loss: 0.049307] [G loss: 7.281221] time: 0:04:45.615636\n",
      "(30, 64, 64, 3)\n",
      "0.8707824\n",
      "[Epoch 2/5] [Batch 136/360] [D loss: 0.036198] [G loss: 7.197370] time: 0:04:45.805400\n",
      "(30, 64, 64, 3)\n",
      "0.86407095\n",
      "[Epoch 2/5] [Batch 137/360] [D loss: 0.031770] [G loss: 8.074692] time: 0:04:46.000699\n",
      "(30, 64, 64, 3)\n",
      "0.894064\n",
      "[Epoch 2/5] [Batch 138/360] [D loss: 0.037280] [G loss: 7.628061] time: 0:04:46.182917\n",
      "(30, 64, 64, 3)\n",
      "0.884248\n",
      "[Epoch 2/5] [Batch 139/360] [D loss: 0.035826] [G loss: 7.742716] time: 0:04:46.423172\n",
      "(30, 64, 64, 3)\n",
      "0.82958835\n",
      "[Epoch 2/5] [Batch 140/360] [D loss: 0.032243] [G loss: 7.545616] time: 0:04:46.622503\n",
      "(30, 64, 64, 3)\n",
      "0.8209071\n",
      "[Epoch 2/5] [Batch 141/360] [D loss: 0.045569] [G loss: 7.860268] time: 0:04:46.835982\n",
      "(30, 64, 64, 3)\n",
      "0.916929\n",
      "[Epoch 2/5] [Batch 142/360] [D loss: 0.108207] [G loss: 7.598845] time: 0:04:47.020799\n",
      "(30, 64, 64, 3)\n",
      "0.87901324\n",
      "[Epoch 2/5] [Batch 143/360] [D loss: 0.409806] [G loss: 7.465102] time: 0:04:47.256724\n",
      "(30, 64, 64, 3)\n",
      "0.8705295\n",
      "[Epoch 2/5] [Batch 144/360] [D loss: 0.076579] [G loss: 7.961498] time: 0:04:47.481117\n",
      "(30, 64, 64, 3)\n",
      "0.82445353\n",
      "[Epoch 2/5] [Batch 145/360] [D loss: 0.080313] [G loss: 7.184301] time: 0:04:47.721677\n",
      "(30, 64, 64, 3)\n",
      "0.84126455\n",
      "[Epoch 2/5] [Batch 146/360] [D loss: 0.098047] [G loss: 7.517327] time: 0:04:47.930230\n",
      "(30, 64, 64, 3)\n",
      "0.78222543\n",
      "[Epoch 2/5] [Batch 147/360] [D loss: 0.080329] [G loss: 7.353908] time: 0:04:48.110267\n",
      "(30, 64, 64, 3)\n",
      "0.8881884\n",
      "[Epoch 2/5] [Batch 148/360] [D loss: 0.047009] [G loss: 7.594724] time: 0:04:48.319497\n",
      "(30, 64, 64, 3)\n",
      "0.8912077\n",
      "[Epoch 2/5] [Batch 149/360] [D loss: 0.053652] [G loss: 7.080897] time: 0:04:48.507117\n",
      "(30, 64, 64, 3)\n",
      "0.8576812\n",
      "[Epoch 2/5] [Batch 150/360] [D loss: 0.044802] [G loss: 7.368513] time: 0:04:48.731978\n",
      "(30, 64, 64, 3)\n",
      "0.87642556\n",
      "[Epoch 2/5] [Batch 151/360] [D loss: 0.028555] [G loss: 7.503169] time: 0:04:48.969553\n",
      "(30, 64, 64, 3)\n",
      "0.84407735\n",
      "[Epoch 2/5] [Batch 152/360] [D loss: 0.025001] [G loss: 7.562111] time: 0:04:49.151835\n",
      "(30, 64, 64, 3)\n",
      "0.8380553\n",
      "[Epoch 2/5] [Batch 153/360] [D loss: 0.025086] [G loss: 7.654046] time: 0:04:49.329840\n",
      "(30, 64, 64, 3)\n",
      "0.8454096\n",
      "[Epoch 2/5] [Batch 154/360] [D loss: 0.048379] [G loss: 7.455315] time: 0:04:49.538317\n",
      "(30, 64, 64, 3)\n",
      "0.9096144\n",
      "[Epoch 2/5] [Batch 155/360] [D loss: 0.167919] [G loss: 7.889795] time: 0:04:49.753239\n",
      "(30, 64, 64, 3)\n",
      "0.81631273\n",
      "[Epoch 2/5] [Batch 156/360] [D loss: 0.465243] [G loss: 6.701303] time: 0:04:49.952368\n",
      "(30, 64, 64, 3)\n",
      "0.9101365\n",
      "[Epoch 2/5] [Batch 157/360] [D loss: 0.104978] [G loss: 7.371297] time: 0:04:50.158848\n",
      "(30, 64, 64, 3)\n",
      "0.81986856\n",
      "[Epoch 2/5] [Batch 158/360] [D loss: 0.082504] [G loss: 7.359524] time: 0:04:50.342258\n",
      "(30, 64, 64, 3)\n",
      "0.8912678\n",
      "[Epoch 2/5] [Batch 159/360] [D loss: 0.093935] [G loss: 7.584564] time: 0:04:50.562921\n",
      "(30, 64, 64, 3)\n",
      "0.910346\n",
      "[Epoch 2/5] [Batch 160/360] [D loss: 0.113584] [G loss: 7.370079] time: 0:04:50.738036\n",
      "(30, 64, 64, 3)\n",
      "0.8921368\n",
      "[Epoch 2/5] [Batch 161/360] [D loss: 0.035008] [G loss: 7.290982] time: 0:04:50.913456\n",
      "(30, 64, 64, 3)\n",
      "0.8836892\n",
      "[Epoch 2/5] [Batch 162/360] [D loss: 0.043814] [G loss: 7.023370] time: 0:04:51.086773\n",
      "(30, 64, 64, 3)\n",
      "0.92156225\n",
      "[Epoch 2/5] [Batch 163/360] [D loss: 0.028288] [G loss: 7.731320] time: 0:04:51.322730\n",
      "(30, 64, 64, 3)\n",
      "0.8621404\n",
      "[Epoch 2/5] [Batch 164/360] [D loss: 0.030102] [G loss: 7.369248] time: 0:04:51.518139\n",
      "(30, 64, 64, 3)\n",
      "0.8493285\n",
      "[Epoch 2/5] [Batch 165/360] [D loss: 0.028900] [G loss: 7.022588] time: 0:04:51.693450\n",
      "(30, 64, 64, 3)\n",
      "0.8530577\n",
      "[Epoch 2/5] [Batch 166/360] [D loss: 0.027852] [G loss: 7.345344] time: 0:04:51.902084\n",
      "(30, 64, 64, 3)\n",
      "0.81625485\n",
      "[Epoch 2/5] [Batch 167/360] [D loss: 0.075898] [G loss: 7.533359] time: 0:04:52.139715\n",
      "(30, 64, 64, 3)\n",
      "0.885516\n",
      "[Epoch 2/5] [Batch 168/360] [D loss: 0.092414] [G loss: 7.901878] time: 0:04:52.357901\n",
      "(30, 64, 64, 3)\n",
      "0.84806895\n",
      "[Epoch 2/5] [Batch 169/360] [D loss: 0.051813] [G loss: 7.172698] time: 0:04:52.593530\n",
      "(30, 64, 64, 3)\n",
      "0.9305434\n",
      "[Epoch 2/5] [Batch 170/360] [D loss: 0.020888] [G loss: 7.667854] time: 0:04:52.804846\n",
      "(30, 64, 64, 3)\n",
      "0.8481843\n",
      "[Epoch 2/5] [Batch 171/360] [D loss: 0.033410] [G loss: 6.853996] time: 0:04:53.004400\n",
      "(30, 64, 64, 3)\n",
      "0.8789675\n",
      "[Epoch 2/5] [Batch 172/360] [D loss: 0.038934] [G loss: 7.236163] time: 0:04:53.213521\n",
      "(30, 64, 64, 3)\n",
      "0.856528\n",
      "[Epoch 2/5] [Batch 173/360] [D loss: 0.078139] [G loss: 7.406964] time: 0:04:53.426488\n",
      "(30, 64, 64, 3)\n",
      "0.81847745\n",
      "[Epoch 2/5] [Batch 174/360] [D loss: 0.238712] [G loss: 7.395112] time: 0:04:53.656044\n",
      "(30, 64, 64, 3)\n",
      "0.89218384\n",
      "[Epoch 2/5] [Batch 175/360] [D loss: 0.076891] [G loss: 7.622932] time: 0:04:53.817175\n",
      "(30, 64, 64, 3)\n",
      "0.8884743\n",
      "[Epoch 2/5] [Batch 176/360] [D loss: 0.026630] [G loss: 7.129149] time: 0:04:54.010356\n",
      "(30, 64, 64, 3)\n",
      "0.91068\n",
      "[Epoch 2/5] [Batch 177/360] [D loss: 0.041243] [G loss: 7.127751] time: 0:04:54.192717\n",
      "(30, 64, 64, 3)\n",
      "0.8771736\n",
      "[Epoch 2/5] [Batch 178/360] [D loss: 0.204335] [G loss: 7.246322] time: 0:04:54.397564\n",
      "(30, 64, 64, 3)\n",
      "0.85753375\n",
      "[Epoch 2/5] [Batch 179/360] [D loss: 0.361046] [G loss: 6.766628] time: 0:04:54.608834\n",
      "(30, 64, 64, 3)\n",
      "0.8681874\n",
      "[Epoch 2/5] [Batch 180/360] [D loss: 0.029016] [G loss: 7.688205] time: 0:04:54.836451\n",
      "(30, 64, 64, 3)\n",
      "0.8290207\n",
      "[Epoch 2/5] [Batch 181/360] [D loss: 0.241346] [G loss: 6.667188] time: 0:04:55.048200\n",
      "(30, 64, 64, 3)\n",
      "0.8521549\n",
      "[Epoch 2/5] [Batch 182/360] [D loss: 0.088477] [G loss: 8.024444] time: 0:04:55.243150\n",
      "(30, 64, 64, 3)\n",
      "0.89035577\n",
      "[Epoch 2/5] [Batch 183/360] [D loss: 0.087399] [G loss: 7.243837] time: 0:04:55.440038\n",
      "(30, 64, 64, 3)\n",
      "0.8543518\n",
      "[Epoch 2/5] [Batch 184/360] [D loss: 0.083781] [G loss: 8.267483] time: 0:04:55.619926\n",
      "(30, 64, 64, 3)\n",
      "0.8397121\n",
      "[Epoch 2/5] [Batch 185/360] [D loss: 0.166873] [G loss: 7.348469] time: 0:04:55.804080\n",
      "(30, 64, 64, 3)\n",
      "0.843241\n",
      "[Epoch 2/5] [Batch 186/360] [D loss: 0.038669] [G loss: 7.216058] time: 0:04:55.978991\n",
      "(30, 64, 64, 3)\n",
      "0.8551695\n",
      "[Epoch 2/5] [Batch 187/360] [D loss: 0.051752] [G loss: 7.107588] time: 0:04:56.149712\n",
      "(30, 64, 64, 3)\n",
      "0.8408992\n",
      "[Epoch 2/5] [Batch 188/360] [D loss: 0.040534] [G loss: 7.074183] time: 0:04:56.365127\n",
      "(30, 64, 64, 3)\n",
      "0.8564852\n",
      "[Epoch 2/5] [Batch 189/360] [D loss: 0.030816] [G loss: 6.949321] time: 0:04:56.589554\n",
      "(30, 64, 64, 3)\n",
      "0.8808708\n",
      "[Epoch 2/5] [Batch 190/360] [D loss: 0.048668] [G loss: 7.038843] time: 0:04:56.825760\n",
      "(30, 64, 64, 3)\n",
      "0.9066059\n",
      "[Epoch 2/5] [Batch 191/360] [D loss: 0.032414] [G loss: 6.718074] time: 0:04:57.006596\n",
      "(30, 64, 64, 3)\n",
      "0.86376387\n",
      "[Epoch 2/5] [Batch 192/360] [D loss: 0.023859] [G loss: 7.152301] time: 0:04:57.231552\n",
      "(30, 64, 64, 3)\n",
      "0.8789455\n",
      "[Epoch 2/5] [Batch 193/360] [D loss: 0.027525] [G loss: 7.368412] time: 0:04:57.414645\n",
      "(30, 64, 64, 3)\n",
      "0.8750667\n",
      "[Epoch 2/5] [Batch 194/360] [D loss: 0.025302] [G loss: 6.725982] time: 0:04:57.588488\n",
      "(30, 64, 64, 3)\n",
      "0.84113437\n",
      "[Epoch 2/5] [Batch 195/360] [D loss: 0.029409] [G loss: 7.054451] time: 0:04:57.771508\n",
      "(30, 64, 64, 3)\n",
      "0.84558994\n",
      "[Epoch 2/5] [Batch 196/360] [D loss: 0.021422] [G loss: 7.441612] time: 0:04:57.962794\n",
      "(30, 64, 64, 3)\n",
      "0.8311201\n",
      "[Epoch 2/5] [Batch 197/360] [D loss: 0.017656] [G loss: 7.414119] time: 0:04:58.118803\n",
      "(30, 64, 64, 3)\n",
      "0.8916761\n",
      "[Epoch 2/5] [Batch 198/360] [D loss: 0.016202] [G loss: 7.260391] time: 0:04:58.297775\n",
      "(30, 64, 64, 3)\n",
      "0.8885026\n",
      "[Epoch 2/5] [Batch 199/360] [D loss: 0.023141] [G loss: 6.768197] time: 0:04:58.458479\n",
      "(30, 64, 64, 3)\n",
      "0.8380385\n",
      "[Epoch 2/5] [Batch 200/360] [D loss: 0.035858] [G loss: 7.439940] time: 0:04:58.649802\n",
      "(30, 64, 64, 3)\n",
      "0.8250577\n",
      "[Epoch 2/5] [Batch 201/360] [D loss: 0.028790] [G loss: 6.935070] time: 0:04:58.856381\n",
      "(30, 64, 64, 3)\n",
      "0.9245265\n",
      "[Epoch 2/5] [Batch 202/360] [D loss: 0.019436] [G loss: 7.271949] time: 0:04:59.095734\n",
      "(30, 64, 64, 3)\n",
      "0.87991714\n",
      "[Epoch 2/5] [Batch 203/360] [D loss: 0.054723] [G loss: 7.296504] time: 0:04:59.276869\n",
      "(30, 64, 64, 3)\n",
      "0.8568986\n",
      "[Epoch 2/5] [Batch 204/360] [D loss: 0.137438] [G loss: 7.271648] time: 0:04:59.486213\n",
      "(30, 64, 64, 3)\n",
      "0.9053251\n",
      "[Epoch 2/5] [Batch 205/360] [D loss: 0.074515] [G loss: 7.497812] time: 0:04:59.690105\n",
      "(30, 64, 64, 3)\n",
      "0.8746233\n",
      "[Epoch 2/5] [Batch 206/360] [D loss: 0.015603] [G loss: 7.170533] time: 0:04:59.844482\n",
      "(30, 64, 64, 3)\n",
      "0.8896411\n",
      "[Epoch 2/5] [Batch 207/360] [D loss: 0.016689] [G loss: 7.350577] time: 0:05:00.038404\n",
      "(30, 64, 64, 3)\n",
      "0.8386452\n",
      "[Epoch 2/5] [Batch 208/360] [D loss: 0.020508] [G loss: 7.374006] time: 0:05:00.245741\n",
      "(30, 64, 64, 3)\n",
      "0.8432633\n",
      "[Epoch 2/5] [Batch 209/360] [D loss: 0.021996] [G loss: 7.407105] time: 0:05:00.454768\n",
      "(30, 64, 64, 3)\n",
      "0.8540407\n",
      "[Epoch 2/5] [Batch 210/360] [D loss: 0.015416] [G loss: 6.555439] time: 0:05:00.635349\n",
      "(30, 64, 64, 3)\n",
      "0.87246084\n",
      "[Epoch 2/5] [Batch 211/360] [D loss: 0.023274] [G loss: 7.583427] time: 0:05:00.797851\n",
      "(30, 64, 64, 3)\n",
      "0.89635324\n",
      "[Epoch 2/5] [Batch 212/360] [D loss: 0.027763] [G loss: 7.105660] time: 0:05:01.033084\n",
      "(30, 64, 64, 3)\n",
      "0.8610514\n",
      "[Epoch 2/5] [Batch 213/360] [D loss: 0.097539] [G loss: 7.476156] time: 0:05:01.240919\n",
      "(30, 64, 64, 3)\n",
      "0.8706843\n",
      "[Epoch 2/5] [Batch 214/360] [D loss: 0.442292] [G loss: 6.532024] time: 0:05:01.460802\n",
      "(30, 64, 64, 3)\n",
      "0.8467373\n",
      "[Epoch 2/5] [Batch 215/360] [D loss: 0.069036] [G loss: 7.346165] time: 0:05:01.657232\n",
      "(30, 64, 64, 3)\n",
      "0.8687787\n",
      "[Epoch 2/5] [Batch 216/360] [D loss: 0.473522] [G loss: 5.697380] time: 0:05:01.855209\n",
      "(30, 64, 64, 3)\n",
      "0.91437435\n",
      "[Epoch 2/5] [Batch 217/360] [D loss: 0.159520] [G loss: 6.115507] time: 0:05:02.035170\n",
      "(30, 64, 64, 3)\n",
      "0.89443177\n",
      "[Epoch 2/5] [Batch 218/360] [D loss: 0.102388] [G loss: 6.485472] time: 0:05:02.225939\n",
      "(30, 64, 64, 3)\n",
      "0.8467925\n",
      "[Epoch 2/5] [Batch 219/360] [D loss: 0.122477] [G loss: 6.575648] time: 0:05:02.433448\n",
      "(30, 64, 64, 3)\n",
      "0.87768775\n",
      "[Epoch 2/5] [Batch 220/360] [D loss: 0.105584] [G loss: 6.661679] time: 0:05:02.629400\n",
      "(30, 64, 64, 3)\n",
      "0.9223562\n",
      "[Epoch 2/5] [Batch 221/360] [D loss: 0.092934] [G loss: 6.599540] time: 0:05:02.846932\n",
      "(30, 64, 64, 3)\n",
      "0.9131322\n",
      "[Epoch 2/5] [Batch 222/360] [D loss: 0.080542] [G loss: 6.760729] time: 0:05:03.074707\n",
      "(30, 64, 64, 3)\n",
      "0.83985823\n",
      "[Epoch 2/5] [Batch 223/360] [D loss: 0.092663] [G loss: 6.458893] time: 0:05:03.295382\n",
      "(30, 64, 64, 3)\n",
      "0.8799613\n",
      "[Epoch 2/5] [Batch 224/360] [D loss: 0.095030] [G loss: 6.498758] time: 0:05:03.499892\n",
      "(30, 64, 64, 3)\n",
      "0.8728852\n",
      "[Epoch 2/5] [Batch 225/360] [D loss: 0.075467] [G loss: 7.061043] time: 0:05:03.736012\n",
      "(30, 64, 64, 3)\n",
      "0.8599172\n",
      "[Epoch 2/5] [Batch 226/360] [D loss: 0.072323] [G loss: 6.576604] time: 0:05:03.913326\n",
      "(30, 64, 64, 3)\n",
      "0.90056294\n",
      "[Epoch 2/5] [Batch 227/360] [D loss: 0.057184] [G loss: 6.844611] time: 0:05:04.097414\n",
      "(30, 64, 64, 3)\n",
      "0.8930149\n",
      "[Epoch 2/5] [Batch 228/360] [D loss: 0.153181] [G loss: 6.945486] time: 0:05:04.318682\n",
      "(30, 64, 64, 3)\n",
      "0.87596816\n",
      "[Epoch 2/5] [Batch 229/360] [D loss: 0.112620] [G loss: 6.694433] time: 0:05:04.521697\n",
      "(30, 64, 64, 3)\n",
      "0.88564175\n",
      "[Epoch 2/5] [Batch 230/360] [D loss: 0.039425] [G loss: 6.838265] time: 0:05:04.676776\n",
      "(30, 64, 64, 3)\n",
      "0.80888253\n",
      "[Epoch 2/5] [Batch 231/360] [D loss: 0.049020] [G loss: 6.875766] time: 0:05:04.902261\n",
      "(30, 64, 64, 3)\n",
      "0.8808675\n",
      "[Epoch 2/5] [Batch 232/360] [D loss: 0.079610] [G loss: 6.483555] time: 0:05:05.063063\n",
      "(30, 64, 64, 3)\n",
      "0.88647693\n",
      "[Epoch 2/5] [Batch 233/360] [D loss: 0.032529] [G loss: 6.144383] time: 0:05:05.225849\n",
      "(30, 64, 64, 3)\n",
      "0.90699434\n",
      "[Epoch 2/5] [Batch 234/360] [D loss: 0.054147] [G loss: 6.446342] time: 0:05:05.426404\n",
      "(30, 64, 64, 3)\n",
      "0.8729966\n",
      "[Epoch 2/5] [Batch 235/360] [D loss: 0.284693] [G loss: 6.627099] time: 0:05:05.646121\n",
      "(30, 64, 64, 3)\n",
      "0.8665512\n",
      "[Epoch 2/5] [Batch 236/360] [D loss: 0.070905] [G loss: 7.087224] time: 0:05:05.835770\n",
      "(30, 64, 64, 3)\n",
      "0.9017076\n",
      "[Epoch 2/5] [Batch 237/360] [D loss: 0.047733] [G loss: 6.864686] time: 0:05:06.029177\n",
      "(30, 64, 64, 3)\n",
      "0.89956886\n",
      "[Epoch 2/5] [Batch 238/360] [D loss: 0.044155] [G loss: 7.233632] time: 0:05:06.235062\n",
      "(30, 64, 64, 3)\n",
      "0.8865535\n",
      "[Epoch 2/5] [Batch 239/360] [D loss: 0.069988] [G loss: 7.290987] time: 0:05:06.438135\n",
      "(30, 64, 64, 3)\n",
      "0.89099246\n",
      "[Epoch 2/5] [Batch 240/360] [D loss: 0.089538] [G loss: 6.739630] time: 0:05:06.697350\n",
      "(30, 64, 64, 3)\n",
      "0.83790874\n",
      "[Epoch 2/5] [Batch 241/360] [D loss: 0.088194] [G loss: 7.110896] time: 0:05:06.899980\n",
      "(30, 64, 64, 3)\n",
      "0.8554514\n",
      "[Epoch 2/5] [Batch 242/360] [D loss: 0.178687] [G loss: 7.095422] time: 0:05:07.093914\n",
      "(30, 64, 64, 3)\n",
      "0.8794908\n",
      "[Epoch 2/5] [Batch 243/360] [D loss: 0.089550] [G loss: 6.952402] time: 0:05:07.306095\n",
      "(30, 64, 64, 3)\n",
      "0.8880527\n",
      "[Epoch 2/5] [Batch 244/360] [D loss: 0.027895] [G loss: 6.582946] time: 0:05:07.502845\n",
      "(30, 64, 64, 3)\n",
      "0.8795716\n",
      "[Epoch 2/5] [Batch 245/360] [D loss: 0.033482] [G loss: 6.857618] time: 0:05:07.689577\n",
      "(30, 64, 64, 3)\n",
      "0.8803196\n",
      "[Epoch 2/5] [Batch 246/360] [D loss: 0.056184] [G loss: 7.015878] time: 0:05:07.882695\n",
      "(30, 64, 64, 3)\n",
      "0.8560193\n",
      "[Epoch 2/5] [Batch 247/360] [D loss: 0.301317] [G loss: 7.114709] time: 0:05:08.088207\n",
      "(30, 64, 64, 3)\n",
      "0.8453768\n",
      "[Epoch 2/5] [Batch 248/360] [D loss: 0.052877] [G loss: 7.006164] time: 0:05:08.325756\n",
      "(30, 64, 64, 3)\n",
      "0.82666683\n",
      "[Epoch 2/5] [Batch 249/360] [D loss: 0.042320] [G loss: 6.700946] time: 0:05:08.557432\n",
      "(30, 64, 64, 3)\n",
      "0.9032743\n",
      "[Epoch 2/5] [Batch 250/360] [D loss: 0.127687] [G loss: 7.327629] time: 0:05:08.744446\n",
      "(30, 64, 64, 3)\n",
      "0.88498306\n",
      "[Epoch 2/5] [Batch 251/360] [D loss: 0.292539] [G loss: 6.961227] time: 0:05:08.916859\n",
      "(30, 64, 64, 3)\n",
      "0.83508563\n",
      "[Epoch 2/5] [Batch 252/360] [D loss: 0.027173] [G loss: 7.031213] time: 0:05:09.102106\n",
      "(30, 64, 64, 3)\n",
      "0.91075546\n",
      "[Epoch 2/5] [Batch 253/360] [D loss: 0.085582] [G loss: 7.114581] time: 0:05:09.272426\n",
      "(30, 64, 64, 3)\n",
      "0.8849716\n",
      "[Epoch 2/5] [Batch 254/360] [D loss: 0.094885] [G loss: 6.581625] time: 0:05:09.450726\n",
      "(30, 64, 64, 3)\n",
      "0.86092085\n",
      "[Epoch 2/5] [Batch 255/360] [D loss: 0.028070] [G loss: 6.676363] time: 0:05:09.803270\n",
      "(30, 64, 64, 3)\n",
      "0.91992015\n",
      "[Epoch 2/5] [Batch 256/360] [D loss: 0.051342] [G loss: 6.175388] time: 0:05:10.019730\n",
      "(30, 64, 64, 3)\n",
      "0.8891471\n",
      "[Epoch 2/5] [Batch 257/360] [D loss: 0.122257] [G loss: 6.667206] time: 0:05:10.230819\n",
      "(30, 64, 64, 3)\n",
      "0.89023966\n",
      "[Epoch 2/5] [Batch 258/360] [D loss: 0.310854] [G loss: 6.099066] time: 0:05:10.408805\n",
      "(30, 64, 64, 3)\n",
      "0.870771\n",
      "[Epoch 2/5] [Batch 259/360] [D loss: 0.093431] [G loss: 6.629173] time: 0:05:10.604447\n",
      "(30, 64, 64, 3)\n",
      "0.8631625\n",
      "[Epoch 2/5] [Batch 260/360] [D loss: 0.127122] [G loss: 6.319006] time: 0:05:10.795808\n",
      "(30, 64, 64, 3)\n",
      "0.90120405\n",
      "[Epoch 2/5] [Batch 261/360] [D loss: 0.104734] [G loss: 6.262616] time: 0:05:10.966543\n",
      "(30, 64, 64, 3)\n",
      "0.9086149\n",
      "[Epoch 2/5] [Batch 262/360] [D loss: 0.086483] [G loss: 6.813387] time: 0:05:11.114150\n",
      "(30, 64, 64, 3)\n",
      "0.89258283\n",
      "[Epoch 2/5] [Batch 263/360] [D loss: 0.098247] [G loss: 6.579489] time: 0:05:11.306908\n",
      "(30, 64, 64, 3)\n",
      "0.8629046\n",
      "[Epoch 2/5] [Batch 264/360] [D loss: 0.086302] [G loss: 6.526937] time: 0:05:11.481167\n",
      "(30, 64, 64, 3)\n",
      "0.83301306\n",
      "[Epoch 2/5] [Batch 265/360] [D loss: 0.097698] [G loss: 6.488541] time: 0:05:11.700408\n",
      "(30, 64, 64, 3)\n",
      "0.8788207\n",
      "[Epoch 2/5] [Batch 266/360] [D loss: 0.096064] [G loss: 6.488579] time: 0:05:11.867251\n",
      "(30, 64, 64, 3)\n",
      "0.8856616\n",
      "[Epoch 2/5] [Batch 267/360] [D loss: 0.067195] [G loss: 6.353868] time: 0:05:12.055351\n",
      "(30, 64, 64, 3)\n",
      "0.8904526\n",
      "[Epoch 2/5] [Batch 268/360] [D loss: 0.024059] [G loss: 6.060432] time: 0:05:12.285249\n",
      "(30, 64, 64, 3)\n",
      "0.8914214\n",
      "[Epoch 2/5] [Batch 269/360] [D loss: 0.035911] [G loss: 6.942231] time: 0:05:12.456016\n",
      "(30, 64, 64, 3)\n",
      "0.841106\n",
      "[Epoch 2/5] [Batch 270/360] [D loss: 0.039083] [G loss: 6.662906] time: 0:05:12.649374\n",
      "(30, 64, 64, 3)\n",
      "0.92304534\n",
      "[Epoch 2/5] [Batch 271/360] [D loss: 0.027574] [G loss: 6.137290] time: 0:05:12.859432\n",
      "(30, 64, 64, 3)\n",
      "0.8762476\n",
      "[Epoch 2/5] [Batch 272/360] [D loss: 0.031818] [G loss: 7.190288] time: 0:05:13.097565\n",
      "(30, 64, 64, 3)\n",
      "0.886982\n",
      "[Epoch 2/5] [Batch 273/360] [D loss: 0.046799] [G loss: 6.866181] time: 0:05:13.343770\n",
      "(30, 64, 64, 3)\n",
      "0.9193086\n",
      "[Epoch 2/5] [Batch 274/360] [D loss: 0.022471] [G loss: 6.665140] time: 0:05:13.603528\n",
      "(30, 64, 64, 3)\n",
      "0.9103861\n",
      "[Epoch 2/5] [Batch 275/360] [D loss: 0.026589] [G loss: 6.101126] time: 0:05:13.842002\n",
      "(30, 64, 64, 3)\n",
      "0.8518569\n",
      "[Epoch 2/5] [Batch 276/360] [D loss: 0.030230] [G loss: 6.028717] time: 0:05:14.021495\n",
      "(30, 64, 64, 3)\n",
      "0.8740187\n",
      "[Epoch 2/5] [Batch 277/360] [D loss: 0.042512] [G loss: 5.573484] time: 0:05:14.231692\n",
      "(30, 64, 64, 3)\n",
      "0.89371556\n",
      "[Epoch 2/5] [Batch 278/360] [D loss: 0.034667] [G loss: 6.002675] time: 0:05:14.418045\n",
      "(30, 64, 64, 3)\n",
      "0.8791509\n",
      "[Epoch 2/5] [Batch 279/360] [D loss: 0.069288] [G loss: 6.398297] time: 0:05:14.601104\n",
      "(30, 64, 64, 3)\n",
      "0.9435794\n",
      "[Epoch 2/5] [Batch 280/360] [D loss: 0.501878] [G loss: 6.120923] time: 0:05:14.818897\n"
     ]
    }
   ],
   "source": [
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import img_as_ubyte\n",
    "import numpy as np \n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "test_first_imgs, test_last_imgs = next(test_batch_generator)\n",
    "\n",
    "for epoch in range(cfg.NUM_EPOCHS):\n",
    "    steps_per_epoch = (nbr_train_data // cfg.BATCH_SIZE) \n",
    "    for batch_i in range(steps_per_epoch):\n",
    "        first_frames, last_frames= next(train_batch_generator)\n",
    "        if first_frames.shape[0] == cfg.BATCH_SIZE: \n",
    "             \n",
    "            # Condition on the first frame and generate the last frame\n",
    "            fake_last_frames = modelObj.generator.predict(first_frames)\n",
    "            #plt.imshow(fake_last_frames[1])\n",
    "            print(fake_last_frames.shape)\n",
    "            #print(tf.keras.backend.mean(fake_last_frames[0]))\n",
    "            print(np.mean(fake_last_frames[0]))\n",
    "\n",
    "            # Train the discriminator with combined loss  \n",
    "            d_loss_real = modelObj.discriminator.train_on_batch([last_frames, first_frames], valid)\n",
    "            d_loss_fake = modelObj.discriminator.train_on_batch([fake_last_frames, first_frames], fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    " \n",
    "            # Train the generator\n",
    "            g_loss = modelObj.combined.train_on_batch([last_frames, first_frames], [valid, last_frames])\n",
    "\n",
    "            elapsed_time = datetime.now() - start_time \n",
    "            print (\"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f] time: %s\" % (epoch, cfg.NUM_EPOCHS,\n",
    "                                                                                               batch_i,\n",
    "                                                                                               steps_per_epoch,\n",
    "                                                                                               d_loss[0], \n",
    "                                                                                               g_loss[0],\n",
    "                                                                                               elapsed_time))\n",
    "            # run some tests to check how the generated images evolve during training\n",
    "            test_fake_last_imgs = modelObj.generator.predict(test_first_imgs)\n",
    "            test_img_name = output_log_dir + \"/gen_img_epoc_\" + str(epoch) + \".png\"\n",
    "            merged_img = np.vstack((first_frames[0],last_frames[0],fake_last_frames[0]))\n",
    "            imageio.imwrite(test_img_name, img_as_ubyte(merged_img)) #scipy.misc.imsave(test_img_name, merged_img)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9) We can test the model with 100 test data which will be saved as images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_i in range(100):\n",
    "    test_first_imgs, test_last_imgs = next(test_batch_generator)\n",
    "    test_fake_last_imgs = modelObj.generator.predict(test_first_imgs) \n",
    "\n",
    "    test_img_name = output_log_dir + \"/gen_img_test_\" + str(batch_i) + \".png\"\n",
    "    merged_img = np.vstack((test_first_imgs[0],test_last_imgs[0],test_fake_last_imgs[0]))\n",
    "    imageio.imwrite(test_img_name, img_as_ubyte(merged_img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1)\n",
    "Update the network architecture given in  **build_generator**  and  **build_discriminator**  of the class GANModel. Please note that the current image resolution is set to 32x32 (i.e. IMAGE_WIDTH and IMAGE_HEIGHT values) in the file configGAN.py. \n",
    "This way initial experiements can run faster. Once you implement the inital version of the network, please set the resolution values back to 128x128. Experimental results should be provided for this high resolution images.  \n",
    "\n",
    "**Hint:** As a generator model, you can use the segmentation model implemented in lab03. Do not forget to adapt the input and output shapes of the generator model in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2) \n",
    "Use different **optimization** (e.g. ADAM, SGD, etc) and **regularization** (e.g. data augmentation, dropout) methods to increase the network accuracy. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
